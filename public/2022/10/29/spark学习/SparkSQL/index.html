<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.13.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="这是文章开头，显示在主页面，详情请点击此处。">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkSQL">
<meta property="og:url" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/index.html">
<meta property="og:site_name" content="Tiger_pop&#39;s Blog">
<meta property="og:description" content="这是文章开头，显示在主页面，详情请点击此处。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.01.12-7050600.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.01.43.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.02.52.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.10.10.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.12.32.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.19.12.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.22.20.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-14%2014.56.10.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-14%2014.58.32.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-14%2016.09.59.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-14%2020.37.57.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2015.17.13.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2016.10.19.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2016.21.20.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2020.36.42.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2022.08.05.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2022.13.32.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2022.26.14.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-16%2016.15.54.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-16%2016.39.22.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-16%2017.53.27.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-16%2019.21.17.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-16%2019.23.45.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-16%2021.57.07.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2015.54.45.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2017.24.34.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2017.29.58.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2017.48.44.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2021.56.10.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2021.58.10.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2021.59.22.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.02.01.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.06.37.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.08.43.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.13.23.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.21.34.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.22.49.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.25.24.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.27.10.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.43.37.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.47.15.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.47.58.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.49.24.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-18%2019.22.53.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-18%2021.06.57.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-19%2012.38.03.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-19%2012.40.12.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2018.01.04.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.14.11.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.17.47.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.24.08.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.30.48.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.45.03.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.47.24.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.59.38.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2021.03.46.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2021.14.20.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2021.33.22.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2021.51.25.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-21%2012.48.49.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-21%2013.24.12.jpg">
<meta property="article:published_time" content="2022-10-29T14:41:37.000Z">
<meta property="article:modified_time" content="2022-10-29T15:00:20.881Z">
<meta property="article:author" content="陈宇韶chenyushao">
<meta property="article:tag" content="sql">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.01.12-7050600.jpg">


<link rel="canonical" href="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/","path":"2022/10/29/spark学习/SparkSQL/","title":"SparkSQL"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>SparkSQL | Tiger_pop's Blog</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Tiger_pop's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Tiger_pop's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">tiger_pop 的博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#DataFrame-%E5%9F%BA%E7%A1%80"><span class="nav-number">1.</span> <span class="nav-text">DataFrame 基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#dataframe-%E6%9E%84%E5%BB%BA"><span class="nav-number">1.1.</span> <span class="nav-text">dataframe 构建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dataframe-%E7%BC%96%E7%A8%8B"><span class="nav-number">1.2.</span> <span class="nav-text">dataframe 编程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DSL-%E4%BB%A3%E7%A0%81%E9%A3%8E%E6%A0%BC"><span class="nav-number">1.2.1.</span> <span class="nav-text">DSL 代码风格</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SQL%E9%A3%8E%E6%A0%BC"><span class="nav-number">1.2.2.</span> <span class="nav-text">SQL风格</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#wordcount-%E6%A1%88%E4%BE%8B"><span class="nav-number">1.2.3.</span> <span class="nav-text">wordcount 案例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%B5%E5%BD%B1%E8%AF%84%E5%88%86-%E5%88%86%E6%9E%90%E6%A1%88%E4%BE%8B"><span class="nav-number">1.2.4.</span> <span class="nav-text">电影评分 分析案例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sparkSQL-shuffle-%E5%88%86%E5%8C%BA%E6%95%B0%E7%9B%AE"><span class="nav-number">1.2.5.</span> <span class="nav-text">sparkSQL shuffle 分区数目</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sparkSQL-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97API"><span class="nav-number">1.2.6.</span> <span class="nav-text">sparkSQL 数据清洗API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dataframe%E6%95%B0%E6%8D%AE%E5%86%99%E5%87%BA"><span class="nav-number">1.2.7.</span> <span class="nav-text">dataframe数据写出</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%BB%E5%85%A5%E5%92%8C%E5%86%99%E5%87%BA-jdbc-%E6%95%B0%E6%8D%AE"><span class="nav-number">1.2.7.1.</span> <span class="nav-text">读入和写出 jdbc 数据</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SparkSQL-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="nav-number">2.</span> <span class="nav-text">SparkSQL 自定义函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A8%E5%86%8C%E4%B8%80%E4%B8%AA%E8%BF%94%E5%9B%9E%E5%80%BC%E4%B8%BAarray%E7%B1%BB%E5%9E%8B%E7%9A%84udf"><span class="nav-number">2.0.1.</span> <span class="nav-text">注册一个返回值为array类型的udf</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A8%E5%86%8C%E4%B8%80%E4%B8%AA%E8%BF%94%E5%9B%9E%E5%80%BC%E4%B8%BAdict%E7%B1%BB%E5%9E%8B%E7%9A%84udf"><span class="nav-number">2.0.2.</span> <span class="nav-text">注册一个返回值为dict类型的udf</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#python%E7%BB%95%E8%B7%AF%E5%AE%9E%E7%8E%B0UDAF"><span class="nav-number">2.0.3.</span> <span class="nav-text">python绕路实现UDAF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sparkSQL-%E4%BD%BF%E7%94%A8%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0"><span class="nav-number">2.0.4.</span> <span class="nav-text">sparkSQL 使用窗口函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SparkSQL-%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="nav-number">3.</span> <span class="nav-text">SparkSQL 执行流程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-on-Hive"><span class="nav-number">4.</span> <span class="nav-text">Spark on Hive</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8FSQL-%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E"><span class="nav-number">5.</span> <span class="nav-text">分布式SQL 执行引擎</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%BC%E5%90%88%E7%BB%83%E4%B9%A0"><span class="nav-number">6.</span> <span class="nav-text">综合练习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark%E6%96%B0%E7%89%B9%E6%80%A7-%E5%8F%8A-%E6%A0%B8%E5%BF%83%E5%9B%9E%E9%A1%BE"><span class="nav-number">7.</span> <span class="nav-text">Spark新特性 及 核心回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#shuffle"><span class="nav-number">7.1.</span> <span class="nav-text">shuffle</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B0%E7%89%B9%E6%80%A7"><span class="nav-number">7.2.</span> <span class="nav-text">新特性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E9%80%82%E5%BA%94%E6%9F%A5%E8%AF%A2"><span class="nav-number">7.2.1.</span> <span class="nav-text">自适应查询</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%E3%80%81%E5%8A%A8%E6%80%81%E5%90%88%E5%B9%B6"><span class="nav-number">7.2.1.1.</span> <span class="nav-text">1、动态合并</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2%E3%80%81%E5%8A%A8%E6%80%81-join-%E7%AD%96%E7%95%A5"><span class="nav-number">7.2.1.2.</span> <span class="nav-text">2、动态 join 策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3%E3%80%81%E5%8A%A8%E6%80%81%E4%BC%98%E5%8C%96%E5%80%BE%E6%96%9Cjoin"><span class="nav-number">7.2.1.3.</span> <span class="nav-text">3、动态优化倾斜join</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A3%81%E5%89%AA"><span class="nav-number">7.2.2.</span> <span class="nav-text">动态分区裁剪</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#koalas"><span class="nav-number">7.2.3.</span> <span class="nav-text">koalas</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="陈宇韶chenyushao"
      src="/images/my.jpg">
  <p class="site-author-name" itemprop="name">陈宇韶chenyushao</p>
  <div class="site-description" itemprop="description">爱学习、爱工作、爱生活;         微信号: Tiger_and_master;         手机号码:18515678348 </div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">378</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">138</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my.jpg">
      <meta itemprop="name" content="陈宇韶chenyushao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tiger_pop's Blog">
      <meta itemprop="description" content="爱学习、爱工作、爱生活;         微信号: Tiger_and_master;         手机号码:18515678348 ">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="SparkSQL | Tiger_pop's Blog">
      <meta itemprop="description" content="这是文章开头，显示在主页面，详情请点击此处。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          SparkSQL
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-10-29 22:41:37 / 修改时间：23:00:20" itemprop="dateCreated datePublished" datetime="2022-10-29T22:41:37+08:00">2022-10-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/spark/" itemprop="url" rel="index"><span itemprop="name">spark</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">这是文章开头，显示在主页面，详情请点击此处。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>简介 <span id="more"></span></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.01.12-7050600.jpg" alt="截屏2022-04-12 22.01.12"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.01.43.jpg" alt="截屏2022-04-12 22.01.43"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.02.52.jpg" alt="截屏2022-04-12 22.02.52"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.10.10.jpg" alt="截屏2022-04-12 22.10.10"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.12.32.jpg" alt="截屏2022-04-12 22.12.32"></p>
<p>​		python 开发spark 都是以dataframe 对象作为核心数据结构。</p>
<p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.19.12.jpg" alt="截屏2022-04-12 22.19.12" style="zoom:50%;"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-12%2022.22.20.jpg" alt="截屏2022-04-12 22.22.20"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 构建sparksession对象。</span></span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>).master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">		</span><br><span class="line">    <span class="comment"># 可以通过sparksession对象 ，构建 sparkcontext对象。</span></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line">		</span><br><span class="line">    df = spark.read.csv(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/stu_score.txt&#x27;</span>,sep=<span class="string">&#x27;,&#x27;</span>,header=<span class="literal">False</span>)</span><br><span class="line">		</span><br><span class="line">    df2 = df.toDF(<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;score&#x27;</span>)  <span class="comment"># 给文件内容加表头。</span></span><br><span class="line">    df2.printSchema()  									<span class="comment"># 输出 表的结构。</span></span><br><span class="line">    df2.show()         									<span class="comment"># 输出表</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># SQL style sql风格处理</span></span><br><span class="line">    df2.createTempView(<span class="string">&#x27;score&#x27;</span>)</span><br><span class="line">    spark.sql(<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        select * from score where name=&#x27;语文&#x27; LIMIT 5</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># DSL style 代码风格处理</span></span><br><span class="line">    df2.where(<span class="string">&quot;name=&#x27;语文&#x27;&quot;</span>).limit(<span class="number">5</span>).show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">root</span><br><span class="line"> |-- <span class="built_in">id</span>: string (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- score: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+----+-----+</span><br><span class="line">| <span class="built_in">id</span>|name|score|</span><br><span class="line">+---+----+-----+</span><br><span class="line">|  <span class="number">1</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">2</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">3</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">4</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">5</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">6</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">7</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">8</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">9</span>|语文|   <span class="number">99</span>|</span><br><span class="line">| <span class="number">10</span>|语文|   <span class="number">99</span>|</span><br><span class="line">| <span class="number">11</span>|语文|   <span class="number">99</span>|</span><br><span class="line">| <span class="number">12</span>|语文|   <span class="number">99</span>|</span><br><span class="line">| <span class="number">13</span>|语文|   <span class="number">99</span>|</span><br><span class="line">| <span class="number">14</span>|语文|   <span class="number">99</span>|</span><br><span class="line">| <span class="number">15</span>|语文|   <span class="number">99</span>|</span><br><span class="line">| <span class="number">16</span>|语文|   <span class="number">99</span>|</span><br><span class="line">| <span class="number">17</span>|语文|   <span class="number">99</span>|</span><br><span class="line">| <span class="number">18</span>|语文|   <span class="number">99</span>|</span><br><span class="line">| <span class="number">19</span>|语文|   <span class="number">99</span>|</span><br><span class="line">| <span class="number">20</span>|语文|   <span class="number">99</span>|</span><br><span class="line">+---+----+-----+</span><br><span class="line">only showing top <span class="number">20</span> rows</span><br><span class="line"></span><br><span class="line">+---+----+-----+</span><br><span class="line">| <span class="built_in">id</span>|name|score|</span><br><span class="line">+---+----+-----+</span><br><span class="line">|  <span class="number">1</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">2</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">3</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">4</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">5</span>|语文|   <span class="number">99</span>|</span><br><span class="line">+---+----+-----+</span><br><span class="line"></span><br><span class="line">+---+----+-----+</span><br><span class="line">| <span class="built_in">id</span>|name|score|</span><br><span class="line">+---+----+-----+</span><br><span class="line">|  <span class="number">1</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">2</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">3</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">4</span>|语文|   <span class="number">99</span>|</span><br><span class="line">|  <span class="number">5</span>|语文|   <span class="number">99</span>|</span><br><span class="line">+---+----+-----+</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h1 id="DataFrame-基础"><a href="#DataFrame-基础" class="headerlink" title="DataFrame 基础"></a>DataFrame 基础</h1><p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-14%2014.56.10.jpg" alt="截屏2022-04-14 14.56.10"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-14%2014.58.32.jpg" alt="截屏2022-04-14 14.58.32"></p>
<h2 id="dataframe-构建"><a href="#dataframe-构建" class="headerlink" title="dataframe 构建"></a>dataframe 构建</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建方式一：从 rdd 构建 dataframe(使用createDataFrame)</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;create_sparksession&#x27;</span>).master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    <span class="comment"># turn rdd to dataframe</span></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line">    rdd = sc.textFile(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/sql/people.txt&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(rdd.collect())  <span class="comment"># context is --  [&#x27;Michael, 29&#x27;, &#x27;Andy, 30&#x27;, &#x27;Justin, 19&#x27;]</span></span><br><span class="line">    rdd1 = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;,&#x27;</span>)).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>],<span class="built_in">int</span>(x[<span class="number">1</span>])))</span><br><span class="line">    </span><br><span class="line">    df = spark.createDataFrame(rdd1,schema=[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;age&#x27;</span>])</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&#x27;people&#x27;</span>)</span><br><span class="line">    spark.sql(<span class="string">&#x27;select * from people where age&lt;30&#x27;</span>).show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建方式二：从 rdd 构建 dataframe(使用StructType().add() + createDataFrame)</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;create_sparksession&#x27;</span>).master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    <span class="comment"># turn rdd to dataframe</span></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line">    rdd = sc.textFile(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/sql/people.txt&#x27;</span>)</span><br><span class="line">    rdd1 = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;,&#x27;</span>)).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>],<span class="built_in">int</span>(x[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line">    schema = StructType().add(<span class="string">&#x27;name&#x27;</span>,StringType(),nullable=<span class="literal">False</span>).\</span><br><span class="line">        add(<span class="string">&#x27;age&#x27;</span>,IntegerType(),nullable=<span class="literal">True</span>)</span><br><span class="line">    df = spark.createDataFrame(rdd1,schema=schema)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建方式三：从 rdd 构建 dataframe(使用toDF()方法)</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;create_sparksession&#x27;</span>).master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    <span class="comment"># turn rdd to dataframe</span></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line">    rdd = sc.textFile(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/sql/people.txt&#x27;</span>)</span><br><span class="line">    rdd1 = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;,&#x27;</span>)).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>],<span class="built_in">int</span>(x[<span class="number">1</span>])))</span><br><span class="line">    rdd1.cache()</span><br><span class="line"></span><br><span class="line">    df1 = rdd1.toDF([<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;age&#x27;</span>])</span><br><span class="line">    df1.printSchema()</span><br><span class="line">    df1.show()</span><br><span class="line"></span><br><span class="line">    schema = StructType().add(<span class="string">&#x27;name&#x27;</span>,StringType(),nullable=<span class="literal">False</span>).\</span><br><span class="line">        add(<span class="string">&#x27;age&#x27;</span>,IntegerType(),nullable=<span class="literal">True</span>)</span><br><span class="line">    df2 = rdd1.toDF(schema=schema)</span><br><span class="line">    df2.printSchema()</span><br><span class="line">    df2.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建方式四：从 pandas的dataframe 构建 spark的dataframe</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;create_sparksession&#x27;</span>).master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    <span class="comment"># turn pandas&#x27;dataframe to spark&#x27;dataframe</span></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line">    pddf = pd.DataFrame(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&#x27;id&#x27;</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">            <span class="string">&#x27;name&#x27;</span>:[<span class="string">&#x27;zhangsan&#x27;</span>,<span class="string">&#x27;lisi&#x27;</span>,<span class="string">&#x27;wanwu&#x27;</span>],</span><br><span class="line">            <span class="string">&#x27;age&#x27;</span>:[<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>]</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    df = spark.createDataFrame(pddf)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-14%2016.09.59.jpg" alt="截屏2022-04-14 16.09.59"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取外部text 文件。</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;create_sparksession&#x27;</span>).master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    <span class="comment"># turn pandas&#x27;dataframe to spark&#x27;dataframe</span></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># if data is text format, whole row as one elements into dataframe&#x27;field, new dataframe with only one field.</span></span><br><span class="line">    schema = StructType().add(<span class="string">&#x27;just_one_filed&#x27;</span>,StringType(),nullable=<span class="literal">True</span>)</span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;text&#x27;</span>).schema(schema=schema).\</span><br><span class="line">        load(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/sql/people.txt&#x27;</span>)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output  读text数据，特点是原数据中一整行被读成一个元素，新的dataframe只有一个field，列名叫 value 。这里把默认的 列名改成 just_one_filed 了。</span></span><br><span class="line">root</span><br><span class="line"> |-- just_one_filed: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+--------------+</span><br><span class="line">|just_one_filed|</span><br><span class="line">+--------------+</span><br><span class="line">|   Michael, <span class="number">29</span>|</span><br><span class="line">|      Andy, <span class="number">30</span>|</span><br><span class="line">|    Justin, <span class="number">19</span>|</span><br><span class="line">+--------------+</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取外部 json 文件。</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;create_sparksession&#x27;</span>).master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    <span class="comment"># turn pandas&#x27;dataframe to spark&#x27;dataframe</span></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># json is so clear and easy.</span></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;json&#x27;</span>).load(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/sql/people.json&#x27;</span>)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取外部 csv 文件。</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;create_sparksession&#x27;</span>).master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># read csv</span></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;csv&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;sep&#x27;</span>,<span class="string">&#x27;;&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;header&#x27;</span>,<span class="literal">True</span>).\  <span class="comment"># 这个csv 文件有表头的。</span></span><br><span class="line">        option(<span class="string">&#x27;encoding&#x27;</span>,<span class="string">&#x27;utf-8&#x27;</span>).\</span><br><span class="line">        schema(<span class="string">&#x27;name STRING,age INT,job STRING&#x27;</span>).\</span><br><span class="line">        load(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/sql/people.csv&#x27;</span>)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-14%2020.37.57.jpg" alt="截屏2022-04-14 20.37.57"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取外部 parquet 文件(parquet 文件带压缩)</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;create_sparksession&#x27;</span>).master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># read parquet</span></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;parquet&#x27;</span>).load(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/sql/users.parquet&#x27;</span>)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- favorite_color: string (nullable = true)</span><br><span class="line"> |-- favorite_numbers: array (nullable = true)</span><br><span class="line"> |    |-- element: integer (containsNull = true)</span><br><span class="line"></span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|  name|favorite_color|favorite_numbers|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|Alyssa|          null|  [<span class="number">3</span>, <span class="number">9</span>, <span class="number">15</span>, <span class="number">20</span>]|</span><br><span class="line">|   Ben|           red|              []|</span><br><span class="line">+------+--------------+----------------+</span><br></pre></td></tr></table></figure>

<p>在idea加载 Avro parquet view 工具，只需要把文件拖入代码框 下方的 Avro parquet view 窗口即可查看parquet 文件。</p>
<h2 id="dataframe-编程"><a href="#dataframe-编程" class="headerlink" title="dataframe 编程"></a>dataframe 编程</h2><p>​		idea 编程小技巧。除了<code>ctrl+command+点击</code>查看方法源码。还可以 <code>ctrl+p</code> 大致查看方法需要的形参。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2015.17.13.jpg" alt="截屏2022-04-15 15.17.13"></p>
<p>​		select有两种调用形式：</p>
<p>​				第一种<code>*cols</code> , 的形参要求是column或者str两种写法。</p>
<p>​						<code>df.select(&#39;id&#39;,&#39;subject&#39;).show()</code></p>
<p>​						<code>df.select(id_column,subject_column).show()</code></p>
<p>​				第二种 <code>__cols</code> , 的形参要求 是list包裹column或者list包裹str写法。</p>
<p>​						<code>df.select([&#39;id&#39;,&#39;subjuct&#39;]).show()</code></p>
<p>​						<code>df.select([id_column,subject_column]).show()</code></p>
<h3 id="DSL-代码风格"><a href="#DSL-代码风格" class="headerlink" title="DSL 代码风格"></a>DSL 代码风格</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DSL 代码风格的dataframe编程。</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>).master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;csv&#x27;</span>).schema(<span class="string">&#x27;id INT,subject STRING,score INT&#x27;</span>).\</span><br><span class="line">        load(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/sql/stu_score.txt&#x27;</span>)</span><br><span class="line">    <span class="comment"># obtain column object</span></span><br><span class="line">    id_column = df[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">    subject_column = df[<span class="string">&#x27;subject&#x27;</span>]</span><br><span class="line">    <span class="comment"># DSL code style</span></span><br><span class="line">    df.select([<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;subject&#x27;</span>]).show()</span><br><span class="line">    df.select(<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;subject&#x27;</span>).show()</span><br><span class="line">    df.select(id_column,subject_column).show()</span><br><span class="line">    df.select([id_column,subject_column]).show()</span><br><span class="line"></span><br><span class="line">    df.<span class="built_in">filter</span>(<span class="string">&#x27;score&lt;99&#x27;</span>).show()</span><br><span class="line">    df.<span class="built_in">filter</span>(df[<span class="string">&#x27;score&#x27;</span>]&lt;<span class="number">99</span>).show()</span><br><span class="line"></span><br><span class="line">    df.where(<span class="string">&#x27;score&lt;99&#x27;</span>).show()</span><br><span class="line">    df.where(df[<span class="string">&#x27;score&#x27;</span>]&lt;<span class="number">99</span>).show()</span><br><span class="line"></span><br><span class="line">    df.groupBy(<span class="string">&#x27;subject&#x27;</span>).count().show()</span><br><span class="line">    df.groupBy(df[<span class="string">&#x27;subject&#x27;</span>]).count().show()</span><br><span class="line">    r = df.groupBy(df[<span class="string">&#x27;subject&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(r))  <span class="comment"># type is groupdata not dataframe ,so it havenot show method.</span></span><br><span class="line">    r.<span class="built_in">sum</span>().show()  <span class="comment"># if groupdata tyle used accumulate method ,it&#x27;s result is dataframe.</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(r.<span class="built_in">sum</span>()))  <span class="comment"># result is dataframe style。</span></span><br></pre></td></tr></table></figure>

<h3 id="SQL风格"><a href="#SQL风格" class="headerlink" title="SQL风格"></a>SQL风格</h3><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2016.10.19.jpg" alt="截屏2022-04-15 16.10.19"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SQL风格 dataframe编程。</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>).master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;csv&#x27;</span>).schema(<span class="string">&#x27;id INT,subject STRING,score INT&#x27;</span>). \</span><br><span class="line">        load(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/sql/stu_score.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    df.createTempView(<span class="string">&#x27;score&#x27;</span>)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&#x27;score2&#x27;</span>)</span><br><span class="line">    df.createGlobalTempView(<span class="string">&#x27;score3&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&#x27;select subject, count(*) as cnt from score group by subject&#x27;</span>).show()</span><br><span class="line">    spark.sql(<span class="string">&#x27;select subject, count(*) as cnt from score2 group by subject&#x27;</span>).show()</span><br><span class="line">    spark.sql(<span class="string">&#x27;select subject, count(*) as cnt from global_temp.score3 group by subject&#x27;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="wordcount-案例"><a href="#wordcount-案例" class="headerlink" title="wordcount 案例"></a>wordcount 案例</h3><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2016.21.20.jpg" alt="截屏2022-04-15 16.21.20"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hello spark</span></span><br><span class="line"><span class="comment"># hello hadoop</span></span><br><span class="line"><span class="comment"># hello flink</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>).master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sql style</span></span><br><span class="line">    rdd = sc.textFile(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/words.txt&#x27;</span>).\</span><br><span class="line">    flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27; &#x27;</span>)).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: [x])</span><br><span class="line">    df = rdd.toDF([<span class="string">&#x27;word&#x27;</span>])</span><br><span class="line">    df.createTempView(<span class="string">&#x27;words&#x27;</span>)</span><br><span class="line">    spark.sql(<span class="string">&#x27;select word ,count(*) as cnt from words group by word order by cnt DESC &#x27;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dsl style</span></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;text&#x27;</span>).load(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/words.txt&#x27;</span>)</span><br><span class="line">    <span class="comment"># withcolumn 方法是用后面指定的操作新建一个字段，如果字段已经存在就替换掉。</span></span><br><span class="line">    <span class="comment"># explode 是将横向的 [a,b,c] 炸开成纵向的 a b c 这样的。</span></span><br><span class="line">    df2 = df.withColumn(<span class="string">&#x27;value&#x27;</span>,F.explode(F.split(df[<span class="string">&#x27;value&#x27;</span>],<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">    df2.show()</span><br><span class="line">    df2.groupBy(<span class="string">&#x27;value&#x27;</span>).count().withColumnRenamed(<span class="string">&#x27;value&#x27;</span>,<span class="string">&#x27;word&#x27;</span>).\</span><br><span class="line">        withColumnRenamed(<span class="string">&#x27;count&#x27;</span>,<span class="string">&#x27;cnt&#x27;</span>).orderBy(<span class="string">&#x27;cnt&#x27;</span>,ascending=<span class="literal">False</span>).show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">+------+---+</span><br><span class="line">|  word|cnt|</span><br><span class="line">+------+---+</span><br><span class="line">| hello|  <span class="number">3</span>|</span><br><span class="line">| spark|  <span class="number">1</span>|</span><br><span class="line">| flink|  <span class="number">1</span>|</span><br><span class="line">|hadoop|  <span class="number">1</span>|</span><br><span class="line">+------+---+</span><br><span class="line"></span><br><span class="line">+------+</span><br><span class="line">| value|</span><br><span class="line">+------+</span><br><span class="line">| hello|</span><br><span class="line">| spark|</span><br><span class="line">| hello|</span><br><span class="line">|hadoop|</span><br><span class="line">| hello|</span><br><span class="line">| flink|</span><br><span class="line">+------+</span><br><span class="line"></span><br><span class="line">+------+---+</span><br><span class="line">|  word|cnt|</span><br><span class="line">+------+---+</span><br><span class="line">| hello|  <span class="number">3</span>|</span><br><span class="line">| spark|  <span class="number">1</span>|</span><br><span class="line">|hadoop|  <span class="number">1</span>|</span><br><span class="line">| flink|  <span class="number">1</span>|</span><br><span class="line">+------+---+</span><br></pre></td></tr></table></figure>

<h3 id="电影评分-分析案例"><a href="#电影评分-分析案例" class="headerlink" title="电影评分 分析案例"></a>电影评分 分析案例</h3><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2020.36.42.jpg" alt="截屏2022-04-15 20.36.42"></p>
<p>​		</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>).master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    schema = StructType().add(<span class="string">&#x27;user_id&#x27;</span>,StringType(),nullable=<span class="literal">True</span>).\</span><br><span class="line">        add(<span class="string">&#x27;movie_id&#x27;</span>,IntegerType(),nullable=<span class="literal">True</span>).\</span><br><span class="line">        add(<span class="string">&#x27;rank&#x27;</span>,IntegerType(),nullable=<span class="literal">True</span>).\</span><br><span class="line">        add(<span class="string">&#x27;time&#x27;</span>,StringType(),nullable=<span class="literal">True</span>)</span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;csv&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;sep&#x27;</span>,<span class="string">&#x27;\t&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;header&#x27;</span>,<span class="literal">False</span>).\</span><br><span class="line">        option(<span class="string">&#x27;encoding&#x27;</span>,<span class="string">&#x27;utf-8&#x27;</span>).\</span><br><span class="line">        schema(schema=schema).\</span><br><span class="line">        load(<span class="string">&#x27;/home/chenyushao/Documents/spark测试数据/sql/u.data&#x27;</span>)</span><br><span class="line"><span class="comment"># 需求一：</span></span><br><span class="line"><span class="comment"># 查看用户平均分</span></span><br><span class="line"><span class="comment"># df.groupBy(&#x27;user_id&#x27;).avg(&#x27;rank&#x27;).show()</span></span><br><span class="line">    df.groupBy(<span class="string">&#x27;user_id&#x27;</span>).avg(<span class="string">&#x27;rank&#x27;</span>).\</span><br><span class="line">  			withColumn(<span class="string">&#x27;avg(rank)&#x27;</span>,F.<span class="built_in">round</span>(<span class="string">&#x27;avg(rank)&#x27;</span>,<span class="number">2</span>)).\</span><br><span class="line">        orderBy(<span class="string">&#x27;avg(rank)&#x27;</span>,ascending=<span class="literal">False</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需求二：</span></span><br><span class="line"><span class="comment"># 电影平均分</span></span><br><span class="line"><span class="comment"># df.groupBy(&#x27;movie_id&#x27;).avg(&#x27;rank&#x27;).\</span></span><br><span class="line">    <span class="comment">#     withColumn(&#x27;avg(rank)&#x27;,F.round(&#x27;avg(rank)&#x27;,2)).\</span></span><br><span class="line">    <span class="comment">#     orderBy(&#x27;avg(rank)&#x27;,ascending=False).show()</span></span><br><span class="line">    df.createTempView(<span class="string">&#x27;movie&#x27;</span>)</span><br><span class="line">    spark.sql(<span class="string">&#x27;select movie_id,round(avg(rank),2) as a_r from movie group by movie_id order by a_r DESC&#x27;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需求三：</span></span><br><span class="line"><span class="comment"># 查询大于平均分电影的数量</span></span><br><span class="line">    df.select(F.avg(df[<span class="string">&#x27;rank&#x27;</span>])).show()</span><br><span class="line">    num = df.where( df[<span class="string">&#x27;rank&#x27;</span>] &gt; df.select(F.avg(df[<span class="string">&#x27;rank&#x27;</span>])).first()[<span class="string">&#x27;avg(rank)&#x27;</span>]).count()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;大于平均分电影的数量: &#x27;</span>,num )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需求四：</span></span><br><span class="line"><span class="comment"># 查询rank&gt;3分的电影中，打分次数最多的用户，此人打分的平均分。</span></span><br><span class="line">    user_id = df.where(df[<span class="string">&#x27;rank&#x27;</span>]&gt;<span class="number">3</span>).groupBy(<span class="string">&#x27;user_id&#x27;</span>).count().\</span><br><span class="line">        orderBy(<span class="string">&#x27;count&#x27;</span>,ascending=<span class="literal">False</span>).\</span><br><span class="line">        first()[<span class="string">&#x27;user_id&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(user_id)</span><br><span class="line">    df.<span class="built_in">filter</span>(df[<span class="string">&#x27;user_id&#x27;</span>]==user_id).\</span><br><span class="line">    select(F.<span class="built_in">round</span>(F.avg(df[<span class="string">&#x27;rank&#x27;</span>]),<span class="number">2</span>)).show()</span><br><span class="line"><span class="comment"># 需求五：</span></span><br><span class="line"><span class="comment"># 查询每个用户的平均打分、最低打分、最高打分。（多个聚合函数同时使用，用到agg方法）</span></span><br><span class="line">    df.groupBy(<span class="string">&#x27;user_id&#x27;</span>).\</span><br><span class="line">        agg(</span><br><span class="line">        F.<span class="built_in">round</span>(F.avg(df[<span class="string">&#x27;rank&#x27;</span>]),<span class="number">2</span>).alias(<span class="string">&#x27;avg_rank&#x27;</span>),</span><br><span class="line">        F.<span class="built_in">min</span>(df[<span class="string">&#x27;rank&#x27;</span>]).alias(<span class="string">&#x27;min_rank&#x27;</span>),</span><br><span class="line">        F.<span class="built_in">max</span>(<span class="string">&#x27;rank&#x27;</span>).alias(<span class="string">&#x27;max_rank&#x27;</span>)</span><br><span class="line">    ).show()</span><br><span class="line"><span class="comment"># 需求六：</span></span><br><span class="line"><span class="comment"># 查询评分 超过100次的电影，平均分排名 前10的.</span></span><br><span class="line">    df.groupBy(<span class="string">&#x27;movie_id&#x27;</span>).\</span><br><span class="line">        agg(</span><br><span class="line">        F.count(<span class="string">&#x27;rank&#x27;</span>).alias(<span class="string">&#x27;cnt&#x27;</span>),</span><br><span class="line">        F.<span class="built_in">round</span>(F.avg(df[<span class="string">&#x27;rank&#x27;</span>]),<span class="number">2</span>).alias(<span class="string">&#x27;avg_rank&#x27;</span>)</span><br><span class="line">    ).where(<span class="string">&#x27;cnt &gt; 100&#x27;</span>).\</span><br><span class="line">        orderBy(<span class="string">&#x27;avg_rank&#x27;</span>,ascending=<span class="literal">False</span>).\</span><br><span class="line">        limit(<span class="number">10</span>).show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">+-------+---------+</span><br><span class="line">|user_id|avg(rank)|</span><br><span class="line">+-------+---------+</span><br><span class="line">|    <span class="number">849</span>|     <span class="number">4.87</span>|</span><br><span class="line">|    <span class="number">688</span>|     <span class="number">4.83</span>|</span><br><span class="line">|    <span class="number">507</span>|     <span class="number">4.72</span>|</span><br><span class="line">|    <span class="number">628</span>|      <span class="number">4.7</span>|</span><br><span class="line">|    <span class="number">928</span>|     <span class="number">4.69</span>|</span><br><span class="line">|    <span class="number">118</span>|     <span class="number">4.66</span>|</span><br><span class="line">|    <span class="number">907</span>|     <span class="number">4.57</span>|</span><br><span class="line">|    <span class="number">686</span>|     <span class="number">4.56</span>|</span><br><span class="line">|    <span class="number">427</span>|     <span class="number">4.55</span>|</span><br><span class="line">|    <span class="number">565</span>|     <span class="number">4.54</span>|</span><br><span class="line">|    <span class="number">469</span>|     <span class="number">4.53</span>|</span><br><span class="line">|    <span class="number">850</span>|     <span class="number">4.53</span>|</span><br><span class="line">|    <span class="number">225</span>|     <span class="number">4.52</span>|</span><br><span class="line">|    <span class="number">330</span>|      <span class="number">4.5</span>|</span><br><span class="line">|    <span class="number">477</span>|     <span class="number">4.46</span>|</span><br><span class="line">|    <span class="number">636</span>|     <span class="number">4.45</span>|</span><br><span class="line">|    <span class="number">242</span>|     <span class="number">4.45</span>|</span><br><span class="line">|    <span class="number">583</span>|     <span class="number">4.44</span>|</span><br><span class="line">|    <span class="number">252</span>|     <span class="number">4.43</span>|</span><br><span class="line">|    <span class="number">767</span>|     <span class="number">4.43</span>|</span><br><span class="line">+-------+---------+</span><br><span class="line">only showing top <span class="number">20</span> rows</span><br><span class="line"></span><br><span class="line">+--------+----+</span><br><span class="line">|movie_id| a_r|</span><br><span class="line">+--------+----+</span><br><span class="line">|    <span class="number">1653</span>| <span class="number">5.0</span>|</span><br><span class="line">|    <span class="number">1122</span>| <span class="number">5.0</span>|</span><br><span class="line">|    <span class="number">1467</span>| <span class="number">5.0</span>|</span><br><span class="line">|    <span class="number">1201</span>| <span class="number">5.0</span>|</span><br><span class="line">|    <span class="number">1189</span>| <span class="number">5.0</span>|</span><br><span class="line">|    <span class="number">1293</span>| <span class="number">5.0</span>|</span><br><span class="line">|    <span class="number">1599</span>| <span class="number">5.0</span>|</span><br><span class="line">|    <span class="number">1536</span>| <span class="number">5.0</span>|</span><br><span class="line">|     <span class="number">814</span>| <span class="number">5.0</span>|</span><br><span class="line">|    <span class="number">1500</span>| <span class="number">5.0</span>|</span><br><span class="line">|    <span class="number">1449</span>|<span class="number">4.63</span>|</span><br><span class="line">|    <span class="number">1398</span>| <span class="number">4.5</span>|</span><br><span class="line">|    <span class="number">1594</span>| <span class="number">4.5</span>|</span><br><span class="line">|     <span class="number">119</span>| <span class="number">4.5</span>|</span><br><span class="line">|    <span class="number">1642</span>| <span class="number">4.5</span>|</span><br><span class="line">|     <span class="number">408</span>|<span class="number">4.49</span>|</span><br><span class="line">|     <span class="number">169</span>|<span class="number">4.47</span>|</span><br><span class="line">|     <span class="number">318</span>|<span class="number">4.47</span>|</span><br><span class="line">|     <span class="number">483</span>|<span class="number">4.46</span>|</span><br><span class="line">|      <span class="number">64</span>|<span class="number">4.45</span>|</span><br><span class="line">+--------+----+</span><br><span class="line">only showing top <span class="number">20</span> rows</span><br><span class="line"></span><br><span class="line">+---------+</span><br><span class="line">|avg(rank)|</span><br><span class="line">+---------+</span><br><span class="line">|  <span class="number">3.52986</span>|</span><br><span class="line">+---------+</span><br><span class="line"></span><br><span class="line">大于平均分电影的数量:  <span class="number">55375</span></span><br><span class="line"><span class="number">450</span></span><br><span class="line">+-------------------+</span><br><span class="line">|<span class="built_in">round</span>(avg(rank), <span class="number">2</span>)|</span><br><span class="line">+-------------------+</span><br><span class="line">|               <span class="number">3.86</span>|</span><br><span class="line">+-------------------+</span><br><span class="line"></span><br><span class="line">+-------+--------+--------+--------+</span><br><span class="line">|user_id|avg_rank|min_rank|max_rank|</span><br><span class="line">+-------+--------+--------+--------+</span><br><span class="line">|    <span class="number">296</span>|    <span class="number">4.18</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">467</span>|    <span class="number">3.68</span>|       <span class="number">2</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">691</span>|    <span class="number">4.22</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">675</span>|    <span class="number">3.71</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">829</span>|    <span class="number">3.55</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">125</span>|    <span class="number">3.44</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">451</span>|    <span class="number">2.73</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">800</span>|    <span class="number">3.75</span>|       <span class="number">2</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">853</span>|    <span class="number">2.98</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">666</span>|    <span class="number">3.67</span>|       <span class="number">2</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">870</span>|    <span class="number">3.45</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">919</span>|    <span class="number">3.47</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">926</span>|     <span class="number">3.3</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|      <span class="number">7</span>|    <span class="number">3.97</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">124</span>|     <span class="number">3.5</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|     <span class="number">51</span>|    <span class="number">3.57</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">447</span>|     <span class="number">3.6</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">591</span>|    <span class="number">3.65</span>|       <span class="number">2</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">307</span>|    <span class="number">3.79</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">|    <span class="number">475</span>|     <span class="number">3.6</span>|       <span class="number">1</span>|       <span class="number">5</span>|</span><br><span class="line">+-------+--------+--------+--------+</span><br><span class="line">only showing top <span class="number">20</span> rows</span><br><span class="line"></span><br><span class="line">+--------+---+--------+</span><br><span class="line">|movie_id|cnt|avg_rank|</span><br><span class="line">+--------+---+--------+</span><br><span class="line">|     <span class="number">408</span>|<span class="number">112</span>|    <span class="number">4.49</span>|</span><br><span class="line">|     <span class="number">318</span>|<span class="number">298</span>|    <span class="number">4.47</span>|</span><br><span class="line">|     <span class="number">169</span>|<span class="number">118</span>|    <span class="number">4.47</span>|</span><br><span class="line">|     <span class="number">483</span>|<span class="number">243</span>|    <span class="number">4.46</span>|</span><br><span class="line">|      <span class="number">64</span>|<span class="number">283</span>|    <span class="number">4.45</span>|</span><br><span class="line">|      <span class="number">12</span>|<span class="number">267</span>|    <span class="number">4.39</span>|</span><br><span class="line">|     <span class="number">603</span>|<span class="number">209</span>|    <span class="number">4.39</span>|</span><br><span class="line">|      <span class="number">50</span>|<span class="number">583</span>|    <span class="number">4.36</span>|</span><br><span class="line">|     <span class="number">178</span>|<span class="number">125</span>|    <span class="number">4.34</span>|</span><br><span class="line">|     <span class="number">357</span>|<span class="number">264</span>|    <span class="number">4.29</span>|</span><br><span class="line">+--------+---+--------+</span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2022.08.05.jpg" alt="截屏2022-04-15 22.08.05"></p>
<h3 id="sparkSQL-shuffle-分区数目"><a href="#sparkSQL-shuffle-分区数目" class="headerlink" title="sparkSQL shuffle 分区数目"></a>sparkSQL shuffle 分区数目</h3><p>​		在前面电影评分的案例中，加上<code>time.sleep(10000)</code> ,再在4040端口查看。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2022.13.32.jpg" alt="截屏2022-04-15 22.13.32"></p>
<p>​		task 也就是线程默认200个，分区有200个，太多了。最多的task 数目匹配机器的CPU核心数目，效率才能最大化。</p>
<p>​		注意：<code>config(&#39;spark.sql.shuffle.partitions&#39;,&#39;2&#39;)</code> 这个设置和rdd 的并行度设置 是相互独0立的！</p>
<h3 id="sparkSQL-数据清洗API"><a href="#sparkSQL-数据清洗API" class="headerlink" title="sparkSQL 数据清洗API"></a>sparkSQL 数据清洗API</h3><p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-15%2022.26.14.jpg" alt="截屏2022-04-15 22.26.14" style="zoom:50%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>). \</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>,<span class="string">&#x27;2&#x27;</span>). \</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;csv&#x27;</span>).option(<span class="string">&#x27;sep&#x27;</span>,<span class="string">&#x27;;&#x27;</span>).option(<span class="string">&#x27;header&#x27;</span>,<span class="literal">True</span>).\</span><br><span class="line">        load(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/sql/people.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># data_clear</span></span><br><span class="line">    df.dropDuplicates().show() <span class="comment"># 去重，不带参数，全部重复，就删除整行。</span></span><br><span class="line">    df.dropDuplicates([<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;job&#x27;</span>]).show() <span class="comment"># 针对指定字段 去重，指定字段一致就删除整行。</span></span><br><span class="line"></span><br><span class="line">    df.dropna().show() <span class="comment"># 去除空，不带参数，只要有空，就删除整行。</span></span><br><span class="line">    df.dropna(thresh=<span class="number">2</span>,subset=[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;age&#x27;</span>]).show() <span class="comment"># thresh指至少满足2个有效列，否则删除，subset用于指定字段。</span></span><br><span class="line"></span><br><span class="line">    df.fillna(<span class="string">&#x27;loss&#x27;</span>).show() <span class="comment"># 用 ‘loss’ 填充所有的空位置。</span></span><br><span class="line">    df.fillna(<span class="string">&#x27;N/A&#x27;</span>,subset=[<span class="string">&#x27;job&#x27;</span>]).show() <span class="comment"># 用‘n/a’填充 job字段的空位置。</span></span><br><span class="line">    df.fillna(&#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;unknow_name&#x27;</span>,<span class="string">&#x27;age&#x27;</span>:<span class="number">1</span>,<span class="string">&#x27;job&#x27;</span>:<span class="string">&#x27;work&#x27;</span>&#125;).show() <span class="comment"># 用字典，对每一个key值字段的空位置 ，以相应的value值 填充。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">+-----+----+---------+</span><br><span class="line">| name| age|      job|</span><br><span class="line">+-----+----+---------+</span><br><span class="line">|  Bob|  <span class="number">32</span>|Developer|</span><br><span class="line">| Lily|  <span class="number">11</span>|  Manager|</span><br><span class="line">|Alice|   <span class="number">9</span>|     null|</span><br><span class="line">|Jorge|  <span class="number">30</span>|Developer|</span><br><span class="line">|  Ani|  <span class="number">11</span>|Developer|</span><br><span class="line">|  Put|  <span class="number">11</span>|Developer|</span><br><span class="line">|Alice|   <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|null|  Manager|</span><br><span class="line">+-----+----+---------+</span><br><span class="line"></span><br><span class="line">+-----+----+---------+</span><br><span class="line">| name| age|      job|</span><br><span class="line">+-----+----+---------+</span><br><span class="line">|Alice|null|  Manager|</span><br><span class="line">|  Ani|  <span class="number">11</span>|Developer|</span><br><span class="line">| Lily|  <span class="number">11</span>|  Manager|</span><br><span class="line">|Jorge|  <span class="number">30</span>|Developer|</span><br><span class="line">|  Bob|  <span class="number">32</span>|Developer|</span><br><span class="line">|Alice|   <span class="number">9</span>|     null|</span><br><span class="line">|Alice|   <span class="number">9</span>|  Manager|</span><br><span class="line">+-----+----+---------+</span><br><span class="line"></span><br><span class="line">+-----+---+---------+</span><br><span class="line">| name|age|      job|</span><br><span class="line">+-----+---+---------+</span><br><span class="line">|Jorge| <span class="number">30</span>|Developer|</span><br><span class="line">|  Bob| <span class="number">32</span>|Developer|</span><br><span class="line">|  Ani| <span class="number">11</span>|Developer|</span><br><span class="line">| Lily| <span class="number">11</span>|  Manager|</span><br><span class="line">|  Put| <span class="number">11</span>|Developer|</span><br><span class="line">|Alice|  <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|  <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|  <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|  <span class="number">9</span>|  Manager|</span><br><span class="line">+-----+---+---------+</span><br><span class="line"></span><br><span class="line">+-----+---+---------+</span><br><span class="line">| name|age|      job|</span><br><span class="line">+-----+---+---------+</span><br><span class="line">|Jorge| <span class="number">30</span>|Developer|</span><br><span class="line">|  Bob| <span class="number">32</span>|Developer|</span><br><span class="line">|  Ani| <span class="number">11</span>|Developer|</span><br><span class="line">| Lily| <span class="number">11</span>|  Manager|</span><br><span class="line">|  Put| <span class="number">11</span>|Developer|</span><br><span class="line">|Alice|  <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|  <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|  <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|  <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|  <span class="number">9</span>|     null|</span><br><span class="line">+-----+---+---------+</span><br><span class="line"></span><br><span class="line">+-----+----+---------+</span><br><span class="line">| name| age|      job|</span><br><span class="line">+-----+----+---------+</span><br><span class="line">|Jorge|  <span class="number">30</span>|Developer|</span><br><span class="line">|  Bob|  <span class="number">32</span>|Developer|</span><br><span class="line">|  Ani|  <span class="number">11</span>|Developer|</span><br><span class="line">| Lily|  <span class="number">11</span>|  Manager|</span><br><span class="line">|  Put|  <span class="number">11</span>|Developer|</span><br><span class="line">|Alice|   <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|   <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|   <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|   <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|loss|  Manager|</span><br><span class="line">|Alice|   <span class="number">9</span>|     loss|</span><br><span class="line">+-----+----+---------+</span><br><span class="line"></span><br><span class="line">+-----+----+---------+</span><br><span class="line">| name| age|      job|</span><br><span class="line">+-----+----+---------+</span><br><span class="line">|Jorge|  <span class="number">30</span>|Developer|</span><br><span class="line">|  Bob|  <span class="number">32</span>|Developer|</span><br><span class="line">|  Ani|  <span class="number">11</span>|Developer|</span><br><span class="line">| Lily|  <span class="number">11</span>|  Manager|</span><br><span class="line">|  Put|  <span class="number">11</span>|Developer|</span><br><span class="line">|Alice|   <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|   <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|   <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|   <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|null|  Manager|</span><br><span class="line">|Alice|   <span class="number">9</span>|      N/A|</span><br><span class="line">+-----+----+---------+</span><br><span class="line"></span><br><span class="line">+-----+---+---------+</span><br><span class="line">| name|age|      job|</span><br><span class="line">+-----+---+---------+</span><br><span class="line">|Jorge| <span class="number">30</span>|Developer|</span><br><span class="line">|  Bob| <span class="number">32</span>|Developer|</span><br><span class="line">|  Ani| <span class="number">11</span>|Developer|</span><br><span class="line">| Lily| <span class="number">11</span>|  Manager|</span><br><span class="line">|  Put| <span class="number">11</span>|Developer|</span><br><span class="line">|Alice|  <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|  <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|  <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|  <span class="number">9</span>|  Manager|</span><br><span class="line">|Alice|  <span class="number">1</span>|  Manager|</span><br><span class="line">|Alice|  <span class="number">9</span>|     work|</span><br><span class="line">+-----+---+---------+</span><br></pre></td></tr></table></figure>

<h3 id="dataframe数据写出"><a href="#dataframe数据写出" class="headerlink" title="dataframe数据写出"></a>dataframe数据写出</h3><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-16%2016.15.54.jpg" alt="截屏2022-04-16 16.15.54"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>). \</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>,<span class="string">&#x27;2&#x27;</span>). \</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    schema = StructType().add(<span class="string">&#x27;user_id&#x27;</span>,StringType(),nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&#x27;movie_id&#x27;</span>,IntegerType(),nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&#x27;rank&#x27;</span>,IntegerType(),nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&#x27;time&#x27;</span>,StringType(),nullable=<span class="literal">True</span>)</span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;csv&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;sep&#x27;</span>,<span class="string">&#x27;\t&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;header&#x27;</span>,<span class="literal">False</span>). \</span><br><span class="line">        option(<span class="string">&#x27;encoding&#x27;</span>,<span class="string">&#x27;utf-8&#x27;</span>). \</span><br><span class="line">        schema(schema=schema). \</span><br><span class="line">        load(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/sql/u.data&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># write text style dataframe output,just have one field .</span></span><br><span class="line">    <span class="comment"># text 要注意保存为 单一 field 。</span></span><br><span class="line">    df.select(F.concat_ws(<span class="string">&#x27;\t&#x27;</span>,<span class="string">&#x27;user_id&#x27;</span>,<span class="string">&#x27;movie_id&#x27;</span>,<span class="string">&#x27;rank&#x27;</span>,<span class="string">&#x27;time&#x27;</span>)).\</span><br><span class="line">        write.mode(<span class="string">&#x27;overwrite&#x27;</span>).<span class="built_in">format</span>(<span class="string">&#x27;text&#x27;</span>).\</span><br><span class="line">        save(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/output/text&#x27;</span>)</span><br><span class="line">    <span class="comment"># write csv style dataframe output, need some optin() used to sep and header.</span></span><br><span class="line">    <span class="comment"># csv 要注意，保存时指定 分隔符 和 表头。因为前面的dataframe 已经指定了表头，所以这里也确定一下表头。	</span></span><br><span class="line">    df.write.mode(<span class="string">&#x27;overwrite&#x27;</span>).<span class="built_in">format</span>(<span class="string">&#x27;csv&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;sep&#x27;</span>,<span class="string">&#x27;;&#x27;</span>).option(<span class="string">&#x27;header&#x27;</span>,<span class="literal">True</span>).\</span><br><span class="line">        save(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/output/csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    df.write.mode(<span class="string">&#x27;overwrite&#x27;</span>).<span class="built_in">format</span>(<span class="string">&#x27;json&#x27;</span>).\</span><br><span class="line">        save(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/output/json&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    df.write.mode(<span class="string">&#x27;overwrite&#x27;</span>).<span class="built_in">format</span>(<span class="string">&#x27;parquet&#x27;</span>).\</span><br><span class="line">        save(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/output/parquet&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="读入和写出-jdbc-数据"><a href="#读入和写出-jdbc-数据" class="headerlink" title="读入和写出 jdbc 数据"></a>读入和写出 jdbc 数据</h4><p>​		读入和写出 jdbc 数据（连接mysql数据库），会稍微有一点不一样，因为涉及到登录mysql和驱动。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-16%2016.39.22.jpg" alt="截屏2022-04-16 16.39.22"></p>
<p>​		这个mysql的java驱动jar包，我们在之前hive安装中，在hive的lib包内配置好了，但是当时我们是放入hive 的lib 内  ，这个路径 spark是找不到的。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">我的mysql版本  Server version: 5.7.26 MySQL</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">mysql官方推荐mysql5.6以上 使用connector/j 8.0 ，而且在使用时需要对时区进行设置。java推荐java1.8以后都用connector/j 8.0 。</span></span><br><span class="line">cp mysql-connector-java-8.0.25.jar /opt/hive/apache-hive-3.1.2-bin/lib # 或者用finalshell直接传进hive的lib中。</span><br></pre></td></tr></table></figure>

<p>​		</p>
<p>​		（一）这里教程推荐我们把 驱动包直接 放入 conda –我们用的虚拟环境– site-packages包集合– 我们用的虚拟环境对应的jar下面。以方便spark使用jdbc。（以后别的jar包也可以这么加）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp mysql-connector-java-8.0.25.jar /opt/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/jars</span><br></pre></td></tr></table></figure>

<p>​		（二）在mysql中新建一个databases，专门来存放spark保存过来的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database spark_databases;</span><br></pre></td></tr></table></figure>

<p>​		（三）打开mysql的远程权限。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grant all privileges on *.* to &#x27;root&#x27;@&#x27;node01&#x27; identified by &#x27;自己的密码&#x27; with grant option;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>

<p>​		（四）写出 读入 jdbc 数据。数据存放在mysql中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写出 读入 jdbc 数据。数据存放在mysql中。</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>). \</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>,<span class="string">&#x27;2&#x27;</span>). \</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    schema = StructType().add(<span class="string">&#x27;user_id&#x27;</span>,StringType(),nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&#x27;movie_id&#x27;</span>,IntegerType(),nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&#x27;rank&#x27;</span>,IntegerType(),nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&#x27;time&#x27;</span>,StringType(),nullable=<span class="literal">True</span>)</span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;csv&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;sep&#x27;</span>,<span class="string">&#x27;\t&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;header&#x27;</span>,<span class="literal">False</span>). \</span><br><span class="line">        option(<span class="string">&#x27;encoding&#x27;</span>,<span class="string">&#x27;utf-8&#x27;</span>). \</span><br><span class="line">        schema(schema=schema). \</span><br><span class="line">        load(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/sql/u.data&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># mysql的默认端口是3306，可以编辑用户目录下的 .my.cnf 文件进行修改。</span></span><br><span class="line">    <span class="comment"># 这里用过3306端口登录mysql的spark_databases数据库，并开启unicode保证中文，</span></span><br><span class="line">    <span class="comment"># 设置好登录mysql的用户密码等等。</span></span><br><span class="line">    <span class="comment"># 最后以mysql中一个普通表 的形式 保存。</span></span><br><span class="line">    df.write.mode(<span class="string">&#x27;overwrite&#x27;</span>).\</span><br><span class="line">        <span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;url&#x27;</span>,<span class="string">&#x27;jdbc:mysql://node01:3306/spark_databases?useSSL=false&amp;useUnicode=true&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;dbtable&#x27;</span>,<span class="string">&#x27;movie_table&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;user&#x27;</span>,<span class="string">&#x27;root&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;password&#x27;</span>,<span class="string">&#x27;自己的密码&#x27;</span>).\</span><br><span class="line">        save()</span><br><span class="line">    <span class="comment"># 远程经3306端口 ，从mysql指定库中 选取指定表，读到 spark的dataframe 中。</span></span><br><span class="line">    df2 = spark.read. \</span><br><span class="line">        <span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;url&#x27;</span>,<span class="string">&#x27;jdbc:mysql://node01:3306/spark_databases?useSSL=false&amp;useUnicode=true&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;dbtable&#x27;</span>,<span class="string">&#x27;movie_table&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;user&#x27;</span>,<span class="string">&#x27;root&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;password&#x27;</span>,<span class="string">&#x27;自己的密码&#x27;</span>).\</span><br><span class="line">        load()</span><br><span class="line">    df2.printSchema()</span><br><span class="line">    df2.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-16%2017.53.27.jpg" alt="截屏2022-04-16 17.53.27"></p>
<h1 id="SparkSQL-自定义函数"><a href="#SparkSQL-自定义函数" class="headerlink" title="SparkSQL 自定义函数"></a>SparkSQL 自定义函数</h1><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-16%2019.21.17.jpg" alt="截屏2022-04-16 19.21.17"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-16%2019.23.45.jpg" alt="截屏2022-04-16 19.23.45"></p>
<p>​		所以，要定义udaf 、udtf 还是要用到hive帮助，或者用rdd等等模拟出udaf、udtf的效果，或者 干脆用Scala 来写。</p>
<p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-16%2021.57.07.jpg" alt="截屏2022-04-16 21.57.07"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>). \</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>,<span class="string">&#x27;2&#x27;</span>). \</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: [x])</span><br><span class="line">    df = rdd.toDF([<span class="string">&#x27;num&#x27;</span>])</span><br><span class="line">		<span class="comment"># udf 函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_ride_10</span>(<span class="params">num</span>):</span><br><span class="line">        <span class="keyword">return</span> num*<span class="number">10</span></span><br><span class="line">		<span class="comment"># 方式一 注册 udf 函数。</span></span><br><span class="line">    <span class="comment"># 参数1:注册的udf名称，仅可用于sql风格；</span></span><br><span class="line">    <span class="comment"># 参数2:udf 函数。</span></span><br><span class="line">    <span class="comment"># 参数3:udf 函数的返回值类型。</span></span><br><span class="line">    udf2 = spark.udf.register(<span class="string">&#x27;udf1&#x27;</span>,num_ride_10,IntegerType())</span><br><span class="line">		<span class="comment"># sql 风格使用</span></span><br><span class="line">    df.selectExpr(<span class="string">&#x27;udf1(num)&#x27;</span>).show()</span><br><span class="line">		<span class="comment"># dsl 风格使用</span></span><br><span class="line">    <span class="comment"># 注册的udf方法，传入参数一定是 column对象。</span></span><br><span class="line">    df.select(udf2(df[<span class="string">&#x27;num&#x27;</span>])).show()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 方式二 注册udf 函数。</span></span><br><span class="line">    <span class="comment"># 由于没有 在传参内部给定 注册udf的名字，所以没办法 以sql风格使用，只能以dsl风格使用。</span></span><br><span class="line">    udf3 = F.udf(num_ride_10,IntegerType())</span><br><span class="line">    df.select(udf3(df[<span class="string">&#x27;num&#x27;</span>])).show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|       <span class="number">10</span>|</span><br><span class="line">|       <span class="number">20</span>|</span><br><span class="line">|       <span class="number">30</span>|</span><br><span class="line">|       <span class="number">40</span>|</span><br><span class="line">|       <span class="number">50</span>|</span><br><span class="line">|       <span class="number">60</span>|</span><br><span class="line">|       <span class="number">70</span>|</span><br><span class="line">|       <span class="number">80</span>|</span><br><span class="line">|       <span class="number">90</span>|</span><br><span class="line">+---------+</span><br><span class="line"></span><br><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|       <span class="number">10</span>|</span><br><span class="line">|       <span class="number">20</span>|</span><br><span class="line">|       <span class="number">30</span>|</span><br><span class="line">|       <span class="number">40</span>|</span><br><span class="line">|       <span class="number">50</span>|</span><br><span class="line">|       <span class="number">60</span>|</span><br><span class="line">|       <span class="number">70</span>|</span><br><span class="line">|       <span class="number">80</span>|</span><br><span class="line">|       <span class="number">90</span>|</span><br><span class="line">+---------+</span><br><span class="line"><span class="comment"># 由于第二种注册 方式，没有指定注册的名称，所以直接使用了 udf函数的名称来显示。</span></span><br><span class="line">+----------------+</span><br><span class="line">|num_ride_10(num)|</span><br><span class="line">+----------------+</span><br><span class="line">|              <span class="number">10</span>|</span><br><span class="line">|              <span class="number">20</span>|</span><br><span class="line">|              <span class="number">30</span>|</span><br><span class="line">|              <span class="number">40</span>|</span><br><span class="line">|              <span class="number">50</span>|</span><br><span class="line">|              <span class="number">60</span>|</span><br><span class="line">|              <span class="number">70</span>|</span><br><span class="line">|              <span class="number">80</span>|</span><br><span class="line">|              <span class="number">90</span>|</span><br><span class="line">+----------------+</span><br></pre></td></tr></table></figure>

<h3 id="注册一个返回值为array类型的udf"><a href="#注册一个返回值为array类型的udf" class="headerlink" title="注册一个返回值为array类型的udf"></a>注册一个返回值为array类型的udf</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType,ArrayType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>). \</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>,<span class="string">&#x27;2&#x27;</span>). \</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([[<span class="string">&#x27;hadoop flink spark&#x27;</span>],[<span class="string">&#x27;java python c++&#x27;</span>]])</span><br><span class="line">    df = rdd.toDF([<span class="string">&#x27;line&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">split_line</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="keyword">return</span> data.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># use mathod 1 to register.</span></span><br><span class="line">    <span class="comment"># 注意：在返回值类型是array时，一定还要标出 array 内元素的类型。</span></span><br><span class="line">    udf2 = spark.udf.register(<span class="string">&#x27;udf1&#x27;</span>,split_line,ArrayType(StringType()))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># DSL style use</span></span><br><span class="line">    <span class="comment"># df.select(df[&#x27;line&#x27;]).show()</span></span><br><span class="line">    df.select(udf2(df[<span class="string">&#x27;line&#x27;</span>])).show()</span><br><span class="line">    <span class="comment"># sql style use</span></span><br><span class="line">    df.createTempView(<span class="string">&#x27;lines&#x27;</span>)</span><br><span class="line">    <span class="comment"># spark.sql(&#x27;select * from lines&#x27;).show()</span></span><br><span class="line">    spark.sql(<span class="string">&#x27;select udf1(line) from lines&#x27;</span>).show(truncate=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    udf3 = F.udf(split_line,ArrayType(StringType()))</span><br><span class="line">    df.select(udf3(df[<span class="string">&#x27;line&#x27;</span>])).show(truncate=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">+--------------------+</span><br><span class="line">|          udf1(line)|</span><br><span class="line">+--------------------+</span><br><span class="line">|[hadoop, flink, s...|</span><br><span class="line">| [java, python, c++]|</span><br><span class="line">+--------------------+</span><br><span class="line"><span class="comment"># 有truncate=false 就是全量显示。</span></span><br><span class="line">+----------------------+</span><br><span class="line">|udf1(line)            |</span><br><span class="line">+----------------------+</span><br><span class="line">|[hadoop, flink, spark]|</span><br><span class="line">|[java, python, c++]   |</span><br><span class="line">+----------------------+</span><br><span class="line"></span><br><span class="line">+----------------------+</span><br><span class="line">|    split_line(line)  |</span><br><span class="line">+----------------------+</span><br><span class="line">|[hadoop, flink, spark |</span><br><span class="line">| [java, python, c++]  |</span><br><span class="line">+----------------------+</span><br></pre></td></tr></table></figure>

<h3 id="注册一个返回值为dict类型的udf"><a href="#注册一个返回值为dict类型的udf" class="headerlink" title="注册一个返回值为dict类型的udf"></a>注册一个返回值为dict类型的udf</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType,ArrayType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>). \</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>,<span class="string">&#x27;2&#x27;</span>). \</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line">    df = rdd.toDF([<span class="string">&#x27;num&#x27;</span>])</span><br><span class="line">		</span><br><span class="line">    <span class="comment"># udf 函数，输入的是一个int类型的数字，返回的是一个dict。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;num&#x27;</span>:data,<span class="string">&#x27;letter&#x27;</span>:string.ascii_letters[data]&#125;</span><br><span class="line">		</span><br><span class="line">    <span class="comment"># 注册udf函数； 注意： 返回值为dict，注册时要用structtype().add(key的名称，value的类型，能否为空)来 定下注册udf返回值的结构。</span></span><br><span class="line">    udf1 = spark.udf.\</span><br><span class="line">        register(<span class="string">&#x27;udf1&#x27;</span>,process,StructType().\</span><br><span class="line">                 add(<span class="string">&#x27;num&#x27;</span>,IntegerType(),nullable=<span class="literal">True</span>).\</span><br><span class="line">                 add(<span class="string">&#x27;letter&#x27;</span>,StringType(),nullable=<span class="literal">True</span>))</span><br><span class="line">		<span class="comment"># dsl风格和sql风格的调用。</span></span><br><span class="line">    df.select(udf1(df[<span class="string">&#x27;num&#x27;</span>])).show(truncate=<span class="literal">False</span>)</span><br><span class="line">    df.selectExpr(<span class="string">&#x27;udf1(num)&#x27;</span>).show(truncate=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output </span></span><br><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|&#123;<span class="number">1</span>, b&#125;   |</span><br><span class="line">|&#123;<span class="number">2</span>, c&#125;   |</span><br><span class="line">|&#123;<span class="number">3</span>, d&#125;   |</span><br><span class="line">+---------+</span><br><span class="line"></span><br><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|&#123;<span class="number">1</span>, b&#125;   |</span><br><span class="line">|&#123;<span class="number">2</span>, c&#125;   |</span><br><span class="line">|&#123;<span class="number">3</span>, d&#125;   |</span><br><span class="line">+---------+</span><br></pre></td></tr></table></figure>

<h3 id="python绕路实现UDAF"><a href="#python绕路实现UDAF" class="headerlink" title="python绕路实现UDAF"></a>python绕路实现UDAF</h3><p>​		之前说过python不像java可以直接编写udaf方法，hive集成了相应方法都是又从spark找hive解决问题有点麻烦。</p>
<p>​		所以头铁一定要用python的前提下，选择用rdd间接实现udaf方法。</p>
<p><code>和map一次处理一个元素不同的是，mapPartion算子是整个迭代器传递。走网络的时候，一次性传输一个迭代器对象 效率一定高于一次传一个元素。mappartion 的输入和输出都是list对象。</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绕道利用rdd 的mappartition算子 ，实现 dataframe求和的 udaf聚合函数。</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType,ArrayType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>). \</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>,<span class="string">&#x27;2&#x27;</span>). \</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line">		<span class="comment"># 制作一个dataframe，只有一列，列名为 num。</span></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],<span class="number">3</span>)</span><br><span class="line">    df = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: [x]).toDF([<span class="string">&#x27;num&#x27;</span>])</span><br><span class="line">		</span><br><span class="line">    <span class="comment"># 把dataframe 转成rdd，利用rdd的mappartition算子来实现dataframe 中自定义udaf；</span></span><br><span class="line">    <span class="comment"># 注意： 先要把分区数 改为1，为了方便mappartition算子对全局计算。</span></span><br><span class="line">    single_partition_rdd = df.rdd.repartition(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(single_partition_rdd.collect())</span><br><span class="line">    <span class="comment"># [Row(num=1), Row(num=2), Row(num=3), Row(num=4), Row(num=5), Row(num=6)]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process</span>(<span class="params"><span class="built_in">iter</span></span>):</span><br><span class="line">        <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">iter</span>:    <span class="comment"># 每一个row其实是 row对象，不是一个数字。</span></span><br><span class="line">            <span class="built_in">sum</span> += row[<span class="string">&#x27;num&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">sum</span>]        <span class="comment"># mappartition算子 返回值只能是list。</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(single_partition_rdd.mapPartitions(process).collect())</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output </span></span><br><span class="line">[Row(num=<span class="number">1</span>), Row(num=<span class="number">2</span>), Row(num=<span class="number">3</span>), Row(num=<span class="number">4</span>), Row(num=<span class="number">5</span>), Row(num=<span class="number">6</span>)]</span><br><span class="line">[<span class="number">21</span>]</span><br></pre></td></tr></table></figure>

<p>​		如果好奇不把分区数改为1的话会怎么样，我们试一试，把上述代码改动。</p>
<p><code>single_partition_rdd = df.rdd</code></p>
<p>​		结果变为</p>
<p><code>[Row(num=1), Row(num=2), Row(num=3), Row(num=4), Row(num=5), Row(num=6)]</code></p>
<p><code>[3, 7, 11]</code></p>
<p>​		可见，mappartition算子对每一个分区都计算了一次，返回了一个list，最后collect只是把他们收集了起来，形成一个新的list。实际上影响了 udaf聚合的作用域。</p>
<h3 id="sparkSQL-使用窗口函数"><a href="#sparkSQL-使用窗口函数" class="headerlink" title="sparkSQL 使用窗口函数"></a>sparkSQL 使用窗口函数</h3><p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2015.54.45.jpg" alt="截屏2022-04-17 15.54.45"></p>
<p>​		这里可以回顾一下，hive学习中的内容《hive参数配置与函数、运算符的使用》-《高阶函数介绍》《窗口函数》。</p>
<p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2017.24.34.jpg" alt="截屏2022-04-17 17.24.34"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2017.29.58.jpg" alt="截屏2022-04-17 17.29.58"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType,ArrayType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>). \</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>,<span class="string">&#x27;2&#x27;</span>). \</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([</span><br><span class="line">        (<span class="string">&#x27;zhansan&#x27;</span>,<span class="string">&#x27;calss_1&#x27;</span>,<span class="number">23</span>),</span><br><span class="line">        (<span class="string">&#x27;lisi&#x27;</span>,<span class="string">&#x27;calss_1&#x27;</span>,<span class="number">20</span>),</span><br><span class="line">        (<span class="string">&#x27;wanwu&#x27;</span>,<span class="string">&#x27;calss_2&#x27;</span>,<span class="number">25</span>),</span><br><span class="line">        (<span class="string">&#x27;miaoyang&#x27;</span>,<span class="string">&#x27;calss_3&#x27;</span>,<span class="number">3</span>),</span><br><span class="line">        (<span class="string">&#x27;zhoujielun&#x27;</span>,<span class="string">&#x27;calss_2&#x27;</span>,<span class="number">45</span>),</span><br><span class="line">        (<span class="string">&#x27;zhoujie&#x27;</span>,<span class="string">&#x27;calss_2&#x27;</span>,<span class="number">50</span>),</span><br><span class="line">        (<span class="string">&#x27;liudehua&#x27;</span>,<span class="string">&#x27;calss_3&#x27;</span>,<span class="number">65</span>)</span><br><span class="line">    ])</span><br><span class="line">    schema = StructType().add(<span class="string">&#x27;name&#x27;</span>,StringType()).\</span><br><span class="line">        add(<span class="string">&#x27;class&#x27;</span>,StringType()).add(<span class="string">&#x27;score&#x27;</span>,IntegerType())</span><br><span class="line">    df = rdd.toDF(schema)</span><br><span class="line"></span><br><span class="line">    df.createTempView(<span class="string">&#x27;stu&#x27;</span>)</span><br><span class="line">    <span class="comment"># spark.sql(&#x27;select * ,avg(score) over() from stu &#x27;).show()</span></span><br><span class="line">		<span class="comment"># 可以参考hive 的窗口 函数，一样的。</span></span><br><span class="line">    spark.sql(<span class="string">&#x27;&#x27;&#x27;select * ,row_number() over(order by score) as row_number_rank,</span></span><br><span class="line"><span class="string">              dense_rank() over(partition by class order by score DESC) as dense_rank,</span></span><br><span class="line"><span class="string">              rank() over(order by score) as rank from stu  &#x27;&#x27;&#x27;</span>).show()</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&#x27;&#x27;&#x27; select * ,ntile(6) over(order by score desc) as ntile from stu&#x27;&#x27;&#x27;</span>).show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">+----------+-------+-----+---------------+----------+----+</span><br><span class="line">|      name|  <span class="keyword">class</span>|score|row_number_rank|dense_rank|rank|</span><br><span class="line">+----------+-------+-----+---------------+----------+----+</span><br><span class="line">|   zhansan|calss_1|   <span class="number">23</span>|              <span class="number">3</span>|         <span class="number">1</span>|   <span class="number">3</span>|</span><br><span class="line">|      lisi|calss_1|   <span class="number">20</span>|              <span class="number">2</span>|         <span class="number">2</span>|   <span class="number">2</span>|</span><br><span class="line">|   zhoujie|calss_2|   <span class="number">50</span>|              <span class="number">6</span>|         <span class="number">1</span>|   <span class="number">6</span>|</span><br><span class="line">|zhoujielun|calss_2|   <span class="number">45</span>|              <span class="number">5</span>|         <span class="number">2</span>|   <span class="number">5</span>|</span><br><span class="line">|     wanwu|calss_2|   <span class="number">25</span>|              <span class="number">4</span>|         <span class="number">3</span>|   <span class="number">4</span>|</span><br><span class="line">|  liudehua|calss_3|   <span class="number">65</span>|              <span class="number">7</span>|         <span class="number">1</span>|   <span class="number">7</span>|</span><br><span class="line">|  miaoyang|calss_3|    <span class="number">3</span>|              <span class="number">1</span>|         <span class="number">2</span>|   <span class="number">1</span>|</span><br><span class="line">+----------+-------+-----+---------------+----------+----+</span><br><span class="line"></span><br><span class="line">+----------+-------+-----+-----+</span><br><span class="line">|      name|  <span class="keyword">class</span>|score|ntile|</span><br><span class="line">+----------+-------+-----+-----+</span><br><span class="line">|  liudehua|calss_3|   <span class="number">65</span>|    <span class="number">1</span>|</span><br><span class="line">|   zhoujie|calss_2|   <span class="number">50</span>|    <span class="number">1</span>|</span><br><span class="line">|zhoujielun|calss_2|   <span class="number">45</span>|    <span class="number">2</span>|</span><br><span class="line">|     wanwu|calss_2|   <span class="number">25</span>|    <span class="number">3</span>|</span><br><span class="line">|   zhansan|calss_1|   <span class="number">23</span>|    <span class="number">4</span>|</span><br><span class="line">|      lisi|calss_1|   <span class="number">20</span>|    <span class="number">5</span>|</span><br><span class="line">|  miaoyang|calss_3|    <span class="number">3</span>|    <span class="number">6</span>|</span><br><span class="line">+----------+-------+-----+-----+</span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2017.48.44.jpg" alt="截屏2022-04-17 17.48.44"></p>
<h1 id="SparkSQL-执行流程"><a href="#SparkSQL-执行流程" class="headerlink" title="SparkSQL 执行流程"></a>SparkSQL 执行流程</h1><p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2021.56.10.jpg" alt="截屏2022-04-17 21.56.10"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2021.58.10.jpg" alt="截屏2022-04-17 21.58.10"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2021.59.22.jpg" alt="截屏2022-04-17 21.59.22"></p>
<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.02.01.jpg" alt="截屏2022-04-17 22.02.01" style="zoom:50%;">

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.06.37.jpg" alt="截屏2022-04-17 22.06.37"></p>
<p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.08.43.jpg" alt="截屏2022-04-17 22.08.43" style="zoom:50%;"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.13.23.jpg" alt="截屏2022-04-17 22.13.23"></p>
<p>​		其实 predicate pushdown 断言下推（过滤前置）和  column pruning 列值裁剪 的核心思想都是 在join 之前 ，尽可能缩小数据规模，减少笛卡尔积。</p>
<p>​		其中列值裁剪 配合 parquet列式存储格式，会有很好的效果，因为以列为单位，直接可以不读取select 之外的column，仅仅保留select 会用到的column。</p>
<p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.21.34.jpg" alt="截屏2022-04-17 22.21.34"></p>
<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.22.49.jpg" alt="截屏2022-04-17 22.22.49" style="zoom:50%;">

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.25.24.jpg" alt="截屏2022-04-17 22.25.24"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.27.10.jpg" alt="截屏2022-04-17 22.27.10"></p>
<h1 id="Spark-on-Hive"><a href="#Spark-on-Hive" class="headerlink" title="Spark on Hive"></a>Spark on Hive</h1><p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.43.37.jpg" alt="截屏2022-04-17 22.43.37" style="zoom:50%;"></p>
<p> <img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.47.15.jpg" alt="截屏2022-04-17 22.47.15"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.47.58.jpg" alt="截屏2022-04-17 22.47.58"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-17%2022.49.24.jpg" alt="截屏2022-04-17 22.49.24"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤一： 来到spark的conf 文件夹内。新建一个 hive-site.xml</span></span><br><span class="line">cd /opt/spark/spark/conf</span><br><span class="line">vim hive-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;!--# 配置metastore的默认hdfs文件夹位置 --&gt;</span><br><span class="line">    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;!-- # 配置 metastore 的连接协议和端口，我的hive在node01机器上 --&gt;</span><br><span class="line">    &lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;thrift://node01:9083&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤二： 给spark 导入 驱动mysql 的jar包。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（之前hive配置和sparksql 读写jdbc数据都导入过mysql的java驱动包，都是位置一个是在hive中一个是在conda的pyspark虚拟环境中，这次我们直接给spark内导入mysql的java驱动包）</span></span><br><span class="line">cd /opt/spark/spark/jars</span><br><span class="line">ll | grep mysql # 可见现在还没有mysql的java驱动包。</span><br><span class="line">cp mysql-connector-java-8.0.25.jar /opt/spark/spark/jars/ </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤三：在hive中 配置metastore的 属性，配置metasore服务(以thrift协议打开9083端口)。</span></span><br><span class="line">cd /opt/hive/apache-hive-3.1.2-bin/conf</span><br><span class="line">vim hive-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!--远程模式部署metastore服务地址,hive就安装在node01上，用thrift协议对外，端口</span><br><span class="line">9083 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;thrift://node01:9083&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤四：打开 metastore 服务。</span></span><br><span class="line">cd /opt/hive/apache-hive-3.1.2-bin/bin</span><br><span class="line">nohup hive --service metastore &amp;</span><br><span class="line">netstat -anp | grep 9083 # 可见 listen 此端口已经被监听了。</span><br></pre></td></tr></table></figure>

<p>​		经过了上面的配置，且开启了hive 的 metastore 服务之后。我们可以试一试直接在spark的pyspark内 调用元数据写代码了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（一）在pyspark内 实验 spark on hive</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">先在Hadoop开启hdfs服务。（hdfs所有机器都打开）</span></span><br><span class="line">cd /opt/hadoop/hadoop/sbin</span><br><span class="line">start-all.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入虚拟环境 ，打开pyspark。</span></span><br><span class="line">conda activate pyspark</span><br><span class="line">cd /opt/spark/spark/bin</span><br><span class="line">pyspark </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在pyspark 内直接用sql语句。新建一个表。</span></span><br><span class="line">spark.sql(&#x27;create table sparkinhive(id int)&#x27;)</span><br><span class="line">spark.sql(&#x27;create table students(id int,name string)&#x27;)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">若不用 pyspark ，直接用 spark-sql ，可以直接用sql语句去操作。</span></span><br><span class="line">spark-sql</span><br><span class="line">insert into students values(1,&#x27;zhangsan&#x27;),(2,&#x27;wanwu&#x27;);</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">我们回到hive 中，检查是否出现了 一个叫 sparkinhive的表。</span></span><br><span class="line">hive</span><br><span class="line">show tables; </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可见刚刚用sparksql 新建的 sparkinhive 表,也在hive 默认的库中。这说明 sparksql和hive 公用了一套元数据metastore。但是 sparksql 底层是用 spark 的rdd来计算的，而hive 底层是 mapreduce 来计算的。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># （二）在代码内 实验 spark on hive</span></span><br><span class="line"><span class="comment"># 在sparksession内添加几个config信息。</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType,IntegerType,StringType,ArrayType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># config 定义好 hive 原数据的默认位置，以及用thrift协议打开元数据的远程连接端口9083，以及打开hivesupport。</span></span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>). \</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.warehouse.dir&#x27;</span>,<span class="string">&#x27;hdfs://node01:9000/user/hive/warehouse&#x27;</span>).\</span><br><span class="line">        config(<span class="string">&#x27;hive.metastore.uris&#x27;</span>,<span class="string">&#x27;thrift://node01:9083&#x27;</span>).\</span><br><span class="line">        enableHiveSupport().\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&#x27;select * from students&#x27;</span>).show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output 从输出中可见 元数据是 共用的。</span></span><br><span class="line">+---+--------+</span><br><span class="line">| <span class="built_in">id</span>|    name|</span><br><span class="line">+---+--------+</span><br><span class="line">|  <span class="number">1</span>|zhangsan|</span><br><span class="line">|  <span class="number">2</span>|   wanwu|</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure>



<h1 id="分布式SQL-执行引擎"><a href="#分布式SQL-执行引擎" class="headerlink" title="分布式SQL 执行引擎"></a>分布式SQL 执行引擎</h1><p>（简述就是降低门槛，仅用sql操作sparkSQL）</p>
<p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-18%2019.22.53.jpg" alt="截屏2022-04-18 19.22.53"></p>
<p>​		其实类似于 hive的 hiveserver2 。就是让纯sql 代码调用 metastore 更加方便直观。</p>
<p>​		在已经配置好 spark on hive 之后，只需要启动 <code>ThriftServer</code> 即可 实现 <code>hiveserver2</code>一样的效果。	</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/spark/spark/sbin</span><br><span class="line">start-thriftserver.sh  --hiveconf hive.server2.thrift.port=10000  --hiveconf hive.server2.thrift.bind.host=node01 --master local[*]</span><br><span class="line"></span><br><span class="line">netstat -anp | grep 10000 # 可以看见10000端口正在被监听。</span><br></pre></td></tr></table></figure>

<p>​		thriftserver 服务打开之后， 就像之前hive 学习时，用idea利用hiveserver2 登录元数据库一样，thriftserver 用法一样，一些软件可利用它连接到 元数据库，比如 datagrip、heidisql等等软件，目的是实现 纯 sql 编程，但是底层是spark的 rdd在跑，保证了 性能 与 编程的简单。</p>
<p>​		下载一个pyhive ，就可以用 jdbc 协议 连接 thriftserver。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum -y install cyrus-sasl cyrus-sasl-devel cyrus-sasl-lib</span><br><span class="line">pip install sasl -i  https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">pip install thrift -i  https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">pip install thrift-sasl -i  https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">pip install pyhive -i  https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyhive <span class="keyword">import</span> hive</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">		<span class="comment"># 通过 thriftserver 连接到 hive。</span></span><br><span class="line">    conn = hive.Connection(host=<span class="string">&#x27;node01&#x27;</span>,port=<span class="number">10000</span>,username=<span class="string">&#x27;root&#x27;</span> )</span><br><span class="line">    <span class="comment"># 获取游标对象，jdbc的经典写法。</span></span><br><span class="line">    cursor = conn.cursor()</span><br><span class="line">		<span class="comment"># 执行sql</span></span><br><span class="line">    cursor.execute(<span class="string">&#x27;select * from students&#x27;</span>)</span><br><span class="line">		<span class="comment"># 通过fetchall 获得返回值。</span></span><br><span class="line">    result = cursor.fetchall()</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[(<span class="number">1</span>, <span class="string">&#x27;zhangsan&#x27;</span>), (<span class="number">2</span>, <span class="string">&#x27;wanwu&#x27;</span>)]</span><br></pre></td></tr></table></figure>

<p>​		实际上就是 让仅仅懂sql 的人也能 用上 spark 的计算 和 hive 的元数据。降低使用门槛。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-18%2021.06.57.jpg" alt="截屏2022-04-18 21.06.57"></p>
<h1 id="综合练习"><a href="#综合练习" class="headerlink" title="综合练习"></a>综合练习</h1><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-19%2012.38.03.jpg" alt="截屏2022-04-19 12.38.03"></p>
<p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-19%2012.40.12.jpg" alt="截屏2022-04-19 12.40.12" style="zoom:50%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">demand 1: 每个省份销售额统计</span></span><br><span class="line"><span class="string">demand 2: top3 销售额的省份中，有多少家店铺 存在日销售额&gt;1000的情形？</span></span><br><span class="line"><span class="string">demand 3: top3 销售额的省份中，平均单价。</span></span><br><span class="line"><span class="string">demand 4: top3 销售额的省份中，各个省份支付类型比例。 </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">receivalbe: 销售额</span></span><br><span class="line"><span class="string">storeProvince:</span></span><br><span class="line"><span class="string">dateTS: 时间</span></span><br><span class="line"><span class="string">payType: 支付类型</span></span><br><span class="line"><span class="string">storeID: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">操作： 写出结果到mysql；</span></span><br><span class="line"><span class="string">	  写出结果到 hive库。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;sparksql_example&#x27;</span>). \</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>). \</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>,<span class="string">&#x27;2&#x27;</span>). \</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.warehouse.dir&#x27;</span>,<span class="string">&#x27;hdfs://node01:9000/user/hive/warehouse&#x27;</span>). \</span><br><span class="line">        config(<span class="string">&#x27;hive.metastore.uris&#x27;</span>,<span class="string">&#x27;thrift://node01:9083&#x27;</span>). \</span><br><span class="line">        enableHiveSupport(). \</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;json&#x27;</span>). \</span><br><span class="line">        load(<span class="string">&#x27;file:///home/chenyushao/Documents/spark测试数据/minimini.json&#x27;</span>). \</span><br><span class="line">        dropna(thresh=<span class="number">1</span>,subset=[<span class="string">&#x27;storeProvince&#x27;</span>]). \</span><br><span class="line">        <span class="built_in">filter</span>(<span class="string">&quot;storeProvince != &#x27;null&#x27;&quot;</span>). \</span><br><span class="line">        select(<span class="string">&#x27;storeProvince&#x27;</span>,<span class="string">&#x27;storeID&#x27;</span>,<span class="string">&#x27;receivable&#x27;</span>,<span class="string">&#x27;dateTS&#x27;</span>,<span class="string">&#x27;payType&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># demand 1:</span></span><br><span class="line">    province_sale_df = df.groupBy(<span class="string">&#x27;storeProvince&#x27;</span>).<span class="built_in">sum</span>(<span class="string">&#x27;receivable&#x27;</span>). \</span><br><span class="line">        withColumnRenamed(<span class="string">&#x27;sum(receivable)&#x27;</span>,<span class="string">&#x27;money&#x27;</span>). \</span><br><span class="line">        withColumn(<span class="string">&#x27;money&#x27;</span>,F.<span class="built_in">round</span>(<span class="string">&#x27;money&#x27;</span>,<span class="number">2</span>)). \</span><br><span class="line">        orderBy(<span class="string">&#x27;money&#x27;</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">    province_sale_df.show()</span><br><span class="line">    <span class="comment"># write to mysql</span></span><br><span class="line">    province_sale_df.write.mode(<span class="string">&#x27;overwrite&#x27;</span>).\</span><br><span class="line">        <span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;url&#x27;</span>,<span class="string">&#x27;jdbc:mysql://node01:3306/spark_databases?useSSL=false&amp;useUnicode=true&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;dbtable&#x27;</span>,<span class="string">&#x27;province_sale_table&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;user&#x27;</span>,<span class="string">&#x27;root&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;password&#x27;</span>,<span class="string">&#x27;密码&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;encoding&#x27;</span>,<span class="string">&#x27;utf-8&#x27;</span>).\</span><br><span class="line">        save()</span><br><span class="line">    <span class="comment"># write to hive. sparksession have been config hive&#x27;s information,so just need to saveastable.</span></span><br><span class="line">    province_sale_df.write.mode(<span class="string">&#x27;overwrite&#x27;</span>).saveAsTable(<span class="string">&#x27;province_sale_table&#x27;</span>,<span class="string">&#x27;parquet&#x27;</span>)</span><br><span class="line">		<span class="comment"># 配置好 spark on hive ，直接在spark-sql内可看到province_sale_table（正常select即可，与parquet格式无关。）。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># demand 2:</span></span><br><span class="line"><span class="comment"># 湖南省 id=1 12日 1200</span></span><br><span class="line"><span class="comment"># 湖南省 id=1 13日 1800</span></span><br><span class="line"><span class="comment"># 湖南省 id=1 12号 2200</span></span><br><span class="line"><span class="comment"># 广东省 id=1 13日 890</span></span><br><span class="line"><span class="comment"># 广西省 id=12 15日 4000</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># 先找到top3的销售省份，再找 日销售额，再找 存在日销售额 大于 1000的商家数目，以省份为分组统计。</span></span><br><span class="line"><span class="comment"># 题目表述有误，不求是日均销售额 大于 1000的商家数目，而是求 日销售额 大于 1000 的商家数目（只要有一天日销售额大于1000，这样的商家就记入）。</span></span><br><span class="line">    top3_province_df = province_sale_df.limit(<span class="number">3</span>).select(<span class="string">&#x27;storeProvince&#x27;</span>).\</span><br><span class="line">        withColumnRenamed(<span class="string">&#x27;storeProvince&#x27;</span>,<span class="string">&#x27;top3_storeProvince&#x27;</span>) <span class="comment"># 改个column名以防 join后的select出现 重名 模棱两可。</span></span><br><span class="line"></span><br><span class="line">    top3_province_joined_df = df.join(top3_province_df,on = df[<span class="string">&#x27;storeProvince&#x27;</span>] ==top3_province_df[<span class="string">&#x27;top3_storeProvince&#x27;</span>])</span><br><span class="line">    top3_province_joined_df.show()</span><br><span class="line">    top3_province_joined_df.persist( StorageLevel.MEMORY_AND_DISK) <span class="comment"># demand 3还会用到，所以这里做个持久化处理。记得代码尾部unpersist().</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 给每一家店铺 每天的 销量求和，过滤出大于1000的，如果有12日、13日 这样多日销量大于1000的，要去重，因为其实是一家店。</span></span><br><span class="line">    <span class="comment"># 最后以 省来分组 统计 这样的店的数量。</span></span><br><span class="line">    <span class="comment"># 注意：unixtime 10位才是秒级，文件是毫秒级，取10位成毫秒级再转换显示。</span></span><br><span class="line">    <span class="comment"># 注意：（）内的column用alias改列名，（）外的dataframe 用withcolumnrenamed 改列名。</span></span><br><span class="line">    province_over1000_storenum = top3_province_joined_df.groupBy(<span class="string">&#x27;storeProvince&#x27;</span>,<span class="string">&#x27;storeID&#x27;</span>,</span><br><span class="line">                                    F.from_unixtime(df[<span class="string">&#x27;dateTS&#x27;</span>].substr(<span class="number">0</span>,<span class="number">10</span>),<span class="string">&#x27;yyyy-MM-dd&#x27;</span>).alias(<span class="string">&#x27;day&#x27;</span>)).\</span><br><span class="line">        <span class="built_in">sum</span>(<span class="string">&#x27;receivable&#x27;</span>).withColumnRenamed(<span class="string">&#x27;sum(receivable)&#x27;</span>,<span class="string">&#x27;money&#x27;</span>).\</span><br><span class="line">        <span class="built_in">filter</span>(<span class="string">&#x27;money &gt; 1000&#x27;</span>).\</span><br><span class="line">        dropDuplicates(subset=[<span class="string">&#x27;storeProvince&#x27;</span>,<span class="string">&#x27;storeID&#x27;</span>]).\</span><br><span class="line">        groupBy(<span class="string">&#x27;storeProvince&#x27;</span>).count()</span><br><span class="line">    <span class="comment"># write to mysql</span></span><br><span class="line">    province_over1000_storenum.write.mode(<span class="string">&#x27;overwrite&#x27;</span>).\</span><br><span class="line">        <span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;url&#x27;</span>,<span class="string">&#x27;jdbc:mysql://node01:3306/spark_databases?useSSL=false&amp;useUnicode=true&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;dbtable&#x27;</span>,<span class="string">&#x27;province_over1000_storenum_table&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;user&#x27;</span>,<span class="string">&#x27;root&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;password&#x27;</span>,<span class="string">&#x27;密码&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;encoding&#x27;</span>,<span class="string">&#x27;utf-8&#x27;</span>).\</span><br><span class="line">        save()</span><br><span class="line">    <span class="comment"># write to hive.</span></span><br><span class="line">    province_over1000_storenum.write.mode(<span class="string">&#x27;overwrite&#x27;</span>).saveAsTable(<span class="string">&#x27;province_over1000_storenum_table&#x27;</span>,<span class="string">&#x27;parquet&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    province_over1000_storenum.show() </span><br><span class="line"></span><br><span class="line"><span class="comment"># demand 3:</span></span><br><span class="line">    top3_province_order_avg_df = top3_province_joined_df.groupBy(<span class="string">&#x27;storeProvince&#x27;</span>).\</span><br><span class="line">        avg(<span class="string">&#x27;receivable&#x27;</span>).withColumnRenamed(<span class="string">&#x27;avg(receivable)&#x27;</span>,<span class="string">&#x27;money&#x27;</span>).\</span><br><span class="line">        withColumn(<span class="string">&#x27;money&#x27;</span>,F.<span class="built_in">round</span>(<span class="string">&#x27;money&#x27;</span>,<span class="number">2</span>)).\</span><br><span class="line">        orderBy(<span class="string">&#x27;money&#x27;</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">    top3_province_order_avg_df.show()</span><br><span class="line">    <span class="comment"># write into mysql</span></span><br><span class="line">    top3_province_order_avg_df.write.mode(<span class="string">&#x27;overwrite&#x27;</span>). \</span><br><span class="line">        <span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;url&#x27;</span>,<span class="string">&#x27;jdbc:mysql://node01:3306/spark_databases?useSSL=false&amp;useUnicode=true&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;dbtable&#x27;</span>,<span class="string">&#x27;top3_province_order_avg_table&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;user&#x27;</span>,<span class="string">&#x27;root&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;password&#x27;</span>,<span class="string">&#x27;密码&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;encoding&#x27;</span>,<span class="string">&#x27;utf-8&#x27;</span>). \</span><br><span class="line">        save()</span><br><span class="line">    <span class="comment"># write into hive</span></span><br><span class="line">    top3_province_order_avg_df.write.mode(<span class="string">&#x27;overwrite&#x27;</span>).saveAsTable(<span class="string">&#x27;top3_province_order_avg_table&#x27;</span>,<span class="string">&#x27;parquet&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># demand 4:</span></span><br><span class="line"><span class="comment"># 销售额的省份中，各个省份支付类型比例。</span></span><br><span class="line">		<span class="comment"># 先把dataframe 注册成一个视图表，方便后面sql语句的使用。</span></span><br><span class="line">    top3_province_joined_df.createTempView(<span class="string">&#x27;province_pay&#x27;</span>)</span><br><span class="line">		</span><br><span class="line">    <span class="comment"># 用一个自制函数 润色 一个column。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">udf_func</span>(<span class="params">percent</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">str</span>(<span class="built_in">round</span>(percent*<span class="number">100</span>,<span class="number">2</span>))+<span class="string">&#x27;%&#x27;</span></span><br><span class="line">    percent_udf = F.udf(udf_func,StringType())</span><br><span class="line">		</span><br><span class="line">    <span class="comment"># 先写子查询，子查询中运用窗口函数over 把每个省的总支付数算出来，用窗口函数是方便编写过程中 显示效果。</span></span><br><span class="line">    <span class="comment"># 对每一种支付方式 求和 再 除以 total 得出 百分比； </span></span><br><span class="line">    <span class="comment"># 注意： select 的聚合函数count外 用了 storeprovince、paytype、total，就一定要按照这三个分组（mysql知识点）</span></span><br><span class="line">    <span class="comment"># 用withcolumn 对指定字段 调用自己注册的 udf 函数。</span></span><br><span class="line">    pay_type_df = spark.sql(<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    select storeProvince,payType,(count(payType)/total) as percent_paytype from</span></span><br><span class="line"><span class="string">        (select storeProvince,payType,count(1) over(partition by storeProvince) as total from province_pay)</span></span><br><span class="line"><span class="string">        group by storeProvince, payType, total</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span>).withColumn(<span class="string">&#x27;percent_paytype&#x27;</span>,percent_udf(<span class="string">&#x27;percent_paytype&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    pay_type_df.show()</span><br><span class="line">    <span class="comment"># write into mysql</span></span><br><span class="line">    pay_type_df.write.mode(<span class="string">&#x27;overwrite&#x27;</span>). \</span><br><span class="line">        <span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;url&#x27;</span>,<span class="string">&#x27;jdbc:mysql://node01:3306/spark_databases?useSSL=false&amp;useUnicode=true&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;dbtable&#x27;</span>,<span class="string">&#x27;pay_type_df_table&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;user&#x27;</span>,<span class="string">&#x27;root&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;password&#x27;</span>,<span class="string">&#x27;密码&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&#x27;encoding&#x27;</span>,<span class="string">&#x27;utf-8&#x27;</span>). \</span><br><span class="line">        save()</span><br><span class="line">    <span class="comment"># write into hive</span></span><br><span class="line">    pay_type_df.write.mode(<span class="string">&#x27;overwrite&#x27;</span>).saveAsTable(<span class="string">&#x27;pay_type_df_table&#x27;</span>,<span class="string">&#x27;parquet&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    top3_province_joined_df.unpersist()  <span class="comment"># 记得给前面持久化的 df 释放掉。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">demand <span class="number">1</span>:</span><br><span class="line">+--------------+--------+</span><br><span class="line">| storeProvince|   money|</span><br><span class="line">+--------------+--------+</span><br><span class="line">|        湖南省|<span class="number">24088.95</span>|</span><br><span class="line">|        广东省| <span class="number">13235.0</span>|</span><br><span class="line">|广西壮族自治区|   <span class="number">515.5</span>|</span><br><span class="line">|        江苏省|   <span class="number">308.0</span>|</span><br><span class="line">|        北京市|    <span class="number">53.0</span>|</span><br><span class="line">+--------------+--------+</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">demand <span class="number">2</span>: </span><br><span class="line"><span class="number">2022</span>-04-<span class="number">20</span> 02:<span class="number">50</span>:<span class="number">55</span>,<span class="number">428</span> WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting <span class="string">&#x27;spark.sql.debug.maxToStringFields&#x27;</span>.</span><br><span class="line"><span class="number">2022</span>-04-<span class="number">20</span> 02:<span class="number">50</span>:<span class="number">57</span>,<span class="number">157</span> WARN conf.HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does <span class="keyword">not</span> exist</span><br><span class="line"><span class="number">2022</span>-04-<span class="number">20</span> 02:<span class="number">50</span>:<span class="number">59</span>,<span class="number">851</span> WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager <span class="keyword">is</span> <span class="built_in">set</span> to instance of HiveAuthorizerFactory.</span><br><span class="line">+--------------+-------+----------+-------------+-------+------------------+</span><br><span class="line">| storeProvince|storeID|receivable|       dateTS|payType|top3_storeProvince|</span><br><span class="line">+--------------+-------+----------+-------------+-------+------------------+</span><br><span class="line">|        湖南省|   <span class="number">4064</span>|      <span class="number">22.5</span>|<span class="number">1563758583000</span>| alipay|            湖南省|</span><br><span class="line">|        湖南省|    <span class="number">718</span>|       <span class="number">7.0</span>|<span class="number">1546737450000</span>| alipay|            湖南省|</span><br><span class="line">|        湖南省|   <span class="number">1786</span>|      <span class="number">10.0</span>|<span class="number">1546478081000</span>|   cash|            湖南省|</span><br><span class="line">|        广东省|   <span class="number">3702</span>|      <span class="number">10.5</span>|<span class="number">1559133703000</span>| wechat|            广东省|</span><br><span class="line">|广西壮族自治区|   <span class="number">1156</span>|      <span class="number">10.0</span>|<span class="number">1548594458000</span>|   cash|    广西壮族自治区|</span><br><span class="line">|        广东省|    <span class="number">318</span>|       <span class="number">3.0</span>|<span class="number">1548292824000</span>| wechat|            广东省|</span><br><span class="line">|        湖南省|   <span class="number">1699</span>|       <span class="number">6.5</span>|<span class="number">1545356344000</span>|   cash|            湖南省|</span><br><span class="line">|        湖南省|   <span class="number">1167</span>|      <span class="number">17.0</span>|<span class="number">1547284514000</span>| alipay|            湖南省|</span><br><span class="line">|        湖南省|   <span class="number">3466</span>|      <span class="number">19.0</span>|<span class="number">1563845059000</span>|   cash|            湖南省|</span><br><span class="line">|        广东省|    <span class="number">333</span>|       <span class="number">4.0</span>|<span class="number">1557231358000</span>| wechat|            广东省|</span><br><span class="line">|        湖南省|   <span class="number">3354</span>|      <span class="number">22.0</span>|<span class="number">1560692925000</span>|   cash|            湖南省|</span><br><span class="line">|        广东省|   <span class="number">3367</span>|      <span class="number">19.0</span>|<span class="number">1552019799000</span>|   cash|            广东省|</span><br><span class="line">|        湖南省|    <span class="number">832</span>|      <span class="number">45.0</span>|<span class="number">1563413959000</span>| wechat|            湖南省|</span><br><span class="line">|        湖南省|    <span class="number">949</span>|       <span class="number">6.0</span>|<span class="number">1565683992000</span>|   cash|            湖南省|</span><br><span class="line">|        广东省|   <span class="number">4213</span>|      <span class="number">22.5</span>|<span class="number">1561301033000</span>|   cash|            广东省|</span><br><span class="line">|        广东省|   <span class="number">3487</span>|       <span class="number">6.0</span>|<span class="number">1563104184000</span>|   cash|            广东省|</span><br><span class="line">|        湖南省|   <span class="number">2583</span>|     <span class="number">900.0</span>|<span class="number">1557558714000</span>|   cash|            湖南省|</span><br><span class="line">|        广东省|   <span class="number">3561</span>|     <span class="number">215.0</span>|<span class="number">1552703028000</span>|   cash|            广东省|</span><br><span class="line">|        湖南省|   <span class="number">1975</span>|      <span class="number">15.0</span>|<span class="number">1560845929000</span>|   cash|            湖南省|</span><br><span class="line">|        广东省|    <span class="number">389</span>|      <span class="number">72.0</span>|<span class="number">1542464496000</span>|   cash|            广东省|</span><br><span class="line">+--------------+-------+----------+-------------+-------+------------------+</span><br><span class="line">only showing top <span class="number">20</span> rows</span><br><span class="line"></span><br><span class="line">+-------------+-----+</span><br><span class="line">|storeProvince|count|</span><br><span class="line">+-------------+-----+</span><br><span class="line">|       湖南省|    <span class="number">3</span>|</span><br><span class="line">+-------------+-----+</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">demand <span class="number">3</span>:</span><br><span class="line">+--------------+-----+</span><br><span class="line">| storeProvince|money|</span><br><span class="line">+--------------+-----+</span><br><span class="line">|广西壮族自治区|<span class="number">51.55</span>|</span><br><span class="line">|        湖南省|<span class="number">49.98</span>|</span><br><span class="line">|        广东省|<span class="number">26.36</span>|</span><br><span class="line">+--------------+-----+</span><br><span class="line">demand <span class="number">4</span>:</span><br><span class="line">+--------------+--------+---------------+</span><br><span class="line">| storeProvince| payType|percent_paytype|</span><br><span class="line">+--------------+--------+---------------+</span><br><span class="line">|        广东省|  wechat|         <span class="number">39.04</span>%|</span><br><span class="line">|        广东省|    cash|         <span class="number">52.19</span>%|</span><br><span class="line">|        广东省|bankcard|           <span class="number">0.6</span>%|</span><br><span class="line">|        广东省|  alipay|          <span class="number">8.17</span>%|</span><br><span class="line">|广西壮族自治区|    cash|          <span class="number">80.0</span>%|</span><br><span class="line">|广西壮族自治区|  wechat|          <span class="number">10.0</span>%|</span><br><span class="line">|广西壮族自治区|  alipay|          <span class="number">10.0</span>%|</span><br><span class="line">|        湖南省|  alipay|          <span class="number">4.77</span>%|</span><br><span class="line">|        湖南省|    cash|         <span class="number">68.05</span>%|</span><br><span class="line">|        湖南省|  wechat|         <span class="number">26.97</span>%|</span><br><span class="line">|        湖南省|bankcard|          <span class="number">0.21</span>%|</span><br><span class="line">+--------------+--------+---------------+</span><br></pre></td></tr></table></figure>



<h1 id="Spark新特性-及-核心回顾"><a href="#Spark新特性-及-核心回顾" class="headerlink" title="Spark新特性 及 核心回顾"></a>Spark新特性 及 核心回顾</h1><h2 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h2><p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2018.01.04.jpg" alt="截屏2022-04-20 18.01.04"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.14.11.jpg" alt="截屏2022-04-20 20.14.11"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.17.47.jpg" alt="截屏2022-04-20 20.17.47"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.24.08.jpg" alt="截屏2022-04-20 20.24.08"></p>
<p>​		可以看出sort shufflemanager 优于 hash shufflemanager。io更少。也是spark后续改良的产物。 </p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.30.48.jpg" alt="截屏2022-04-20 20.30.48"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.45.03.jpg" alt="截屏2022-04-20 20.45.03"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.47.24.jpg" alt="截屏2022-04-20 20.47.24"></p>
<h2 id="新特性"><a href="#新特性" class="headerlink" title="新特性"></a>新特性</h2><h3 id="自适应查询"><a href="#自适应查询" class="headerlink" title="自适应查询"></a>自适应查询</h3><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2020.59.38.jpg" alt="截屏2022-04-20 20.59.38"></p>
<p>​		这个自适应查询 有点像 之前hive 学习中 各种自动优化的设置，打开开关或者再conf配置文件中配置就行。</p>
<h4 id="1、动态合并"><a href="#1、动态合并" class="headerlink" title="1、动态合并"></a>1、动态合并</h4><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2021.03.46.jpg" alt="截屏2022-04-20 21.03.46"></p>
<p>​		动态合并，把几个小分区合并，让每个分区大小差不多，让数据量均衡。</p>
<h4 id="2、动态-join-策略"><a href="#2、动态-join-策略" class="headerlink" title="2、动态 join 策略"></a>2、动态 join 策略</h4><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2021.14.20.jpg" alt="截屏2022-04-20 21.14.20"></p>
<p>​		上图意思是：</p>
<p>​		语法树本来要做的是 将 两个分区内 sort ，再 将两个数据集的内容作 join 操作。</p>
<p>​		但是预想 数据集2 大小是25m，实际上 数据集2 大小是8m，太小了。</p>
<p>​		所以动态将策略调整 –&gt;&gt;（ 取消数据集2内的sort操作，直接把数据集2这么一点点数据以广播方式，广播到数据集1相关的executor内，数据集1相关的executor得到一份完整的数据集2数据，再在数据集1所在的executor内部作join操作；之所以能这样搞，因为数据集2内东西非常少，相当于集中到一个executor内把活都干了，避免io资源浪费。）。</p>
<h4 id="3、动态优化倾斜join"><a href="#3、动态优化倾斜join" class="headerlink" title="3、动态优化倾斜join"></a>3、动态优化倾斜join</h4><p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2021.33.22.jpg" alt="截屏2022-04-20 21.33.22"></p>
<p>​		上图解释：</p>
<p>​		按道理说，红、黄、蓝、绿 四种数据 在经过了shuffle 后会分成 红、黄、蓝、绿 四个分区，但是由于红 数据量太大了，如果把红 只放一个分区，会造成严重的数据倾斜，所以我们把红数据 拆分成 两个 分区，这样 红1、红2、黄、蓝、绿 这5个分区大小都差不多，缓解了数据倾斜；</p>
<p>​		红1、红2 两个分区 在 pipline内存计算管道 内一样享受 高性能的 并行计算，在下一次的shuffle中 还是按照原来的 <code>（红、黄、蓝、绿）衍生色(黄、紫、橙、绿)</code>来进入一个分区，这样就会出现两个分区<code>（红1、黄、蓝、绿）衍生色(黄1、紫、橙、绿)</code>、<code>（红2、黄、蓝、绿）衍生色(黄2、紫、橙、绿)</code>。</p>
<p>​		这样一来，pipline 内从左到右的 计算时间 会大致相同，从而提高了计算效率。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-20%2021.51.25.jpg" alt="截屏2022-04-20 21.51.25"></p>
<h3 id="动态分区裁剪"><a href="#动态分区裁剪" class="headerlink" title="动态分区裁剪"></a>动态分区裁剪</h3><p>dynamic partition pruning，部分条件下性能提升巨大！！！这是spark3.0的重大改进（自动优化无需配置）。</p>
<p>​		我们回顾一下 static pruning静态裁剪，就是将过滤条件前置、或者提前列值裁剪，那动态裁剪 动态在哪呢？我们看一个案例。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dim_iteblog </span><br><span class="line"><span class="keyword">JOIN</span> fact_iteblog </span><br><span class="line"><span class="keyword">ON</span> (dim_iteblog.partcol <span class="operator">=</span> fact_iteblog.partcol) </span><br><span class="line"><span class="keyword">WHERE</span> dim_iteblog.othercol <span class="operator">&gt;</span> <span class="number">10</span></span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-21%2012.48.49.jpg" alt="截屏2022-04-21 12.48.49"></p>
<p>​		左图为 静态裁剪 优化的执行计划，右图为动态裁剪优化的执行计划。</p>
<p>​		通俗来解释就是：</p>
<p>​		有a、b两个表 join，但是where 过滤条件 仅仅针对 其中a表！！！</p>
<p>​		如果按静态裁剪 就是先对有过滤要求的a表过滤，再join b表。</p>
<p>​		动态裁剪 聪明之处在于，考虑到 join on 后的条件，b表势必会受到a表过滤后数据的影响，所以b表针对a表的过滤结果，也提前过滤b表的数据，实际让join之前 a、b两张表都精简了很多。</p>
<p>​		例如：join on 的条件是 a.id&#x3D;b.id ，a.id in(1,2)，b.id in(1,2,5,6,7,9)，a经过where过滤后 仅剩 a.id in(1)，b 针对 a 的过滤结果 也提前过滤一遍，得到 b.id in(1),过滤掉了2，5，6，7，9，实际就是 a.id&#x3D;1 和 b.id&#x3D;1 join。笛卡尔积被大量精简，性能得到巨大提升！！！</p>
<p>​		</p>
<h3 id="koalas"><a href="#koalas" class="headerlink" title="koalas"></a>koalas</h3><p>​		koalas 就是 让pandas 借用spark 实现 分布式计算的一个包，方便只会pandas的人用spark。</p>
<p>​		这里不多介绍了，比较简单就能完成，下载包再按pandas写代码就行。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/%E6%88%AA%E5%B1%8F2022-04-21%2013.24.12.jpg" alt="截屏2022-04-21 13.24.12"></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/sql/" rel="tag"># sql</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/" rel="prev" title="spark基础入门及安装部署">
                  <i class="fa fa-chevron-left"></i> spark基础入门及安装部署
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/" rel="next" title="sparkstreaming_内涵structurestreaming">
                  sparkstreaming_内涵structurestreaming <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">陈宇韶chenyushao</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  





</body>
</html>
