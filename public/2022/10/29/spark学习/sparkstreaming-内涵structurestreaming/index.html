<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.13.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="这是文章开头，显示在主页面，详情请点击此处。">
<meta property="og:type" content="article">
<meta property="og:title" content="sparkstreaming_内涵structurestreaming">
<meta property="og:url" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/index.html">
<meta property="og:site_name" content="Tiger_pop&#39;s Blog">
<meta property="og:description" content="这是文章开头，显示在主页面，详情请点击此处。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-12%2021.51.33-7050662.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-12%2021.57.14.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-12%2022.02.48.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-13%2018.16.42.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-13%2015.33.09.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-13%2016.17.55.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-14%2009.06.30.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-14%2014.25.48.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-14%2016.23.51.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-14%2016.24.11.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-18%2021.33.48.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-18%2021.36.11.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-18%2021.45.14.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-18%2021.45.43.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-26%2017.08.04.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-27%2018.28.50.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-16%2011.07.35.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-16%2011.27.53.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-16%2011.28.44.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-16%2016.21.10.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-17%2012.48.56.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-17%2012.49.21.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-17%2012.49.54.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2011.24.17.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2014.51.51.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2015.00.14.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2015.26.21.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2015.47.52.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2018.09.32.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2018.53.40.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2011.03.44.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.05.58.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.07.01.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.05.40.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.08.05.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.17.55.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-31%2011.56.42.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2011.03.44.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2011.00.16.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.32.02.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-31%2013.07.08.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-31%2013.07.32.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-31%2013.08.22.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-31%2016.48.30.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-02%2010.27.01.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-02%2017.22.01.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-07%2020.44.36.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-07%2021.19.30.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-08%2011.46.20.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-27%2017.38.48.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-27%2017.55.34.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-01%2023.16.47.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-04%2010.08.55.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-12%2015.28.13.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-15%2021.12.08.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-15%2021.12.28.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-15%2021.14.12.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-15%2021.20.11.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2011.00.16.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.32.02.jpg">
<meta property="article:published_time" content="2022-10-29T14:42:11.000Z">
<meta property="article:modified_time" content="2022-10-29T18:13:28.569Z">
<meta property="article:author" content="陈宇韶chenyushao">
<meta property="article:tag" content="spark_streaming">
<meta property="article:tag" content="structure_streaming">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-12%2021.51.33-7050662.jpg">


<link rel="canonical" href="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/","path":"2022/10/29/spark学习/sparkstreaming-内涵structurestreaming/","title":"sparkstreaming_内涵structurestreaming"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>sparkstreaming_内涵structurestreaming | Tiger_pop's Blog</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Tiger_pop's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Tiger_pop's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">tiger_pop 的博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#spark-streaming-%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">spark streaming 学习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dstream-%E6%95%B0%E6%8D%AE%E6%8A%BD%E8%B1%A1"><span class="nav-number">2.</span> <span class="nav-text">Dstream 数据抽象</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Python-%E7%89%88-SparkStreaming"><span class="nav-number">3.</span> <span class="nav-text">Python 版 SparkStreaming</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-Streaming%E7%A8%8B%E5%BA%8F%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4"><span class="nav-number">3.1.</span> <span class="nav-text">Spark Streaming程序基本步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BAStreamingContext%E5%AF%B9%E8%B1%A1"><span class="nav-number">3.1.1.</span> <span class="nav-text">创建StreamingContext对象</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E6%BA%90"><span class="nav-number">3.1.2.</span> <span class="nav-text">输入源</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E6%96%87%E4%BB%B6%E6%B5%81"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">一、文件流</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E5%A5%97%E6%8E%A5%E5%AD%97%E6%B5%81"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">二、套接字流</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#a%E3%80%81%E5%A2%9E%E5%8A%A0%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86"><span class="nav-number">3.1.2.2.1.</span> <span class="nav-text">a、增加状态管理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#b%E3%80%81%E7%8A%B6%E6%80%81%E6%81%A2%E5%A4%8D"><span class="nav-number">3.1.2.2.2.</span> <span class="nav-text">b、状态恢复</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%89%E3%80%81rdd-%E9%98%9F%E5%88%97%E6%B5%81"><span class="nav-number">3.1.2.3.</span> <span class="nav-text">三、rdd 队列流</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%9B%E3%80%81kafuka%EF%BC%88Structured-Streaming-%E5%AE%9E%E7%8E%B0%EF%BC%89"><span class="nav-number">3.1.2.4.</span> <span class="nav-text">四、kafuka（Structured Streaming 实现）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-1-structured%E6%8E%A5%E6%94%B6kafka%E6%B6%88%E6%81%AF"><span class="nav-number">3.1.2.4.1.</span> <span class="nav-text">4.1 structured接收kafka消息</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-%E5%AF%BC%E5%85%A5%E4%BE%9D%E8%B5%96%EF%BC%9A"><span class="nav-number">3.1.2.4.1.1.</span> <span class="nav-text">1.导入依赖：</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-%E5%BB%BA%E7%AB%8Bkafka%E7%94%9F%E4%BA%A7%E8%80%85"><span class="nav-number">3.1.2.4.1.2.</span> <span class="nav-text">2.建立kafka生产者</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-%E6%94%B6%E5%8F%96kafka%E6%95%B0%E6%8D%AE"><span class="nav-number">3.1.2.4.1.3.</span> <span class="nav-text">3.收取kafka数据</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkStreaming-%E5%BA%94%E7%94%A8%E7%AA%97%E5%8F%A3%E8%AE%A1%E7%AE%97"><span class="nav-number">3.2.</span> <span class="nav-text">SparkStreaming 应用窗口计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BE%93%E5%87%BA"><span class="nav-number">3.2.1.</span> <span class="nav-text">自定义输出</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spark-structure-stream%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.</span> <span class="nav-text">spark- structure-stream学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#kafka-streaming"><span class="nav-number">4.1.</span> <span class="nav-text">kafka- streaming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ratesource"><span class="nav-number">4.2.</span> <span class="nav-text">ratesource</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#window%E6%93%8D%E4%BD%9C"><span class="nav-number">4.3.</span> <span class="nav-text">window操作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E7%AA%97%E5%8F%A3%E6%95%B0%EF%BC%88%E9%87%8D%E5%8F%A0%E6%95%B0%EF%BC%89"><span class="nav-number">4.3.0.1.</span> <span class="nav-text">最大窗口数（重叠数）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#update%E6%A8%A1%E5%BC%8F%E4%B8%8B%E5%8A%A0watermask"><span class="nav-number">4.3.0.2.</span> <span class="nav-text">update模式下加watermask</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#append%E6%A8%A1%E5%BC%8F%E4%B8%8B%E5%8A%A0watermask"><span class="nav-number">4.3.0.3.</span> <span class="nav-text">append模式下加watermask</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B0%B4%E5%8D%B0%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93"><span class="nav-number">4.3.0.4.</span> <span class="nav-text">水印机制总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%BB%E9%87%8D%E7%AE%97%E5%AD%90"><span class="nav-number">4.4.</span> <span class="nav-text">去重算子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E4%B8%8E%E9%9D%99%E6%80%81%E6%95%B0%E6%8D%AEjoin"><span class="nav-number">4.5.</span> <span class="nav-text">流与静态数据join</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E4%B8%8E%E6%B5%81join"><span class="nav-number">4.6.</span> <span class="nav-text">流与流join</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%81%E4%B8%8E%E6%B5%81%E7%9A%84outjoin"><span class="nav-number">4.6.0.1.</span> <span class="nav-text">流与流的outjoin</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Support-matrix-for-joins-in-streaming-queries"><span class="nav-number">4.6.0.1.1.</span> <span class="nav-text">Support matrix for joins in streaming queries</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sink"><span class="nav-number">4.7.</span> <span class="nav-text">sink</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#hdfs-sink%EF%BC%88or-%E6%9C%AC%E5%9C%B0%EF%BC%89"><span class="nav-number">4.7.0.1.</span> <span class="nav-text">hdfs sink（or 本地）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#kafka-sink"><span class="nav-number">4.7.0.2.</span> <span class="nav-text">kafka sink</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Foreach-writer-%EF%BC%88mysql%EF%BC%89"><span class="nav-number">4.7.0.3.</span> <span class="nav-text">Foreach writer （mysql）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Foreachbatch"><span class="nav-number">4.7.0.4.</span> <span class="nav-text">Foreachbatch</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Trigger-%E8%A7%A6%E5%8F%91%E5%99%A8"><span class="nav-number">4.8.</span> <span class="nav-text">Trigger(触发器)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B%EF%BC%88%E5%86%99%E5%85%A5redis%EF%BC%89"><span class="nav-number">4.9.</span> <span class="nav-text">实践案例（写入redis）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E6%8B%9F%E6%95%B0%E6%8D%AE"><span class="nav-number">4.9.0.1.</span> <span class="nav-text">模拟数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%BB%91%E5%90%8D%E5%8D%95%E5%AE%9E%E6%97%B6%E7%BB%9F%E8%AE%A1"><span class="nav-number">4.9.0.2.</span> <span class="nav-text">黑名单实时统计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BB%E9%87%8F"><span class="nav-number">4.9.0.3.</span> <span class="nav-text">统计广告点击量</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1"><span class="nav-number">4.10.</span> <span class="nav-text">词频统计</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="陈宇韶chenyushao"
      src="/images/my.jpg">
  <p class="site-author-name" itemprop="name">陈宇韶chenyushao</p>
  <div class="site-description" itemprop="description">爱学习、爱工作、爱生活;         微信号: Tiger_and_master;         手机号码:18515678348 </div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">378</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">138</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my.jpg">
      <meta itemprop="name" content="陈宇韶chenyushao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tiger_pop's Blog">
      <meta itemprop="description" content="爱学习、爱工作、爱生活;         微信号: Tiger_and_master;         手机号码:18515678348 ">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="sparkstreaming_内涵structurestreaming | Tiger_pop's Blog">
      <meta itemprop="description" content="这是文章开头，显示在主页面，详情请点击此处。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          sparkstreaming_内涵structurestreaming
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-10-29 22:42:11" itemprop="dateCreated datePublished" datetime="2022-10-29T22:42:11+08:00">2022-10-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-10-30 02:13:28" itemprop="dateModified" datetime="2022-10-30T02:13:28+08:00">2022-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/spark/" itemprop="url" rel="index"><span itemprop="name">spark</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">这是文章开头，显示在主页面，详情请点击此处。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>简介 <span id="more"></span></p>
<h1 id="spark-streaming-学习"><a href="#spark-streaming-学习" class="headerlink" title="spark streaming 学习"></a>spark streaming 学习</h1><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-12%2021.51.33-7050662.jpg" alt="截屏2022-05-12 21.51.33"></p>
<h1 id="Dstream-数据抽象"><a href="#Dstream-数据抽象" class="headerlink" title="Dstream 数据抽象"></a>Dstream 数据抽象</h1><p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-12%2021.57.14.jpg" alt="截屏2022-05-12 21.57.14"></p>
<p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-12%2022.02.48.jpg" alt="截屏2022-05-12 22.02.48"></p>
<h1 id="Python-版-SparkStreaming"><a href="#Python-版-SparkStreaming" class="headerlink" title="Python 版 SparkStreaming"></a>Python 版 SparkStreaming</h1><p>​		我们在spark官网<a target="_blank" rel="noopener" href="https://spark.apache.org/examples.html">https://spark.apache.org/examples.html</a> 没看见spark streaming 的python examples，所以我们参考 <a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/1709-2/">http://dblab.xmu.edu.cn/blog/1709-2/</a>  厦大的林子雨老师的 python 版 spark教程。	</p>
<h2 id="Spark-Streaming程序基本步骤"><a href="#Spark-Streaming程序基本步骤" class="headerlink" title="Spark Streaming程序基本步骤"></a>Spark Streaming程序基本步骤</h2><p>编写Spark Streaming程序的基本步骤是：<br>1.通过创建输入DStream来定义输入源<br>2.通过对DStream应用转换操作和输出操作来定义流计算。<br>3.用streamingContext.start()来开始接收数据和处理流程。<br>4.通过streamingContext.awaitTermination()方法来等待处理结束（手动结束或因为错误而结束）。<br>5.可以通过streamingContext.stop()来手动结束流计算进程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0 准备环境</span></span><br><span class="line"><span class="comment"># 1 加载数据</span></span><br><span class="line"><span class="comment"># 2 处理数据</span></span><br><span class="line"><span class="comment"># 3 输出结果</span></span><br><span class="line"><span class="comment"># 4 启动并等待结束</span></span><br><span class="line"><span class="comment"># 5 关闭资源</span></span><br></pre></td></tr></table></figure>



<h3 id="创建StreamingContext对象"><a href="#创建StreamingContext对象" class="headerlink" title="创建StreamingContext对象"></a>创建StreamingContext对象</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line">conf = SparkConf()</span><br><span class="line">conf.setAppName(<span class="string">&#x27;TestDStream&#x27;</span>)</span><br><span class="line">conf.setMaster(<span class="string">&#x27;local[*]&#x27;</span>)</span><br><span class="line">sc = SparkContext(conf = conf)</span><br><span class="line">ssc = StreamingContext(sc, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 1表示每隔1秒钟就自动执行一次流计算，这个秒数可以自由设定。dstream的一个rdd生成并计算。不管有无数据输入。</span></span><br></pre></td></tr></table></figure>

<h3 id="输入源"><a href="#输入源" class="headerlink" title="输入源"></a>输入源</h3><h4 id="一、文件流"><a href="#一、文件流" class="headerlink" title="一、文件流"></a>一、文件流</h4><p>​		在待监听文件夹logfile中新建两个日志文件log1.txt和log2.txt，里面可以随便输入一些内容。	</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line">conf = SparkConf()</span><br><span class="line">conf.setAppName(<span class="string">&#x27;TestDStream&#x27;</span>)</span><br><span class="line">conf.setMaster(<span class="string">&#x27;local[*]&#x27;</span>)</span><br><span class="line">sc = SparkContext(conf = conf)</span><br><span class="line">ssc = StreamingContext(sc, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">lines = ssc.textFileStream(<span class="string">&#x27;file:///opt/data/logfile/logfile&#x27;</span>)</span><br><span class="line">words = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">wordCounts = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> x : (x,<span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> a,b: a+b)</span><br><span class="line">wordCounts.pprint()</span><br><span class="line">ssc.start() </span><br><span class="line"><span class="comment"># 实际上，当你输入这行回车后，Spark Streaming就开始进行循环监听，下面的ssc.awaitTermination()是无法输入到屏幕上的，但是，为了程序完整性，这里还是给出ssc.awaitTermination()</span></span><br><span class="line"><span class="comment"># start() 必须在 pprint 或者其他类似操作之后。</span></span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<p>​		监听程序只监听”logfile”目录下在程序启动后新增的文件，不会去处理历史上已经存在的文件。所以，为了能够让程序读取文件内容并显示到屏幕上，让我们能够看到效果，这时，我们需要到”logfile”目录下再新建一个log3.txt文件，才可见运行后的效果。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-13%2018.16.42.jpg" alt="截屏2022-05-13 18.16.42"></p>
<h4 id="二、套接字流"><a href="#二、套接字流" class="headerlink" title="二、套接字流"></a>二、套接字流</h4><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-13%2015.33.09.jpg" alt="截屏2022-05-13 15.33.09"></p>
<p>​		利用nc 在node01机器 用9999端口 向node01 发送内容。模拟流式数据。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-13%2016.17.55.jpg" alt="截屏2022-05-13 16.17.55"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line">conf = SparkConf()</span><br><span class="line">conf.setAppName(<span class="string">&#x27;TestDStream&#x27;</span>)</span><br><span class="line">conf.setMaster(<span class="string">&#x27;local[*]&#x27;</span>)</span><br><span class="line">sc = SparkContext(conf = conf)</span><br><span class="line">ssc = StreamingContext(sc, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">lines = ssc.socketTextStream(<span class="string">&#x27;node01&#x27;</span>,<span class="number">9999</span>)</span><br><span class="line">counts = lines.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&#x27; &#x27;</span>)).\</span><br><span class="line">    <span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x,<span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> a,b: a+b)</span><br><span class="line">counts.pprint()</span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-14%2009.06.30.jpg" alt="截屏2022-05-14 09.06.30"></p>
<p>​		解释警告的原因：</p>
<p>​		<code>WARN storage.RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</code></p>
<p>​		<code>While a Spark Streaming driver program is running, the system receives data from various sources and and divides it into batches. Each batch of data is treated as an RDD, that is, an immutable parallel collection of data. These input RDDs are saved in memory and replicated to two nodes for fault-tolerance.</code></p>
<p>​		<code>The warning in your case means that incoming data from stream are not replicated at all. The reason for that may be that you run the app with just one instance of Spark worker or running in local mode. Try to start more Spark workers and see if the warning is gone.</code></p>
<p>​		意思就是sparkstream的容错机制会把 传来的数据复制一份备份，在local模式下可能出现这样的警告，spark on yarn 或者 spark alone的集群应该就不会。</p>
<h5 id="a、增加状态管理"><a href="#a、增加状态管理" class="headerlink" title="a、增加状态管理"></a>a、增加状态管理</h5><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-14%2014.25.48.jpg" alt="截屏2022-05-14 14.25.48"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line">conf = SparkConf()</span><br><span class="line">conf.setAppName(<span class="string">&#x27;TestDStream&#x27;</span>)</span><br><span class="line">conf.setMaster(<span class="string">&#x27;local[*]&#x27;</span>)</span><br><span class="line">sc = SparkContext(conf = conf)</span><br><span class="line">ssc = StreamingContext(sc, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加一个checkpoint 来存state状态。记得chmod修改一个权限。</span></span><br><span class="line">ssc.checkpoint(directory=<span class="string">&#x27;file:///opt/data/sparkstream_checkpoint&#x27;</span>)</span><br><span class="line"><span class="comment"># 自定义一个状态更新方法，记得ctrl+点击在idea中看源码去理解。updateFunc: (Iterable[V], Optional[S]) -&gt; S</span></span><br><span class="line"><span class="comment"># 状态更新方法 的第一个参数是迭代器 第二是参数是 单独的数。</span></span><br><span class="line"><span class="comment"># 第一个参数迭代器里面其实是nc新来的内容 按照key分组后，由对应value组成的数组。比如nc 输入 spark spark spark ，Iterable[V]就是[3]</span></span><br><span class="line"><span class="comment"># 第二个参数是key的历史value值。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateFunc</span>(<span class="params">currentvalue, historyvalue</span>):</span><br><span class="line">    <span class="keyword">if</span> historyvalue==<span class="literal">None</span>:</span><br><span class="line">        historyvalue = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(currentvalue)&gt;<span class="number">0</span>:</span><br><span class="line">        historyvalue = <span class="built_in">sum</span>(currentvalue) + <span class="built_in">int</span>(historyvalue)</span><br><span class="line">    <span class="keyword">return</span> historyvalue</span><br><span class="line"></span><br><span class="line">lines = ssc.socketTextStream(<span class="string">&#x27;node01&#x27;</span>,<span class="number">9999</span>)</span><br><span class="line">counts = lines.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot; &quot;</span>)).\</span><br><span class="line">    <span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x,<span class="number">1</span>)).\</span><br><span class="line">    updateStateByKey(updateFunc) <span class="comment"># 使用updatestatebykey按照key分组后调用自定义的状态更新方法。</span></span><br><span class="line">counts.pprint()</span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="comment"># 代码和上面一样，标注了一个步骤流程而已。</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateFunc</span>(<span class="params">currentvalue_list, historyvalue</span>):</span><br><span class="line">    <span class="keyword">if</span> historyvalue==<span class="literal">None</span>:</span><br><span class="line">        historyvalue = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(currentvalue_list)&gt;<span class="number">0</span>:</span><br><span class="line">        historyvalue = <span class="built_in">sum</span>(currentvalue_list) + <span class="built_in">int</span>(historyvalue)</span><br><span class="line">    <span class="keyword">return</span> historyvalue</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0 准备环境</span></span><br><span class="line">    conf = SparkConf()</span><br><span class="line">    conf.setAppName(<span class="string">&#x27;TestDStream&#x27;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&#x27;local[*]&#x27;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">5</span>)</span><br><span class="line">    <span class="comment"># 1 加载数据</span></span><br><span class="line">    ssc.checkpoint(directory=<span class="string">&#x27;file:///opt/data/sparkstream_checkpoint&#x27;</span>)</span><br><span class="line">    lines = ssc.socketTextStream(<span class="string">&#x27;node01&#x27;</span>,<span class="number">9999</span>)</span><br><span class="line">    <span class="comment"># 2 处理数据</span></span><br><span class="line">    counts = lines.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot; &quot;</span>)). \</span><br><span class="line">    <span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x,<span class="number">1</span>)). \</span><br><span class="line">    updateStateByKey(updateFunc)</span><br><span class="line">    <span class="comment"># 3 输出结果</span></span><br><span class="line">    counts.pprint()</span><br><span class="line">    <span class="comment"># 4 启动并等待结束</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    <span class="comment"># 5 关闭资源</span></span><br><span class="line">    ssc.stop(stopSparkContext=<span class="literal">True</span>,stopGraceFully=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-14%2016.23.51.jpg" alt="截屏2022-05-14 16.23.51"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-14%2016.24.11.jpg" alt="截屏2022-05-14 16.24.11"></p>
<p>​		可见加入了我们自定义的状态管理方法后，统计了历史状态的全部单词计数。（我不小心打的空格键也被计数记录了进去）。</p>
<h5 id="b、状态恢复"><a href="#b、状态恢复" class="headerlink" title="b、状态恢复"></a>b、状态恢复</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="comment"># 记得在idea中看 StreamingContext.getOrCreate 的源码！！！</span></span><br><span class="line"><span class="comment"># state路径中有记录就会读取记录，没有的话就会调用自定义的setupfunc方法去构建一个streamingcontext。通过这样的做法来实现状态恢复。</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateFunc</span>(<span class="params">currentvalue_list, historyvalue</span>):</span><br><span class="line">    <span class="keyword">if</span> historyvalue==<span class="literal">None</span>:</span><br><span class="line">        historyvalue = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(currentvalue_list)&gt;<span class="number">0</span>:</span><br><span class="line">        historyvalue = <span class="built_in">sum</span>(currentvalue_list) + <span class="built_in">int</span>(historyvalue)</span><br><span class="line">    <span class="keyword">return</span> historyvalue</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setupfunc</span>():</span><br><span class="line">    conf = SparkConf()</span><br><span class="line">    conf.setAppName(<span class="string">&#x27;TestDStream&#x27;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&#x27;local[*]&#x27;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    ssc.checkpoint(directory=<span class="string">&#x27;file:///opt/data/sparkstream_checkpoint&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    lines = ssc.socketTextStream(<span class="string">&#x27;node01&#x27;</span>,<span class="number">9999</span>)</span><br><span class="line">    counts = lines.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot; &quot;</span>)).\</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x,<span class="number">1</span>)).\</span><br><span class="line">        updateStateByKey(updateFunc)</span><br><span class="line">    counts.pprint()</span><br><span class="line">    <span class="keyword">return</span> ssc</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">  	<span class="comment"># ssc 的state状态信息中还包含了计算的代码，拿到ssc之后，start（）会让计算在旧有state上继续运行，严格来说是重新运行。</span></span><br><span class="line">    <span class="comment"># 没有旧有ssc的state状态，就用自定义方法setupfunc传回一个ssc。</span></span><br><span class="line">    ssc = StreamingContext.getOrCreate(checkpointPath=<span class="string">&#x27;file:///opt/data/sparkstream_checkpoint&#x27;</span>,</span><br><span class="line">                                       setupFunc=setupfunc)</span><br><span class="line">    ssc.start()</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    ssc.stop(stopSparkContext=<span class="literal">True</span>,stopGraceFully=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="comment"># 和上面的代码一样的，标注了一个步骤流程而已。</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateFunc</span>(<span class="params">currentvalue_list, historyvalue</span>):</span><br><span class="line">    <span class="keyword">if</span> historyvalue==<span class="literal">None</span>:</span><br><span class="line">        historyvalue = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(currentvalue_list)&gt;<span class="number">0</span>:</span><br><span class="line">        historyvalue = <span class="built_in">sum</span>(currentvalue_list) + <span class="built_in">int</span>(historyvalue)</span><br><span class="line">    <span class="keyword">return</span> historyvalue</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setupfunc</span>():</span><br><span class="line">    conf = SparkConf()</span><br><span class="line">    conf.setAppName(<span class="string">&#x27;TestDStream&#x27;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&#x27;local[*]&#x27;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    ssc.checkpoint(directory=<span class="string">&#x27;file:///opt/data/sparkstream_checkpoint&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    lines = ssc.socketTextStream(<span class="string">&#x27;node01&#x27;</span>,<span class="number">9999</span>)</span><br><span class="line">    <span class="comment"># 2 处理数据</span></span><br><span class="line">    counts = lines.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot; &quot;</span>)).\</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x,<span class="number">1</span>)).\</span><br><span class="line">        updateStateByKey(updateFunc)</span><br><span class="line">    <span class="comment"># 3 输出结果</span></span><br><span class="line">    counts.pprint()</span><br><span class="line">    <span class="keyword">return</span> ssc</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0 准备环境</span></span><br><span class="line">    <span class="comment"># 1 加载数据</span></span><br><span class="line">    ssc = StreamingContext.getOrCreate(checkpointPath=<span class="string">&#x27;file:///opt/data/sparkstream_checkpoint&#x27;</span>,</span><br><span class="line">                                       setupFunc=setupfunc)</span><br><span class="line">    <span class="comment"># 4 启动并等待结束</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    <span class="comment"># 5 关闭资源</span></span><br><span class="line">    ssc.stop(stopSparkContext=<span class="literal">True</span>,stopGraceFully=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>​		我们先在nc 中输入 <code>spark spark spark hadoop</code> 。</p>
<p>​		sparkstreaming的程序显示 <code>【spark 3     hadoop 1】</code></p>
<p>​		故意把sparkstreaming的程序关闭。</p>
<p>​		重新运行sparkstreaming的程序，状态恢复！可见<code>【spark 3     hadoop 1】</code>再次被显示。</p>
<p>​		然后在nc 中输入 <code>spark spark</code></p>
<p>​		sparkstreaming的程序显示 <code>【spark 5     hadoop 1】</code></p>
<p>​		状态恢复成功，后续sparkstream统计词汇的程序正常运行。</p>
<h4 id="三、rdd-队列流"><a href="#三、rdd-队列流" class="headerlink" title="三、rdd 队列流"></a>三、rdd 队列流</h4><p>​		重新介绍算子 <code>parallelize</code> ，将一个存在的集合，变成一个RDD。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> array = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>)</span><br><span class="line"><span class="keyword">var</span> rdd = sc.parallelize(array，<span class="number">3</span>)		</span><br><span class="line"><span class="comment">// 第一个参数一是一个 Seq集合</span></span><br><span class="line"><span class="comment">// 第二个参数并行数，会由并行数决定分区数	</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line">conf = SparkConf()</span><br><span class="line">conf.setAppName(<span class="string">&#x27;Test_rddqueue_Stream&#x27;</span>)</span><br><span class="line">conf.setMaster(<span class="string">&#x27;local[*]&#x27;</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line">ssc = StreamingContext(sc, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">a = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>)]</span><br><span class="line">rdd = sc.parallelize(a,<span class="number">1</span>)</span><br><span class="line">rdd2 = ssc.sparkContext.parallelize([<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>],<span class="number">2</span>)</span><br><span class="line">b = ssc.queueStream([rdd,rdd2])</span><br><span class="line">b = b.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x,<span class="number">1</span>))</span><br><span class="line">r_b = b.reduceByKey(<span class="keyword">lambda</span> a,b: a+b)</span><br><span class="line">r_b.pprint()</span><br><span class="line">ssc.start()</span><br><span class="line">ssc.stop(stopSparkContext=<span class="literal">True</span>,stopGraceFully=<span class="literal">True</span>)<span class="comment"># 优雅关闭</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output </span></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">2022</span>-05-<span class="number">13</span> <span class="number">19</span>:<span class="number">25</span>:<span class="number">45</span></span><br><span class="line">-------------------------------------------</span><br><span class="line">(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="number">6</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="number">9</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">2022</span>-05-<span class="number">13</span> <span class="number">19</span>:<span class="number">25</span>:<span class="number">50</span></span><br><span class="line">-------------------------------------------</span><br><span class="line">(<span class="number">12</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="number">11</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="number">13</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 可见 是按照 rdd 队列 一个rdd一个rdd 来处理的。</span></span><br></pre></td></tr></table></figure>



<h4 id="四、kafuka（Structured-Streaming-实现）"><a href="#四、kafuka（Structured-Streaming-实现）" class="headerlink" title="四、kafuka（Structured Streaming 实现）"></a>四、kafuka（Structured Streaming 实现）</h4><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-18%2021.33.48.jpg" alt="截屏2022-05-18 21.33.48"></p>
<p>​		或者是</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-18%2021.36.11.jpg" alt="截屏2022-05-18 21.36.11"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-18%2021.45.14.jpg" alt="截屏2022-05-18 21.45.14">		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-18%2021.45.43.jpg" alt="截屏2022-05-18 21.45.43"></p>
<p>​		Approach 2: Direct Approach (No Receivers) 直连kafka模式在Spark1.3中支持Scala和Java，在1.4中可以支持Python。</p>
<h5 id="4-1-structured接收kafka消息"><a href="#4-1-structured接收kafka消息" class="headerlink" title="4.1 structured接收kafka消息"></a>4.1 structured接收kafka消息</h5><p>​		可以参考：</p>
<p>​		<code>https://juejin.cn/post/6844903460169580558</code>	,</p>
<p>​		<code>https://www.cnblogs.com/leimu/p/15179692.html</code>,	</p>
<p>​		<code>https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html</code> <strong>这是我们接下来用到的structured-streaming介绍</strong>。</p>
<blockquote>
<p>​		我们知道了，自Spark 2.4起，Spark Streaming已被逐步弃用，尤其是pyspark，在spark3之后 <code>from pyspark.streaming.kafka import KafkaUtils</code> 就已经没法用了，因为spark3之后spark文件夹中的pyspark.streaming下面已经没有 kafuka.py了。</p>
<p>​		Apache基金会 已经把发展重点放在了 Spark Structured Streaming，我们这里直接用Spark Structured Streaming 来代替 sparkstreaming。</p>
<p>​		Structured Streaming 的关键思想是将持续不断的数据当做一个<strong>不断追加的表</strong>。这使得[流式计算]模型与批处理计算引擎十分相似。使用类似对于静态表的批处理方式来表达流计算，然后 Spark 以在无限表上的增量计算来运行。continuous mode 是传统的流处理模式，通过运行一个 long-running 的 operator 用来处理数据。之前 Spark Streaming是基于 <strong>micro-batch</strong> 模式的，就被很多人诟病不是“真正的”流式处理。</p>
<p>​		Structured Streaming （结构化流）是一种基于 Spark SQL 引擎构建的可扩展且容错的 stream processing engine （流处理引擎）。可以使用Dataset&#x2F;DataFrame API 来表示 streaming aggregations （流聚合）， event-time windows （事件时间窗口）， stream-to-batch joins （流到批处理连接） 等。简而言之，Structured Streaming 提供快速，可扩展，容错，end-to-end exactly-once stream processing （端到端的完全一次性流处理），且无需用户理解 streaming 。</p>
</blockquote>
<h6 id="1-导入依赖："><a href="#1-导入依赖：" class="headerlink" title="1.导入依赖："></a>1.导入依赖：</h6><p>​				Kafka和Flume等高级输入源，需要依赖独立的库（jar文件）。按		照我们前面安装好的Spark版本安装。我们查看自己的Scala和kafka、		spark版本号。</p>
<p>​				<code>cd /opt/kafka/kafka_2.13-3.1.0/libs</code><br>​				<code>/opt/spark/spark/bin/pyspark</code></p>
<p>​				可见 kafka_2.13-3.1.0.jar，前面是Scala版本2.13后面是kafka版本		3.1.0，spark是3.2.1。但是我们去spark 的spark-shell 运行，可知其实运		行在spark中的scala是2.12版本！！！所以我们实际导入的jar包要求是		Scala2.12版本！！！</p>
<p>​				在以下网站下载spark和Scala对应版本的	spark- streaming-kafka		补丁，我的是</p>
<p>​		<code>https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10_2.12/3.2.1</code> </p>
<p>​				<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-26%2017.08.04.jpg" alt="截屏2022-05-26 17.08.04"></p>
<p>​				把 <code>spark-sql-kafka-0-10_2.13-3.2.1.jar</code> 放入 <code>/opt/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/jars/</code> 下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">有的教程会说要添加某某的 pom依赖，举个例子来解释：</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">搭建开发环境需要引入kafka的jar包，一种方式是将Kafka安装包中lib下的jar包加入到项目的classpath中，这种比较简单了。不过我们使用另一种更加流行的方式：使用maven管理jar包依赖。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建好maven项目后，在pom.xml中添加以下依赖：</span></span><br><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql-kafka-0-10_2.13&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.2.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>​		pyspark中调用包间接使用maven，不用在pom.xml文件中配置，但是每次用要 用如下写法间接调用maven的jar包，有点麻烦额，毕竟python只是干儿子。	</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 /opt/data/sparkstreaming/structured_receive_from_kafuka.py</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">--packages 后面跟的是maven格式中的groupId:artifactId:version,中间用 : 隔开。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">运行一次以后，我们不用去管，等待一阵子，maven会被自动下载好，并且上面的相关依赖jar包也会下载好。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">之后程序正常运行。</span></span><br></pre></td></tr></table></figure>



<h6 id="2-建立kafka生产者"><a href="#2-建立kafka生产者" class="headerlink" title="2.建立kafka生产者"></a>2.建立kafka生产者</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动zookeeper集群，在每台机器上开启。</span></span><br><span class="line">/opt/zookeeper/bin/zkServer.sh start</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">集群各节点 启动 kafka 服务。</span></span><br><span class="line">/opt/kafka/kafka_2.13-3.1.0/bin/kafka-server-start.sh -daemon /opt/kafka/kafka_2.13-3.1.0/config/server.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开启一个topic</span></span><br><span class="line">kafka-topics.sh --bootstrap-server node01:<span class="number">9092</span> --create --topic kafka-produce-topic --partitions <span class="number">3</span> --replication-factor <span class="number">2</span></span><br><span class="line"></span><br><span class="line">vim kafka-produce.py</span><br><span class="line">  </span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> kafka <span class="keyword">import</span> KafkaProducer</span><br><span class="line"><span class="keyword">from</span> kafka.errors <span class="keyword">import</span> kafka_errors</span><br><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_producer</span>():</span><br><span class="line">  	<span class="comment"># 假设生产的消息为键值对（不是一定要键值对），且序列化方式为json</span></span><br><span class="line">    producer = KafkaProducer(</span><br><span class="line">        bootstrap_servers=[<span class="string">&#x27;node01:9092&#x27;</span>],</span><br><span class="line">        acks=<span class="number">1</span>,</span><br><span class="line">        key_serializer=<span class="keyword">lambda</span> k: json.dumps(k).encode(),</span><br><span class="line">        value_serializer=<span class="keyword">lambda</span> v: json.dumps(v).encode()</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">10</span>):</span><br><span class="line">      	<span class="comment"># send（）return 一个future metadata类型。</span></span><br><span class="line">        future = producer.send(</span><br><span class="line">            <span class="string">&#x27;kafka-produce-topic&#x27;</span>,</span><br><span class="line">            key=<span class="string">&#x27;count_num&#x27;</span>,  <span class="comment"># 同一个key值，会被送至同一个分区</span></span><br><span class="line">            value=<span class="built_in">str</span>(i),</span><br><span class="line">            partition=<span class="number">2</span>   <span class="comment"># 向分区2发送消息</span></span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;send &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(i)))</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">          	<span class="comment"># 有 get() 就会变成同步操作，一定要等反馈才会发送下一条消息</span></span><br><span class="line">            <span class="comment"># 无 get() 是异步操作，无需等待反馈直接发送消息</span></span><br><span class="line">            <span class="comment"># get() 会触发真实_send(),像把锁，锁住下个真实的_send()</span></span><br><span class="line">            <span class="comment"># get() 返回的是recordmetadata类型，可以看数据到底是发送到了哪个分区，偏移量是多少。</span></span><br><span class="line">            future.get(timeout=<span class="number">10</span>)  <span class="comment"># 监控是否发送成功   </span></span><br><span class="line">            <span class="built_in">print</span>( <span class="string">&#x27;producer send %d is ok! &#x27;</span>%(i))</span><br><span class="line">            <span class="built_in">print</span>(<span class="built_in">type</span>(future))  <span class="comment"># FutureRecordMetadata 类型</span></span><br><span class="line">        <span class="keyword">except</span> kafka_errors:</span><br><span class="line">            traceback.format_exc()</span><br><span class="line">		producer.close()</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    create_producer()</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">运行kafka生产者</span></span><br><span class="line">python kafka-produce.py</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看topic列表</span></span><br><span class="line">kafka-topics.sh --bootstrap-server node01:9092 --list</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">验证创建的 topic</span> </span><br><span class="line">kafka-topics.sh --describe --topic kafka-produce-topic --bootstrap-server node01:9092			</span><br></pre></td></tr></table></figure>

<h6 id="3-收取kafka数据"><a href="#3-收取kafka数据" class="headerlink" title="3.收取kafka数据"></a>3.收取kafka数据</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType,StructType</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Reading Data from Kafka</span></span><br><span class="line"><span class="string">Creating a Kafka Source for Streaming Queries</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;structured_receive_kafka&#x27;</span>).\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>,<span class="string">&#x27;2&#x27;</span>).\</span><br><span class="line">        getOrCreate()</span><br><span class="line">    df = spark.readStream.<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&#x27;subscribe&#x27;</span>,<span class="string">&#x27;kafka-produce-topic&#x27;</span>).\</span><br><span class="line">        load()</span><br><span class="line">    <span class="comment"># df.show()</span></span><br><span class="line">    nums = df.selectExpr( <span class="string">&quot;CAST(value AS STRING)&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(df),df)</span><br><span class="line">		</span><br><span class="line">    <span class="comment"># 控制台显示，没有存去文件夹了。</span></span><br><span class="line">    query = nums.writeStream \</span><br><span class="line">        .<span class="built_in">format</span>(<span class="string">&#x27;console&#x27;</span>)\</span><br><span class="line">        .outputMode(<span class="string">&#x27;append&#x27;</span>)\</span><br><span class="line">        .start()</span><br><span class="line">    query.awaitTermination()</span><br></pre></td></tr></table></figure>

<p>​		执行步骤：</p>
<p>​		1、运行pyspark的structure-stream接收程序。</p>
<p>​		<code>bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 /opt/data/sparkstreaming/structured_receive_from_kafuka.py</code></p>
<p>​		2、运行kafka的生产程序。</p>
<p>​		<code>python kafka-produce.py </code></p>
<p>​		终端可见成功接收kafka生产的数据：</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-27%2018.28.50.jpg" alt="截屏2022-05-27 18.28.50"></p>
<h2 id="SparkStreaming-应用窗口计算"><a href="#SparkStreaming-应用窗口计算" class="headerlink" title="SparkStreaming 应用窗口计算"></a>SparkStreaming 应用窗口计算</h2><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-16%2011.07.35.jpg" alt="截屏2022-05-16 11.07.35"></p>
<p>​		这里的窗口和 hive 、spark 之前说的窗口函数不是一回事，这里的意思是用一个窗口把一个时间段框起来，每隔多久，用Dstream 来计算一次。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-16%2011.27.53.jpg" alt="截屏2022-05-16 11.27.53"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-16%2011.28.44.jpg" alt="截屏2022-05-16 11.28.44"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="comment"># idea内看看源码，加深对dtream的理解，reduceByKeyAndWindow 是一个滑动的计算过程，窗口区间和滑动区间必须是ssc生成rdd间隔时间的整数倍。</span></span><br><span class="line"><span class="comment"># must be a multiple of this DStream&#x27;s batching interval</span></span><br><span class="line"><span class="comment"># 注意 dstream的 rdd 每隔5秒一直在生成并计算，不管有无数据输入。</span></span><br><span class="line"><span class="comment"># 但是 windows区间框定了 实际计算区间，可以往回算一段时间内的数据。</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0 准备环境</span></span><br><span class="line">    conf = SparkConf()</span><br><span class="line">    conf.setAppName(<span class="string">&#x27;TestDStream&#x27;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&#x27;local[*]&#x27;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">5</span>)</span><br><span class="line">    <span class="comment"># 1 加载数据</span></span><br><span class="line">    ssc.checkpoint(directory=<span class="string">&#x27;file:///opt/data/sparkstream_checkpoint&#x27;</span>)</span><br><span class="line">    lines = ssc.socketTextStream(<span class="string">&#x27;node01&#x27;</span>,<span class="number">9999</span>)</span><br><span class="line">    <span class="comment"># 2 处理数据</span></span><br><span class="line">    counts = lines.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot; &quot;</span>)). \</span><br><span class="line">    <span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x,<span class="number">1</span>)). \</span><br><span class="line">    reduceByKeyAndWindow(func=<span class="keyword">lambda</span> a,b:a+b,invFunc=<span class="literal">None</span>,windowDuration=<span class="number">10</span>,slideDuration=<span class="number">5</span>)</span><br><span class="line">    <span class="comment"># 3 输出结果</span></span><br><span class="line">    counts.pprint()</span><br><span class="line">    <span class="comment"># 4 启动并等待结束</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    <span class="comment"># 5 关闭资源</span></span><br><span class="line">    ssc.stop(stopSparkContext=<span class="literal">True</span>,stopGraceFully=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-16%2016.21.10.jpg" alt="截屏2022-05-16 16.21.10"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># def transform(self, func):</span></span><br><span class="line"><span class="comment">#     &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment">#     Return a new DStream in which each RDD is generated by applying a function</span></span><br><span class="line"><span class="comment">#     on each RDD of this DStream.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     `func` can have one argument of `rdd`, or have two arguments of</span></span><br><span class="line"><span class="comment">#     (`time`, `rdd`)</span></span><br><span class="line"><span class="comment">#     &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment">#     if func.__code__.co_argcount == 1:</span></span><br><span class="line"><span class="comment">#         oldfunc = func</span></span><br><span class="line"><span class="comment">#         func = lambda t, rdd: oldfunc(rdd)</span></span><br><span class="line"><span class="comment">#     assert func.__code__.co_argcount == 2, &quot;func should take one or two arguments&quot;</span></span><br><span class="line"><span class="comment">#     return TransformedDStream(self, func)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一下为百度热搜榜单，我们选top3。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">上海分阶段复商复市 上海分阶段复商复市 上海分阶段复商复市 上海分阶段复商复市 上海分阶段复商复市 上海分阶段复商复市</span></span><br><span class="line"><span class="string">四川邻水一周现499例感染者 已外溢 四川邻水一周现499例感染者 已外溢</span></span><br><span class="line"><span class="string">围观火爆全网的国字脸猴子</span></span><br><span class="line"><span class="string">粮食是国之大者咱们一起端稳热 粮食是国之大者咱们一起端稳热 粮食是国之大者咱们一起端稳热</span></span><br><span class="line"><span class="string">鸡鸭吃泡酒桑葚醉死 主人含泪吃3碗</span></span><br><span class="line"><span class="string">1992年茅台起拍价3999万元新</span></span><br><span class="line"><span class="string">河北磁县回应农民春耕办不了通行证 河北磁县回应农民春耕办不了通行证</span></span><br><span class="line"><span class="string">男子购买“疫情险” 男子购买“疫情险” 男子购买“疫情险” 男子购买“疫情险” 男子购买“疫情险” 男子购买“疫情险” 男子购买“疫情险” 男子购买“疫情险”</span></span><br><span class="line"><span class="string">学生反映饭菜难吃被回复没良心新</span></span><br><span class="line"><span class="string">夫人锐利一眼 韩总统放下酒杯</span></span><br><span class="line"><span class="string">记者实拍亚速钢铁厂激烈交战</span></span><br><span class="line"><span class="string">成都天空现黑圈 官方：电力设施故障新 成都天空现黑圈 官方：电力设施故障新 成都天空现黑圈 官方：电力设施故障新 成都天空现黑圈 官方：电力设施故障新</span></span><br><span class="line"><span class="string">苍山13人差一两公里就到飞机坠毁地</span></span><br><span class="line"><span class="string">央媒：“井盖吃人”顽疾必须治</span></span><br><span class="line"><span class="string">不明原因儿童急性肝炎最新发现</span></span><br><span class="line"><span class="string">俄军在乌每天消耗61亿元人民币</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为dstream 调用 transform（func），这个func方法只能返回rdd，</span></span><br><span class="line"><span class="comment"># 所以 利用rdd的action算子返回list 这样的方法不可取，</span></span><br><span class="line"><span class="comment"># 只能在func内直接输出top n，每次调用此func方法就算一遍top n。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dstream_sorted</span>(<span class="params">rdd</span>):</span><br><span class="line">    rdd2 = rdd.sortBy(<span class="keyword">lambda</span> x: x[<span class="number">1</span>],ascending=<span class="literal">False</span>,numPartitions=<span class="number">1</span>)</span><br><span class="line">    rdd2_top3_list = rdd2.take(<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># output top N </span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;===== top3 =====&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(rdd2_top3_list)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;===== top3 =====&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> rdd2</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0 准备环境</span></span><br><span class="line">    conf = SparkConf()</span><br><span class="line">    conf.setAppName(<span class="string">&#x27;TestDStream&#x27;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&#x27;local[*]&#x27;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">5</span>)</span><br><span class="line">    <span class="comment"># 1 加载数据</span></span><br><span class="line">    ssc.checkpoint(directory=<span class="string">&#x27;file:///opt/data/sparkstream_checkpoint&#x27;</span>)</span><br><span class="line">    lines = ssc.socketTextStream(<span class="string">&#x27;node01&#x27;</span>,<span class="number">9999</span>)</span><br><span class="line">    <span class="comment"># 2 处理数据</span></span><br><span class="line">    counts = lines.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot; &quot;</span>)). \</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x,<span class="number">1</span>)). \</span><br><span class="line">        reduceByKeyAndWindow(func=<span class="keyword">lambda</span> a,b:a+b,invFunc=<span class="literal">None</span>,windowDuration=<span class="number">10</span>,slideDuration=<span class="number">5</span>)</span><br><span class="line">    counts_sorted = counts.transform(dstream_sorted)</span><br><span class="line">    <span class="comment"># 3 输出结果</span></span><br><span class="line">    counts_sorted.pprint()</span><br><span class="line">    <span class="comment"># 4 启动并等待结束</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    <span class="comment"># 5 关闭资源</span></span><br><span class="line">    ssc.stop(stopSparkContext=<span class="literal">True</span>,stopGraceFully=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>​		在nc 输入 上述 百度热搜榜单后，可见一个窗口时间内输出如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output </span></span><br><span class="line">===== top3 =====</span><br><span class="line">[(<span class="string">&#x27;男子购买“疫情险”&#x27;</span>, <span class="number">8</span>), (<span class="string">&#x27;上海分阶段复商复市&#x27;</span>, <span class="number">6</span>), (<span class="string">&#x27;官方：电力设施故障新&#x27;</span>, <span class="number">4</span>)]</span><br><span class="line">===== top3 =====</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">2022</span>-05-<span class="number">16</span> 03:<span class="number">33</span>:<span class="number">15</span></span><br><span class="line">-------------------------------------------</span><br><span class="line">(<span class="string">&#x27;男子购买“疫情险”&#x27;</span>, <span class="number">8</span>)</span><br><span class="line">(<span class="string">&#x27;上海分阶段复商复市&#x27;</span>, <span class="number">6</span>)</span><br><span class="line">(<span class="string">&#x27;官方：电力设施故障新&#x27;</span>, <span class="number">4</span>)</span><br><span class="line">(<span class="string">&#x27;成都天空现黑圈&#x27;</span>, <span class="number">4</span>)</span><br><span class="line">(<span class="string">&#x27;粮食是国之大者咱们一起端稳热&#x27;</span>, <span class="number">3</span>)</span><br><span class="line">(<span class="string">&#x27;河北磁县回应农民春耕办不了通行证&#x27;</span>, <span class="number">2</span>)</span><br><span class="line">(<span class="string">&#x27;四川邻水一周现499例感染者&#x27;</span>, <span class="number">2</span>)</span><br><span class="line">(<span class="string">&#x27;已外溢&#x27;</span>, <span class="number">2</span>)</span><br><span class="line">(<span class="string">&#x27;围观火爆全网的国字脸猴子&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="string">&#x27;鸡鸭吃泡酒桑葚醉死&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后面窗口往后推移，又没有新的nc输入，输出就变成了空。</span></span><br><span class="line">===== top3 =====</span><br><span class="line">[]</span><br><span class="line">===== top3 =====</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">2022</span>-05-<span class="number">16</span> 03:<span class="number">36</span>:<span class="number">10</span></span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">===== top3 =====</span><br><span class="line">[]</span><br><span class="line">===== top3 =====</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">2022</span>-05-<span class="number">16</span> 03:<span class="number">36</span>:<span class="number">15</span></span><br><span class="line">-------------------------------------------</span><br></pre></td></tr></table></figure>

<h3 id="自定义输出"><a href="#自定义输出" class="headerlink" title="自定义输出"></a>自定义输出</h3><p>​		dstream的<code>pprint()</code> 源代码就是调用了<code>foreachRDD()</code>方法，自定义输出就是我们自己用 <code>foreachRDD()</code> 方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># def transform(self, func):</span></span><br><span class="line"><span class="comment">#     &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment">#     Return a new DStream in which each RDD is generated by applying a function</span></span><br><span class="line"><span class="comment">#     on each RDD of this DStream.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     `func` can have one argument of `rdd`, or have two arguments of</span></span><br><span class="line"><span class="comment">#     (`time`, `rdd`)</span></span><br><span class="line"><span class="comment">#     &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment">#     if func.__code__.co_argcount == 1:</span></span><br><span class="line"><span class="comment">#         oldfunc = func</span></span><br><span class="line"><span class="comment">#         func = lambda t, rdd: oldfunc(rdd)</span></span><br><span class="line"><span class="comment">#     assert func.__code__.co_argcount == 2, &quot;func should take one or two arguments&quot;</span></span><br><span class="line"><span class="comment">#     return TransformedDStream(self, func)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># def foreachRDD(self, func):</span></span><br><span class="line"><span class="comment">#     &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment">#     Apply a function to each RDD in this DStream.</span></span><br><span class="line"><span class="comment">#     &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment">#     if func.__code__.co_argcount == 1:</span></span><br><span class="line"><span class="comment">#         old_func = func</span></span><br><span class="line"><span class="comment">#         func = lambda t, rdd: old_func(rdd)</span></span><br><span class="line"><span class="comment">#     jfunc = TransformFunction(self._sc, func, self._jrdd_deserializer)</span></span><br><span class="line"><span class="comment">#     api = self._ssc._jvm.PythonDStream</span></span><br><span class="line"><span class="comment">#     api.callForeachRDD(self._jdstream, jfunc)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一下为百度热搜榜单，我们选top3。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">上海分阶段复商复市 上海分阶段复商复市 上海分阶段复商复市 上海分阶段复商复市 上海分阶段复商复市 上海分阶段复商复市</span></span><br><span class="line"><span class="string">四川邻水一周现499例感染者 已外溢 四川邻水一周现499例感染者 已外溢</span></span><br><span class="line"><span class="string">围观火爆全网的国字脸猴子</span></span><br><span class="line"><span class="string">粮食是国之大者咱们一起端稳热 粮食是国之大者咱们一起端稳热 粮食是国之大者咱们一起端稳热</span></span><br><span class="line"><span class="string">鸡鸭吃泡酒桑葚醉死 主人含泪吃3碗</span></span><br><span class="line"><span class="string">1992年茅台起拍价3999万元新</span></span><br><span class="line"><span class="string">河北磁县回应农民春耕办不了通行证 河北磁县回应农民春耕办不了通行证</span></span><br><span class="line"><span class="string">男子购买“疫情险” 男子购买“疫情险” 男子购买“疫情险” 男子购买“疫情险” 男子购买“疫情险” 男子购买“疫情险” 男子购买“疫情险” 男子购买“疫情险”</span></span><br><span class="line"><span class="string">学生反映饭菜难吃被回复没良心新</span></span><br><span class="line"><span class="string">夫人锐利一眼 韩总统放下酒杯</span></span><br><span class="line"><span class="string">记者实拍亚速钢铁厂激烈交战</span></span><br><span class="line"><span class="string">成都天空现黑圈 官方：电力设施故障新 成都天空现黑圈 官方：电力设施故障新 成都天空现黑圈 官方：电力设施故障新 成都天空现黑圈 官方：电力设施故障新</span></span><br><span class="line"><span class="string">苍山13人差一两公里就到飞机坠毁地</span></span><br><span class="line"><span class="string">央媒：“井盖吃人”顽疾必须治</span></span><br><span class="line"><span class="string">不明原因儿童急性肝炎最新发现</span></span><br><span class="line"><span class="string">俄军在乌每天消耗61亿元人民币</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dstream_sorted</span>(<span class="params">rdd</span>):</span><br><span class="line">    rdd2 = rdd.sortBy(<span class="keyword">lambda</span> x: x[<span class="number">1</span>],ascending=<span class="literal">False</span>,numPartitions=<span class="number">1</span>)</span><br><span class="line">    rdd2_top3_list = rdd2.take(<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># output top N</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;===== top3 =====&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(rdd2_top3_list)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;===== top3 =====&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> rdd2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cdo</span>(<span class="params">datetime,rdd</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;===== constomer defined output =====&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(datetime)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;===== constomer defined output =====&#x27;</span>)</span><br><span class="line">    rdd.persist(StorageLevel.MEMORY_ONLY)</span><br><span class="line">    rdd.foreach(<span class="keyword">lambda</span> x: <span class="built_in">print</span>(x))</span><br><span class="line">    datetime_str = re.sub(<span class="string">&#x27;[-: ]&#x27;</span>,<span class="string">&#x27;&#x27;</span>,<span class="built_in">str</span>(datetime)) <span class="comment"># hdfs不识别文件名中有“:”符号，mysql不识别&quot;:- &quot;。</span></span><br><span class="line">    <span class="built_in">print</span>(datetime_str)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(rdd),<span class="string">&#x27;\n has toDF Func? :&#x27;</span>,<span class="built_in">hasattr</span>(rdd,<span class="string">&#x27;toDF&#x27;</span>)) <span class="comment"># 有 SparkSession 对象的生成，rdd中才有toDF补丁方法。</span></span><br><span class="line">    <span class="comment"># dstream中 每隔5秒钟就有 rdd生成，来试图计算可能来的数据。</span></span><br><span class="line">    <span class="comment"># 但是如果没有数据发送过来，会生成一大堆内容是空的rdd，这样的rdd我们就不储存了，节省空间。</span></span><br><span class="line">    <span class="keyword">if</span> rdd.isEmpty() == <span class="literal">False</span> :</span><br><span class="line">        <span class="comment"># 储存到hdfs 中，储存到本地方式也一样。</span></span><br><span class="line">        rdd.repartition(<span class="number">1</span>).saveAsTextFile(<span class="string">&#x27;hdfs://node01:9000/spark_study/sparkstream_study/&#x27;</span>+datetime_str)</span><br><span class="line">        <span class="comment"># 通过jdbc储存到mysql中。</span></span><br><span class="line">        <span class="comment"># 由于mysql是结构化数据，所以我们也先把rdd通过toDF转为dateframe结构化数据，借助dateframe经jdbc存入mysql的套路。</span></span><br><span class="line">        <span class="comment"># 我们在《spark_core.md》中的《储存到jdbc》详细写过。</span></span><br><span class="line">        df = rdd.toDF([<span class="string">&#x27;key&#x27;</span>,<span class="string">&#x27;num_of_search&#x27;</span>])</span><br><span class="line">        df.write.mode(<span class="string">&#x27;overwrite&#x27;</span>). \</span><br><span class="line">            <span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>). \</span><br><span class="line">            option(<span class="string">&#x27;url&#x27;</span>,<span class="string">&#x27;jdbc:mysql://node01:3306/spark_databases?useSSL=false&amp;useUnicode=true&#x27;</span>). \</span><br><span class="line">            option(<span class="string">&#x27;dbtable&#x27;</span>,<span class="string">&#x27;hotsearch_table&#x27;</span>+datetime_str). \</span><br><span class="line">            option(<span class="string">&#x27;user&#x27;</span>,<span class="string">&#x27;root&#x27;</span>). \</span><br><span class="line">            option(<span class="string">&#x27;password&#x27;</span>,<span class="string">&#x27;cys123456&#x27;</span>). \</span><br><span class="line">            save()</span><br><span class="line">    rdd.unpersist()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0 准备环境</span></span><br><span class="line">    conf = SparkConf()</span><br><span class="line">    conf.setAppName(<span class="string">&#x27;TestDStream&#x27;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&#x27;local[*]&#x27;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    sparksession = SparkSession(sc) <span class="comment"># 没有 SparkSession 对象的生成，后续rdd中就没有toDF()这个补丁方法，在储存到mysql时用到。</span></span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">5</span>)</span><br><span class="line">    <span class="comment"># 1 加载数据</span></span><br><span class="line">    ssc.checkpoint(directory=<span class="string">&#x27;file:///opt/data/sparkstream_checkpoint&#x27;</span>)</span><br><span class="line">    lines = ssc.socketTextStream(<span class="string">&#x27;node01&#x27;</span>,<span class="number">9999</span>)</span><br><span class="line">    <span class="comment"># 2 处理数据</span></span><br><span class="line">    counts_stream = lines.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot; &quot;</span>)). \</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x,<span class="number">1</span>)). \</span><br><span class="line">        reduceByKeyAndWindow(func=<span class="keyword">lambda</span> a,b:a+b,invFunc=<span class="literal">None</span>,windowDuration=<span class="number">10</span>,slideDuration=<span class="number">5</span>)</span><br><span class="line">    counts_sorted_stream = counts_stream.transform(dstream_sorted)</span><br><span class="line">    <span class="comment"># 3 输出结果</span></span><br><span class="line">    counts_sorted_stream.pprint()</span><br><span class="line">    <span class="comment"># 自定义输出，foreachRDD 不同于 transform，前者没有返回值，参数方法也没有返回值。</span></span><br><span class="line">    counts_sorted_stream.foreachRDD(cdo)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4 启动并等待结束</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    <span class="comment"># 5 关闭资源</span></span><br><span class="line">    ssc.stop(stopSparkContext=<span class="literal">True</span>,stopGraceFully=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>​		在nc端， 10秒钟之内输入上述热搜内容4次，在mysql对应文件夹和hdfs对应文件夹内可见：</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-17%2012.48.56.jpg" alt="截屏2022-05-17 12.48.56"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-17%2012.49.21.jpg" alt="截屏2022-05-17 12.49.21"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-17%2012.49.54.jpg" alt="截屏2022-05-17 12.49.54"></p>
<p>​		然后随着时间的推移，窗口向后滑动5秒，counts_stream把10秒钟内的历史rdd再都算一遍。<code>男子购买“疫情险”</code> num_of_search 可能会变成32、24、16、8、或者为rdd为空不再储存……</p>
<p>​		再随着时间的推移，窗口向后滑动5秒，counts_stream把10秒钟内的历史rdd再都算一遍。发现counts_stream内的rdd为空，不再储存。其他key 依此类推。</p>
<h1 id="spark-structure-stream学习"><a href="#spark-structure-stream学习" class="headerlink" title="spark- structure-stream学习"></a>spark- structure-stream学习</h1><p>​		前面在spark接收kafka数据的案例中已经用过了spark-structure-stream ，但是在这里我们在补充一些案例。</p>
<p>​		参考：</p>
<p><code>https://blog.csdn.net/qq_33689414/article/details/86469267</code></p>
<p><code>https://blog.csdn.net/weixin_35154281/article/details/116883816?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-116883816-blog-86469267.pc_relevant_antiscanv3&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-116883816-blog-86469267.pc_relevant_antiscanv3&amp;utm_relevant_index=2</code>		</p>
<p><code>https://spark.apache.org/docs/3.2.1/structured-streaming-kafka-integration.html</code></p>
<p><code>https://www.bilibili.com/video/BV1WV411y7NQ?p=6&amp;spm_id_from=pageDriver</code></p>
<h2 id="kafka-streaming"><a href="#kafka-streaming" class="headerlink" title="kafka- streaming"></a>kafka- streaming</h2><p><code>vim kafka_structure_streaming.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">spark = SparkSession.builder\</span><br><span class="line">    .appName(<span class="string">&#x27;structure_word_count&#x27;</span>)\</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)  <span class="comment"># 只显示ERROR日志。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df = spark.readStream\</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>)\</span><br><span class="line">    .option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>)\</span><br><span class="line">    .option(<span class="string">&#x27;subscribe&#x27;</span>,<span class="string">&#x27;kafka-structure-stream&#x27;</span>)\</span><br><span class="line">    .load()</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df.writeStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;console&#x27;</span>) \</span><br><span class="line">    .outputMode(<span class="string">&#x27;update&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;truncate&#x27;</span>,<span class="literal">False</span>)\ </span><br><span class="line">    .start() \</span><br><span class="line">    .awaitTermination()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<p>​		kafka 有标准的输出模式</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server node01:9092 --create --topic kafka-structure-stream --partitions 3 --replication-factor 2</span><br><span class="line"></span><br><span class="line">kafka-console-producer.sh --broker-list node01:9092 --topic kafka-structure-stream</span><br><span class="line"></span><br><span class="line">spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 /opt/data/spark-structure-streaming/kafka_structure_streaming.py</span><br></pre></td></tr></table></figure>

<p>​		可见kafka标准输出格式。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2011.24.17.jpg" alt="截屏2022-05-28 11.24.17"></p>
<p>​		在kafka的生产者输入 <code>a b c</code> spark终端可见：</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2014.51.51.jpg" alt="截屏2022-05-28 14.51.51"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2015.00.14.jpg" alt="截屏2022-05-28 15.00.14"></p>
<p>​		可以看见kafka的标准输入数据类型。所以一般我们在接收kafka数据时给个类型转换。</p>
<p>​		一个最简单的流式词频统计，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">spark = SparkSession.builder\</span><br><span class="line">    .appName(<span class="string">&#x27;structure_word_count&#x27;</span>)\</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)  <span class="comment"># 只显示ERROR日志。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df = spark.readStream\</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>)\</span><br><span class="line">    .option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>)\</span><br><span class="line">    .option(<span class="string">&#x27;subscribe&#x27;</span>,<span class="string">&#x27;kafka-structure-stream&#x27;</span>)\</span><br><span class="line">    .load()\</span><br><span class="line">    .selectExpr(<span class="string">&#x27;cast(value as string)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df = df.select(F.explode( F.split(df.value,<span class="string">&#x27; &#x27;</span>)).alias(<span class="string">&#x27;value&#x27;</span>))\</span><br><span class="line">    .groupBy(<span class="string">&#x27;value&#x27;</span>)\</span><br><span class="line">    .count()</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df.writeStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;console&#x27;</span>) \</span><br><span class="line">    .outputMode(<span class="string">&#x27;update&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;truncate&quot;</span>,<span class="literal">False</span>) \</span><br><span class="line">    .start() \</span><br><span class="line">    .awaitTermination()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<p>在 kafka生产者输入 <code>a b c a</code> ，终端可见</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2015.26.21.jpg" alt="截屏2022-05-28 15.26.21"></p>
<p>​		一个最简单的批处理batch query，词频统计，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">spark = SparkSession.builder\</span><br><span class="line">    .appName(<span class="string">&#x27;structure_word_count&#x27;</span>)\</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)  <span class="comment"># 只显示ERROR日志。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df = spark.read\</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>)\</span><br><span class="line">    .option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>)\</span><br><span class="line">    .option(<span class="string">&#x27;subscribe&#x27;</span>,<span class="string">&#x27;kafka-structure-stream&#x27;</span>)\</span><br><span class="line">    .option(<span class="string">&#x27;startingOffsets&#x27;</span>,<span class="string">&#x27;earliest&#x27;</span>)\</span><br><span class="line">    .option(<span class="string">&#x27;endingOffsets&#x27;</span>,<span class="string">&#x27;latest&#x27;</span>)\</span><br><span class="line">    .load()\</span><br><span class="line">    .selectExpr(<span class="string">&#x27;cast(value as string)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df = df.select(F.explode( F.split(df.value,<span class="string">&#x27; &#x27;</span>)).alias(<span class="string">&#x27;value&#x27;</span>))\</span><br><span class="line">    .groupBy(<span class="string">&#x27;value&#x27;</span>)\</span><br><span class="line">    .count()</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df.write \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;console&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;truncate&quot;</span>,<span class="literal">False</span>) \</span><br><span class="line">    .save()</span><br></pre></td></tr></table></figure>

<p>​		终端可见，kafka生产者还没有新输入内容，kafka的topic内的旧内容被批处理输出了：</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2015.47.52.jpg" alt="截屏2022-05-28 15.47.52"></p>
<p>​		批处理是一次性算完的，没有流式计算的流式。</p>
<h2 id="ratesource"><a href="#ratesource" class="headerlink" title="ratesource"></a>ratesource</h2><p>​		以固定的速率生成固定的格式的数据，用以检测structure streaming 的性能。	</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&#x27;structure_word_count&#x27;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)  <span class="comment"># 只显示ERROR日志。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df = spark.readStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;rate&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;rowsPerSecond&#x27;</span>,<span class="string">&#x27;100&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;rampUpTime&#x27;</span>,<span class="number">1</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;numPartitions&#x27;</span>,<span class="number">3</span>)\</span><br><span class="line">    .load() \</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df.writeStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;console&#x27;</span>) \</span><br><span class="line">    .outputMode(<span class="string">&#x27;update&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;truncate&quot;</span>,<span class="literal">False</span>) \</span><br><span class="line">    .start() \</span><br><span class="line">    .awaitTermination()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<p><code>spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 /opt/data/spark-structure-streaming/rate_source_test.py</code></p>
<p>​		控制台可见</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2018.09.32.jpg" alt="截屏2022-05-28 18.09.32"></p>
<h2 id="window操作"><a href="#window操作" class="headerlink" title="window操作"></a>window操作</h2><p>​		structure-streaming 的window操作一定要求数据有时间戳timestamp。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&#x27;structure_test&#x27;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)  <span class="comment"># 只显示ERROR日志。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df = spark.readStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;subscribe&#x27;</span>,<span class="string">&#x27;kafka-structure-stream&#x27;</span>) \</span><br><span class="line">    .load()</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df.selectExpr(<span class="string">&#x27;cast(value as string)&#x27;</span>,<span class="string">&#x27;timestamp&#x27;</span>)\</span><br><span class="line">    .writeStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;console&#x27;</span>) \</span><br><span class="line">    .outputMode(<span class="string">&#x27;update&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;truncate&quot;</span>,<span class="literal">False</span>) \</span><br><span class="line">    .start() \</span><br><span class="line">    .awaitTermination()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2018.53.40.jpg" alt="截屏2022-05-28 18.53.40"></p>
<p>​		注意：上面的时间戳 实际是spark收到数据的时间点，spark拿收到数据时的时间点 假定为事件发生时间event-time。下图的time就是spark收到数据的时间– 伪event-time。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2011.03.44.jpg" alt="截屏2022-05-28 11.03.44"></p>
<p>对比一下 以下修改dataframe前后的输出结果：</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.05.58.jpg" alt="截屏2022-05-28 21.05.58"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.07.01.jpg" alt="截屏2022-05-28 21.07.01"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.05.40.jpg" alt="截屏2022-05-28 21.05.40"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.08.05.jpg" alt="截屏2022-05-28 21.08.05"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim window_test.py</span></span><br><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&#x27;structure_test&#x27;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)  <span class="comment"># 只显示ERROR日志。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df = spark.readStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;subscribe&#x27;</span>,<span class="string">&#x27;kafka-structure-stream&#x27;</span>) \</span><br><span class="line">    .load()\</span><br><span class="line">    .selectExpr(<span class="string">&#x27;cast(value as string)&#x27;</span>,<span class="string">&#x27;cast(timestamp as string)&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># F中的窗口方法参数，要一个时间戳列和、窗口持续时间、滑动时间。</span></span><br><span class="line">df1 = df.select(F.explode(F.split(df.value,<span class="string">&#x27; &#x27;</span>)).alias(<span class="string">&#x27;word&#x27;</span>),df.timestamp.alias(<span class="string">&#x27;ts&#x27;</span>))</span><br><span class="line">df2 = df1.groupBy(F.window(df1.ts,<span class="string">&#x27;1 minutes&#x27;</span>,<span class="string">&#x27;1 minutes&#x27;</span>),df1.word)\</span><br><span class="line">    .count()</span><br><span class="line"></span><br><span class="line">df2.writeStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;console&#x27;</span>) \</span><br><span class="line">    .outputMode(<span class="string">&#x27;update&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;truncate&quot;</span>,<span class="literal">False</span>) \</span><br><span class="line">    .start() \</span><br><span class="line">    .awaitTermination()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<p><code>spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 /opt/data/spark-structure-streaming/window_test.py</code></p>
<p>​		在kafka生产者输入 <code>a a b</code> ，终端可见：</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.17.55.jpg" alt="截屏2022-05-28 21.17.55"></p>
<p>​		一个常见的错误：</p>
<p><code>org.apache.spark.sql.AnalysisException: resolved attribute(s)</code></p>
<p>​		这是由于 df1 是由前一个df派生出来的，一旦df和df1的属性混乱，会出现这类报错。按照我们上面的例子， <code>df1.groupBy(F.window(df1.ts,&#39;1 minutes&#39;,&#39;1 minutes&#39;),df1.word)</code> 写成了<code> df1.groupBy(F.window(df.ts,&#39;1 minutes&#39;,&#39;1 minutes&#39;),df.word)</code> 就会报此类错误。</p>
<p>​		另外补充一个知识点：</p>
<p>​		<strong>流式计算过程中是不能有dataframe转rdd的操作</strong></p>
<p>​		不然会出现报错：</p>
<p>​		<code>Queries with streaming sources must be executed with writeStream.start()</code> </p>
<h4 id="最大窗口数（重叠数）"><a href="#最大窗口数（重叠数）" class="headerlink" title="最大窗口数（重叠数）"></a>最大窗口数（重叠数）</h4><p>​		最大窗口数maxnumoverlapping &#x3D; 「窗口长度&#x2F;滑动长度」（）向上取整</p>
<p>​		maxnumoverlapping 英文其实更加准确一些，是最大滞留的数目，比如说 windowsduration&#x3D;10，slideduration&#x3D;5，那么maxnumoverlapping&#x3D;10&#x2F;5向上取整&#x3D;2，某个时间点最大滞留在两个窗口内，比如：12最大滞留在两个窗口区间内。<br>$$<br>12 ε [5,15) ||<br>12 ε [10,20)<br>$$<br>​		如下是Scala的窗口时间计算原理：</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-31%2011.56.42.jpg" alt="截屏2022-05-31 11.56.42"></p>
<p>​		我们用一个例子解释上述伪代码：</p>
<p>​		windowsduration&#x3D;10，slideduration&#x3D;5，starttime秒数&#x3D;00，</p>
<p>​		timestamp现在的时间戳&#x3D;52分12秒，</p>
<p>​		maxnumoverlapping&#x3D;10&#x2F;5向上取整&#x3D;2。</p>
<p>​		windowid &#x3D; 52.12&#x2F;5 &#x3D; 11</p>
<p>​		for 循环 会循环出 最大的滞留窗口数：</p>
<p>​				第一个windowstart &#x3D; 11 * 5 + (0-2)*5 + 00 &#x3D; 45.00</p>
<p>​				第二个windowstart &#x3D; 11 * 5 + (1-2)*5 +00 &#x3D; 50.00</p>
<p>​		也就是说 52.00 这个时间点接收了一个hello单词这个 事件，会出现在两个窗口区间内：<br>$$<br>[45.00, 55.00) || [50.00, 60.00)<br>$$<br>​		也就是上图中 window列下的内容。</p>
<h4 id="update模式下加watermask"><a href="#update模式下加watermask" class="headerlink" title="update模式下加watermask"></a>update模式下加watermask</h4><p>​		水印相当于往前找回 迟到的数据返回应该在的位置，水印规定了这个最大的可找回的时间范围。</p>
<p>​		例如，说在12:04（即事件时间）生成的一个字可以在12:11被应用程序接收。应用程序应该使用12:04而不是12:11来更新窗口的较旧计数12:00 - 12:10。这在我们基于窗口的分组中自然发生 - 结构化流可以长时间维持部分聚合的中间状态，以便后期数据可以正确地更新旧窗口的聚合，如下所示。<br><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2011.03.44.jpg" alt="截屏2022-05-28 11.03.44"></p>
<p>​		<strong>就是把落下的数据加到它本来应该在的位置</strong>		</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2011.00.16.jpg" alt="截屏2022-05-28 10.59.05"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.32.02.jpg" alt="截屏2022-05-28 21.32.02"></p>
<p>​		在complete模式下，加水印是没有意义的，因为水印本身是为了舍弃一些超时太久的数据，而complete模式是全输出，根本没有舍弃。</p>
<p>​		以下例子（update模式下的水印）：</p>
<p>​		windowsduration &#x3D; 10，sildeduration &#x3D; 2，watermark &#x3D; 2，最大滞留窗口数&#x3D;10&#x2F;2&#x3D;5。</p>
<p>​		我们在10:55输入两个dog，在11:00输入一个dog，但是10:55输入的一个dog迟到了。被水印找补回来了。</p>
<p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-31%2013.07.08.jpg" alt="截屏2022-05-31 13.07.08"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-31%2013.07.32.jpg" alt="截屏2022-05-31 13.07.32"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-31%2013.08.22.jpg" alt="截屏2022-05-31 13.08.22"></p>
<p>​		尽管10.55的消息在11.00补发来了，但是水印还是保持在之前的10:58。<strong>update模式下的水印用于剔除掉水印时间范围之外的窗口</strong>，最终显示的是满足两种要求的窗口 ：</p>
<p>​		1、水印时间范围之内（这个窗口内包扩了水印）；</p>
<p>​		2、内容发生了变化。</p>
<h4 id="append模式下加watermask"><a href="#append模式下加watermask" class="headerlink" title="append模式下加watermask"></a>append模式下加watermask</h4><p>​		有聚合操作时；</p>
<p>​		还是上面的例子，<strong>append模式是输出那些不可能变化的窗口</strong>，某种意义上和update正好相反，比如说watermask 过期了，update模式会把这样的窗口剔除，而append模式会把这样的窗口输出，因为watermask过期了，窗口不可能再发生变化了。</p>
<p>​		没有聚合操作时；</p>
<p>​		append和update一致，也是只输出更新窗口，水印淘汰超水印时间数据。	</p>
<h4 id="水印机制总结"><a href="#水印机制总结" class="headerlink" title="水印机制总结"></a>水印机制总结</h4><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-31%2016.48.30.jpg" alt="截屏2022-05-31 16.48.30"></p>
<h2 id="去重算子"><a href="#去重算子" class="headerlink" title="去重算子"></a>去重算子</h2><p>​		dropDuplicates(arg&#x3D;iterable) </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&#x27;structure_test&#x27;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df = spark.readStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;subscribe&#x27;</span>,<span class="string">&#x27;kafka-structure-stream&#x27;</span>) \</span><br><span class="line">    .load() \</span><br><span class="line">    .selectExpr(<span class="string">&#x27;cast(value as string)&#x27;</span>,<span class="string">&#x27;timestamp&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df1 = df.select(df.value.alias(<span class="string">&#x27;word&#x27;</span>),df.timestamp.alias(<span class="string">&#x27;ts&#x27;</span>))</span><br><span class="line"></span><br><span class="line">df2 = df1.withWatermark(<span class="string">&#x27;ts&#x27;</span>,<span class="string">&#x27;2 minutes&#x27;</span>)\</span><br><span class="line">    .dropDuplicates([<span class="string">&#x27;word&#x27;</span>])</span><br><span class="line"></span><br><span class="line">df2.writeStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;console&#x27;</span>) \</span><br><span class="line">    .outputMode(<span class="string">&#x27;append&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;truncate&quot;</span>,<span class="literal">False</span>) \</span><br><span class="line">    .trigger(processingTime=<span class="string">&#x27;2 seconds&#x27;</span>) \</span><br><span class="line">    .start() \</span><br><span class="line">    .awaitTermination()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 /opt/data/spark-structure-streaming/dropDuplicates_test.py</span><br></pre></td></tr></table></figure>

<p>​		在kafka生产者输入 a 等4秒再 输入a。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Batch: 3</span><br><span class="line">-------------------------------------------</span><br><span class="line">+----+-----------------------+</span><br><span class="line">|word|ts                     |</span><br><span class="line">+----+-----------------------+</span><br><span class="line">|a   |2022-05-31 02:33:01.487|</span><br><span class="line">+----+-----------------------+</span><br><span class="line"></span><br><span class="line">22/05/31 02:33:10 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 6482 milliseconds</span><br><span class="line">-------------------------------------------</span><br><span class="line">Batch: 4</span><br><span class="line">-------------------------------------------</span><br><span class="line">+----+---+</span><br><span class="line">|word|ts |</span><br><span class="line">+----+---+</span><br><span class="line">+----+---+</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可见第二次输入的a ，被去重了，没有出现在输出中。</span></span><br></pre></td></tr></table></figure>



<h2 id="流与静态数据join"><a href="#流与静态数据join" class="headerlink" title="流与静态数据join"></a>流与静态数据join</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType,StructType,IntegerType,StringType</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&#x27;structure_test&#x27;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df = spark.readStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;subscribe&#x27;</span>,<span class="string">&#x27;kafka-structure-stream&#x27;</span>) \</span><br><span class="line">    .load() \</span><br><span class="line">    .selectExpr(<span class="string">&#x27;cast(value as string)&#x27;</span>,<span class="string">&#x27;timestamp&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df_stream = df.select(F.split(df.value,<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>].alias(<span class="string">&#x27;age&#x27;</span>),</span><br><span class="line">                      F.split(df.value,<span class="string">&#x27; &#x27;</span>)[<span class="number">1</span>].alias(<span class="string">&#x27;sex&#x27;</span>))</span><br><span class="line"></span><br><span class="line">schema = StructType().add(<span class="string">&#x27;name&#x27;</span>,StringType()).add(<span class="string">&#x27;age&#x27;</span>,IntegerType())</span><br><span class="line">rdd = spark.sparkContext.parallelize([[<span class="string">&#x27;zhansan&#x27;</span>,<span class="number">23</span>],[<span class="string">&#x27;lisi&#x27;</span>,<span class="number">24</span>]])</span><br><span class="line">df_static = spark.createDataFrame(rdd,schema=schema)</span><br><span class="line">df_join = df_stream.join(df_static,<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;left&#x27;</span>)<span class="comment"># 左连接只支持stream在左边。可以看源码。</span></span><br><span class="line"></span><br><span class="line">df_join.writeStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;console&#x27;</span>) \</span><br><span class="line">    .outputMode(<span class="string">&#x27;update&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;truncate&quot;</span>,<span class="literal">False</span>) \</span><br><span class="line">    .trigger(processingTime=<span class="string">&#x27;2 seconds&#x27;</span>) \</span><br><span class="line">    .start() \</span><br><span class="line">    .awaitTermination()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在kafka生产者输入：</span></span><br><span class="line">&gt;<span class="number">23</span> m</span><br><span class="line">&gt;<span class="number">24</span> w</span><br><span class="line">&gt;<span class="number">22</span> m</span><br><span class="line">&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行spark程序，终端机出现：</span></span><br><span class="line">-------------------------------------------</span><br><span class="line">Batch: <span class="number">1</span>  左连接 匹配成功。</span><br><span class="line">-------------------------------------------</span><br><span class="line">+---+---+-------+</span><br><span class="line">|age|sex|name   |</span><br><span class="line">+---+---+-------+</span><br><span class="line">|<span class="number">23</span> |m  |zhansan|</span><br><span class="line">+---+---+-------+</span><br><span class="line"></span><br><span class="line"><span class="number">22</span>/05/<span class="number">31</span> 03:<span class="number">42</span>:<span class="number">12</span> WARN ProcessingTimeExecutor: Current batch <span class="keyword">is</span> falling behind. The trigger interval <span class="keyword">is</span> <span class="number">2000</span> milliseconds, but spent <span class="number">4184</span> milliseconds</span><br><span class="line">-------------------------------------------</span><br><span class="line">Batch: <span class="number">2</span>  左连接匹配成功。</span><br><span class="line">-------------------------------------------</span><br><span class="line">+---+---+----+</span><br><span class="line">|age|sex|name|</span><br><span class="line">+---+---+----+</span><br><span class="line">|<span class="number">24</span> |w  |lisi|</span><br><span class="line">+---+---+----+</span><br><span class="line"></span><br><span class="line"><span class="number">22</span>/05/<span class="number">31</span> 03:<span class="number">42</span>:<span class="number">27</span> WARN ProcessingTimeExecutor: Current batch <span class="keyword">is</span> falling behind. The trigger interval <span class="keyword">is</span> <span class="number">2000</span> milliseconds, but spent <span class="number">3375</span> milliseconds</span><br><span class="line">-------------------------------------------</span><br><span class="line">Batch: <span class="number">3</span>  左连接匹配失败，自动填null。</span><br><span class="line">-------------------------------------------</span><br><span class="line">+---+---+----+</span><br><span class="line">|age|sex|name|</span><br><span class="line">+---+---+----+</span><br><span class="line">|<span class="number">22</span> |m  |null|</span><br><span class="line">+---+---+----+</span><br></pre></td></tr></table></figure>



<h2 id="流与流join"><a href="#流与流join" class="headerlink" title="流与流join"></a>流与流join</h2><p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-02%2010.27.01.jpg" alt="截屏2022-06-02 10.27.01"></p>
<p>​		</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType,StructType,IntegerType,StringType</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&#x27;structure_test&#x27;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df_1 = spark.readStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;subscribe&#x27;</span>,<span class="string">&#x27;kafka-structure-stream&#x27;</span>) \</span><br><span class="line">    .load() \</span><br><span class="line">    .selectExpr(<span class="string">&#x27;cast(value as string)&#x27;</span>,<span class="string">&#x27;timestamp&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df_kafka = df_1.select(F.split(df_1.value,<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>].alias(<span class="string">&#x27;age&#x27;</span>),</span><br><span class="line">                      F.split(df_1.value,<span class="string">&#x27; &#x27;</span>)[<span class="number">1</span>].alias(<span class="string">&#x27;sex&#x27;</span>),</span><br><span class="line">                       df_1.timestamp.alias(<span class="string">&#x27;ts1&#x27;</span>))\</span><br><span class="line">    .withWatermark(<span class="string">&#x27;ts1&#x27;</span>,<span class="string">&#x27;2 minutes&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df_socket = spark.readStream\</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;socket&#x27;</span>)\</span><br><span class="line">    .option(<span class="string">&#x27;host&#x27;</span>,<span class="string">&#x27;node01&#x27;</span>)\</span><br><span class="line">    .option(<span class="string">&#x27;port&#x27;</span>,<span class="string">&#x27;9999&#x27;</span>)\</span><br><span class="line">    .load()</span><br><span class="line"></span><br><span class="line">df_socket = df_socket.select(F.split(df_socket.value,<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>].alias(<span class="string">&#x27;name&#x27;</span>),</span><br><span class="line">                             F.split(df_socket.value,<span class="string">&#x27; &#x27;</span>)[<span class="number">1</span>].alias(<span class="string">&#x27;age&#x27;</span>),</span><br><span class="line">                             F.split(df_socket.value,<span class="string">&#x27; &#x27;</span>)[<span class="number">2</span>].alias(<span class="string">&#x27;ts2&#x27;</span>)) \</span><br><span class="line">    .withColumn(<span class="string">&#x27;ts2&#x27;</span>,F.to_timestamp(<span class="string">&#x27;ts2&#x27;</span>))\</span><br><span class="line">    .withWatermark(<span class="string">&#x27;ts2&#x27;</span>,<span class="string">&#x27;1 minutes&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df_join = df_kafka.join(df_socket,<span class="string">&#x27;age&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df_join.writeStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;console&#x27;</span>) \</span><br><span class="line">    .outputMode(<span class="string">&#x27;append&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;truncate&quot;</span>,<span class="literal">False</span>) \</span><br><span class="line">    .trigger(processingTime=<span class="string">&#x27;2 seconds&#x27;</span>) \</span><br><span class="line">    .start() \</span><br><span class="line">    .awaitTermination()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在 nc 端输入</span></span><br><span class="line">zhansan 1 2022-06-01</span><br><span class="line">lisi 2 2022-06-01</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在kafka生产者端输入</span></span><br><span class="line">1 m</span><br><span class="line">2 w</span><br></pre></td></tr></table></figure>

<p>​		输出如下：</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-02%2017.22.01.jpg" alt="截屏2022-06-02 17.22.01"></p>
<h4 id="流与流的outjoin"><a href="#流与流的outjoin" class="headerlink" title="流与流的outjoin"></a>流与流的outjoin</h4><p>​		可以参考 <code>https://docs.databricks.com/_static/notebooks/stream-stream-joins-python.html</code>	</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">df_kafka = df_1.select(F.split(df_1.value,<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>].alias(<span class="string">&#x27;kafka_age&#x27;</span>),</span><br><span class="line">                       F.split(df_1.value,<span class="string">&#x27; &#x27;</span>)[<span class="number">1</span>].alias(<span class="string">&#x27;sex&#x27;</span>),</span><br><span class="line">                       df_1.timestamp.alias(<span class="string">&#x27;ts1&#x27;</span>)) \</span><br><span class="line">    .withWatermark(<span class="string">&#x27;ts1&#x27;</span>,<span class="string">&#x27;4 minutes&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df_socket = spark.readStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;socket&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;host&#x27;</span>,<span class="string">&#x27;node01&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;port&#x27;</span>,<span class="string">&#x27;9999&#x27;</span>) \</span><br><span class="line">    .load()</span><br><span class="line"></span><br><span class="line">df_socket = df_socket.select(F.split(df_socket.value,<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>].alias(<span class="string">&#x27;name&#x27;</span>),</span><br><span class="line">                             F.split(df_socket.value,<span class="string">&#x27; &#x27;</span>)[<span class="number">1</span>].alias(<span class="string">&#x27;socket_age&#x27;</span>),</span><br><span class="line">                             F.split(df_socket.value,<span class="string">&#x27; &#x27;</span>)[<span class="number">2</span>].alias(<span class="string">&#x27;ts2&#x27;</span>)) \</span><br><span class="line">    .withColumn(<span class="string">&#x27;ts2&#x27;</span>,F.to_timestamp(<span class="string">&#x27;ts2&#x27;</span>)) \</span><br><span class="line">    .withWatermark(<span class="string">&#x27;ts2&#x27;</span>,<span class="string">&#x27;2 minutes&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df_join = df_kafka.join(df_socket,F.expr(<span class="string">&#x27;&#x27;&#x27;socket_age == kafka_age AND ts1 &gt; ts2 + interval 1 minutes&#x27;&#x27;&#x27;</span>),<span class="string">&quot;leftouter&quot;</span>) <span class="comment"># full/fullouter</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 但是我实践以后发现，这个outerjoin 和 inner join 效果是一样的，也没有对空位置补null。</span></span><br></pre></td></tr></table></figure>

<p>​		但是我实践以后发现，这个outerjoin 和 inner join 效果是一样的，也没有对空位置补null。</p>
<h5 id="Support-matrix-for-joins-in-streaming-queries"><a href="#Support-matrix-for-joins-in-streaming-queries" class="headerlink" title="Support matrix for joins in streaming queries"></a>Support matrix for joins in streaming queries</h5><table>
<thead>
<tr>
<th align="left">Left Input</th>
<th align="left">Right Input</th>
<th align="left">Join Type</th>
<th align="left"></th>
</tr>
</thead>
<tbody><tr>
<td align="left">Static</td>
<td align="left">Static</td>
<td align="left">All types</td>
<td align="left">Supported, since its not on streaming data even though it can be present in a streaming query</td>
</tr>
<tr>
<td align="left">Stream</td>
<td align="left">Static</td>
<td align="left">Inner</td>
<td align="left">Supported, not stateful</td>
</tr>
<tr>
<td align="left">Left Outer</td>
<td align="left">Supported, not stateful</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Right Outer</td>
<td align="left">Not supported</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Full Outer</td>
<td align="left">Not supported</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Left Semi</td>
<td align="left">Supported, not stateful</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Static</td>
<td align="left">Stream</td>
<td align="left">Inner</td>
<td align="left">Supported, not stateful</td>
</tr>
<tr>
<td align="left">Left Outer</td>
<td align="left">Not supported</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Right Outer</td>
<td align="left">Supported, not stateful</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Full Outer</td>
<td align="left">Not supported</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Left Semi</td>
<td align="left">Not supported</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Stream</td>
<td align="left">Stream</td>
<td align="left">Inner</td>
<td align="left">Supported, optionally specify watermark on both sides + time constraints for state cleanup</td>
</tr>
<tr>
<td align="left">Left Outer</td>
<td align="left">Conditionally supported, must specify watermark on right + time constraints for correct results, optionally specify watermark on left for all state cleanup</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Right Outer</td>
<td align="left">Conditionally supported, must specify watermark on left + time constraints for correct results, optionally specify watermark on right for all state cleanup</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Full Outer</td>
<td align="left">Conditionally supported, must specify watermark on one side + time constraints for correct results, optionally specify watermark on the other side for all state cleanup</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Left Semi</td>
<td align="left">Conditionally supported, must specify watermark on right + time constraints for correct results, optionally specify watermark on left for all state cleanup</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<h2 id="sink"><a href="#sink" class="headerlink" title="sink"></a>sink</h2><h4 id="hdfs-sink（or-本地）"><a href="#hdfs-sink（or-本地）" class="headerlink" title="hdfs sink（or 本地）"></a>hdfs sink（or 本地）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&#x27;structure_word_count&#x27;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)  <span class="comment"># 只显示ERROR日志。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df = spark.readStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;subscribe&#x27;</span>,<span class="string">&#x27;kafka-structure-stream&#x27;</span>) \</span><br><span class="line">    .load() \</span><br><span class="line">    .selectExpr(<span class="string">&#x27;cast(value as string)&#x27;</span>,<span class="string">&#x27;timestamp&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df = df.select(F.explode( F.split(df.value,<span class="string">&#x27; &#x27;</span>)).alias(<span class="string">&#x27;value&#x27;</span>), df.timestamp.alias(<span class="string">&#x27;ts&#x27;</span>))</span><br><span class="line">    <span class="comment"># .groupBy([&#x27;value&#x27;])\</span></span><br><span class="line">    <span class="comment"># .count()</span></span><br><span class="line">    <span class="comment"># .withWatermark(&#x27;ts&#x27;,&#x27;10 seconds&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df.writeStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;csv&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;path&#x27;</span>,<span class="string">&#x27;file:///opt/data/spark-structure-streaming/wordcount_result&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;checkpointLocation&#x27;</span>,<span class="string">&#x27;file:///opt/data/spark-structure-streaming/wordcount_log&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;truncate&quot;</span>,<span class="literal">False</span>) \</span><br><span class="line">    .start() \</span><br><span class="line">    .awaitTermination()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-07%2020.44.36.jpg" alt="截屏2022-06-07 20.44.36"></p>
<h4 id="kafka-sink"><a href="#kafka-sink" class="headerlink" title="kafka sink"></a>kafka sink</h4><p>​		从 <code>Kafka</code> topic1 中获取数据, 简单处理, 再次放入 <code>Kafka</code>的 topic2 。	</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server node01:9092 --create --topic structure-stream-kafka --partitions 3 --replication-factor 2</span><br><span class="line"></span><br><span class="line">kafka-console-consumer.sh --bootstrap-server node01:9092 --topic structure-stream-kafka</span><br><span class="line"></span><br><span class="line">spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 /opt/data/spark-structure-streaming/kafka_sink.py</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&#x27;structure_word_count&#x27;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)  <span class="comment"># 只显示ERROR日志。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df = spark.readStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;subscribe&#x27;</span>,<span class="string">&#x27;kafka-structure-stream&#x27;</span>) \</span><br><span class="line">    .load() \</span><br><span class="line">    .selectExpr(<span class="string">&#x27;cast(value as string)&#x27;</span>,<span class="string">&#x27;timestamp&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df = df.select(F.explode( F.split(df.value,<span class="string">&#x27; &#x27;</span>)).alias(<span class="string">&#x27;value&#x27;</span>), df.timestamp.alias(<span class="string">&#x27;ts&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df.writeStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;topic&#x27;</span>,<span class="string">&#x27;structure-stream-kafka&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;checkpointLocation&#x27;</span>,<span class="string">&#x27;file:///opt/data/spark-structure-streaming/wordcount_log&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;truncate&quot;</span>,<span class="literal">False</span>) \</span><br><span class="line">    .start() \</span><br><span class="line">    .awaitTermination()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<p>​		kafka的生产者内输入：	</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">spark spark flink flink kafka</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hadoop hadoop kafka kafka</span></span><br></pre></td></tr></table></figure>

<p>​		在kafka topic （structure-stream-kafka）中可见时间戳被隐藏了。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-07%2021.19.30.jpg" alt="截屏2022-06-07 21.19.30"></p>
<h4 id="Foreach-writer-（mysql）"><a href="#Foreach-writer-（mysql）" class="headerlink" title="Foreach writer （mysql）"></a>Foreach writer （mysql）</h4><p>​	<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-08%2011.46.20.jpg" alt="截屏2022-06-08 11.46.20">		</p>
<p>​		观察：foreach 的 统计自带了回顾前面内容的能力。</p>
<p>​					相当于，把所有前面 <strong>多次</strong> 输入的内容，重新 <strong>一次</strong> 逐条输入进去，foreach 就是每个都输入一遍，包括过去的输入内容。</p>
<p>​					foreach() 参数是一个类，这个类要重写open连接mysql 方法；重写close断开连接mysql方法；重写process处理df的方法。</p>
<p>​					process方法 传入的参数 row， 是 【foreach 每个都输入一遍，包括过去的输入的内容】运行完的结果df中的一行转化的row对象。例如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">截取部分代码</span></span><br><span class="line">df = df.select(F.explode( F.split(df.value,&#x27; &#x27;)).alias(&#x27;word&#x27;))\</span><br><span class="line">    .groupBy(&#x27;word&#x27;)\</span><br><span class="line">    .count()</span><br><span class="line">class Row_to_mysql():</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.db = None</span><br><span class="line">    def open(self, partition_id, epoch_id):</span><br><span class="line">    		# 连接mysql</span><br><span class="line">    		pass</span><br><span class="line">    def process(self,row):</span><br><span class="line">        print(row)</span><br><span class="line">    def close(self,error):</span><br><span class="line">    		# 断开mysql连接</span><br><span class="line">    		pass</span><br><span class="line">df.writeStream \</span><br><span class="line">    .outputMode(&#x27;update&#x27;) \</span><br><span class="line">    .foreach(Row_to_mysql()) </span><br><span class="line">     </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">我们来看看row到底是什么。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">row 是把多次的输入一次性每一个都输入一遍，运行完的<span class="built_in">df</span>结果，取每一行，变成了row对象。因为这里是update 模式，row只是<span class="built_in">df</span>结果变化的一行。</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在kafka的生产者输入：</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">spark spark</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">output</span></span><br><span class="line">Row(word=&#x27;spark&#x27;, count=2)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在kafka的生产者输入：</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hive hive hive</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">output</span></span><br><span class="line">Row(word=&#x27;hive&#x27;, count=3)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在kafka的生产者输入：</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hive hive</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">output</span></span><br><span class="line">Row(word=&#x27;hive&#x27;, count=5)</span><br></pre></td></tr></table></figure>

<p>​		在mysql 中新建一个table。	</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create database structure_stream_mysql;</span><br><span class="line">use structure_stream_mysql;</span><br><span class="line">create table structure_stream_mysql_table(word char(20),counted int);</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession,Row</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&#x27;structure_word_count&#x27;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)  <span class="comment"># 只显示ERROR日志。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df = spark.readStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;subscribe&#x27;</span>,<span class="string">&#x27;kafka-structure-stream&#x27;</span>) \</span><br><span class="line">    .load() \</span><br><span class="line">    .selectExpr(<span class="string">&#x27;cast(value as string)&#x27;</span>,<span class="string">&#x27;timestamp&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df = df.select(F.explode( F.split(df.value,<span class="string">&#x27; &#x27;</span>)).alias(<span class="string">&#x27;word&#x27;</span>))\</span><br><span class="line">    .groupBy(<span class="string">&#x27;word&#x27;</span>)\</span><br><span class="line">    .count()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Row_to_mysql</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.db = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open</span>(<span class="params">self, partition_id, epoch_id</span>):</span><br><span class="line">        <span class="comment"># print(partition_id,epoch_id)</span></span><br><span class="line">        self.db = pymysql.connect(host=<span class="string">&#x27;node01&#x27;</span>,</span><br><span class="line">                             user=<span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">                             password=<span class="string">&#x27;密码&#x27;</span>,</span><br><span class="line">                             database=<span class="string">&#x27;structure_stream_mysql&#x27;</span>)</span><br><span class="line">        <span class="comment"># if self.db != None:</span></span><br><span class="line">        <span class="comment">#     print(&#x27;db is not none&#x27;)</span></span><br><span class="line">        <span class="comment"># else:</span></span><br><span class="line">        <span class="comment">#     print(&#x27;db is none &#x27;)</span></span><br><span class="line">        <span class="keyword">return</span> self.db!=<span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process</span>(<span class="params">self,row</span>):</span><br><span class="line">        <span class="built_in">print</span>(row)</span><br><span class="line">        cursor = self.db.cursor()</span><br><span class="line">        sql = <span class="string">&quot;insert into structure_stream_mysql_table(word,counted) values (%s,%s) on duplicate key update counted=%s;&quot;</span>        <span class="comment"># 虽然 mysql表中counted 是int 类型，这个还是要用%s，用%d会不匹配。</span></span><br><span class="line">        cursor.execute(sql,[row[<span class="string">&#x27;word&#x27;</span>],row[<span class="string">&#x27;count&#x27;</span>],row[<span class="string">&#x27;count&#x27;</span>]])</span><br><span class="line">        self.db.commit()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close</span>(<span class="params">self,error</span>):</span><br><span class="line">        <span class="comment"># print(&#x27;close&#x27;)</span></span><br><span class="line">        self.db.close()</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df.writeStream \</span><br><span class="line">    .outputMode(<span class="string">&#x27;update&#x27;</span>) \</span><br><span class="line">    .foreach(Row_to_mysql()) \</span><br><span class="line">    .start() \</span><br><span class="line">    .awaitTermination()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在kafka的生产者输入：</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">spark spark spark kafka kafka hive hive</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">spark kafka hive</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hadoop</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">spark</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"></span></span><br><span class="line">select * from structure_stream_mysql_table;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">output</span></span><br><span class="line">+--------+---------+</span><br><span class="line">| word   | counted |</span><br><span class="line">+--------+---------+</span><br><span class="line">| kafka  |       2 |</span><br><span class="line">| spark  |       3 |</span><br><span class="line">| hive   |       2 |</span><br><span class="line">| kafka  |       3 |</span><br><span class="line">| spark  |       5 |</span><br><span class="line">| hadoop |       1 |</span><br><span class="line">| hive   |       3 |</span><br><span class="line">+--------+---------+</span><br><span class="line">7 rows in set (0.02 sec)</span><br></pre></td></tr></table></figure>



<h4 id="Foreachbatch"><a href="#Foreachbatch" class="headerlink" title="Foreachbatch"></a>Foreachbatch</h4><p>​		1、<strong>Reuse existing batch data sources</strong> 		</p>
<p>​		2、<strong>Write to multiple locations</strong></p>
<p>​		3、<strong>Apply additional DataFrame operations</strong></p>
<pre><code> **（foreachbatch 把流式的df变成了静态的df，大大拓展了算子的应用。）**
</code></pre>
<p>​		scala 标准写法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">streamingDF.writeStream.foreachBatch &#123; (batchDF: <span class="type">DataFrame</span>, batchId: <span class="type">Long</span>) =&gt;</span><br><span class="line">  batchDF.persist()</span><br><span class="line">  batchDF.write.format(...).save(...)  <span class="comment">// location 1</span></span><br><span class="line">  batchDF.write.format(...).save(...)  <span class="comment">// location 2</span></span><br><span class="line">  batchDF.unpersist()&#125;</span><br></pre></td></tr></table></figure>

<p>​		python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">foreach_batch_function</span>(<span class="params">df, epoch_id</span>): <span class="comment"># 这个df是静态的df。</span></span><br><span class="line">    <span class="comment"># Transform and write batchDF</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">  </span><br><span class="line">streamingDF.writeStream.foreachBatch(foreach_batch_function).start()   </span><br></pre></td></tr></table></figure>

<p>​		<strong>foreachbatch 案例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&#x27;structure_word_count&#x27;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)  <span class="comment"># 只显示ERROR日志。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df = spark.readStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;kafka.bootstrap.servers&#x27;</span>,<span class="string">&#x27;node01:9092&#x27;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;subscribe&#x27;</span>,<span class="string">&#x27;kafka-structure-stream&#x27;</span>) \</span><br><span class="line">    .load() \</span><br><span class="line">    .selectExpr(<span class="string">&#x27;cast(value as string)&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">foreachBatch_func</span>(<span class="params">df,batchId</span>):</span><br><span class="line">    df2 = df.select(F.explode( F.split(df.value,<span class="string">&#x27; &#x27;</span>)).alias(<span class="string">&#x27;value&#x27;</span>)) \</span><br><span class="line">        .groupBy(<span class="string">&#x27;value&#x27;</span>) \</span><br><span class="line">        .count()</span><br><span class="line">    df2.persist()</span><br><span class="line">    df2.write.mode(<span class="string">&#x27;overwrite&#x27;</span>)\</span><br><span class="line">        .<span class="built_in">format</span>(<span class="string">&#x27;json&#x27;</span>)\</span><br><span class="line">        .save(<span class="string">&#x27;file:///opt/data/spark-structure-streaming/repository_test/json&#x27;</span>)</span><br><span class="line">    df2.write.mode(<span class="string">&#x27;overwrite&#x27;</span>)\</span><br><span class="line">        .<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>) \</span><br><span class="line">        .option(<span class="string">&#x27;url&#x27;</span>,<span class="string">&#x27;jdbc:mysql://node01:3306/structure_stream_mysql?useSSL=false&amp;useUnicode=true&#x27;</span>) \</span><br><span class="line">        .option(<span class="string">&#x27;dbtable&#x27;</span>,<span class="string">&#x27;foreachbatch_table&#x27;</span>) \</span><br><span class="line">        .option(<span class="string">&#x27;user&#x27;</span>,<span class="string">&#x27;root&#x27;</span>) \</span><br><span class="line">        .option(<span class="string">&#x27;password&#x27;</span>,<span class="string">&#x27;密码&#x27;</span>) \</span><br><span class="line">        .save()</span><br><span class="line">    df2.unpersist()</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">df.writeStream \</span><br><span class="line">    .outputMode(<span class="string">&#x27;update&#x27;</span>) \</span><br><span class="line">    .foreachBatch(foreachBatch_func)\</span><br><span class="line">    .start() \</span><br><span class="line">    .awaitTermination()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<p>​		当然，处理df的过程可以放进foreachBatch_func方法内，也可以放在外面，放外面的话，用complete输出模式会适用一些。</p>
<h2 id="Trigger-触发器"><a href="#Trigger-触发器" class="headerlink" title="Trigger(触发器)"></a>Trigger(触发器)</h2><table>
<thead>
<tr>
<th>Trigger Type</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td><em>unspecified (default)</em></td>
<td>没有显示的设定触发器, 表示使用 micro-batch mode, 尽可能块的处理每个批次的数据. 如果无数据可用, 则处于阻塞状态, 等待数据流入</td>
</tr>
<tr>
<td><strong>Fixed interval micro-batches</strong> 固定周期的微批处理</td>
<td>查询会在微批处理模式下执行, 其中微批处理将以用户指定的间隔执行. 1. 如果以前的微批处理在间隔内完成, 则引擎会等待间隔结束, 然后开启下一个微批次 2. 如果前一个微批处理在一个间隔内没有完成(即错过了间隔边界), 则下个微批处理会在上一个完成之后立即启动(不会等待下一个间隔边界) 3. 如果没有新数据可用, 则不会启动微批次. 适用于流式数据的批处理作业</td>
</tr>
<tr>
<td><strong>One-time micro-batch</strong> 一次性微批次</td>
<td>查询将在所有可用数据上执行一次微批次处理, 然后自行停止. 如果你希望定期启动集群, 然后处理集群关闭期间产生的数据, 然后再关闭集群. 这种情况下很有用. 它可以显著的降低成本. 一般用于非实时的数据分析</td>
</tr>
<tr>
<td><strong>Continuous with fixed checkpoint interval</strong> <em>(experimental 2.3</em> 引入*)* 连续处理</td>
<td>以超低延迟处理数据,（但是可能很多前面的流式处理的算子不支持）</td>
</tr>
</tbody></table>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.<span class="type">Trigger</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Default trigger (runs micro-batch as soon as it can)</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ProcessingTime trigger with two-seconds micro-batch interval</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">ProcessingTime</span>(<span class="string">&quot;2 seconds&quot;</span>))</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// One-time trigger</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">Once</span>())</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Available-now trigger</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">AvailableNow</span>())</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Continuous trigger with one-second checkpointing interval</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">Continuous</span>(<span class="string">&quot;1 second&quot;</span>))</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure>





<h2 id="实践案例（写入redis）"><a href="#实践案例（写入redis）" class="headerlink" title="实践案例（写入redis）"></a>实践案例（写入redis）</h2><p>​		代码生成数据-&gt; kafka -&gt; streaming消费 -&gt; redis\mysql...</p>
<h4 id="模拟数据"><a href="#模拟数据" class="headerlink" title="模拟数据"></a>模拟数据</h4><p>​		因为python总是要在控制台里用submit这样的方法运行写的脚本，有点麻烦，Scala写的代码在maven 里配置好spark-sql-kafka就可以在idea中直接运行程序，方便不少。</p>
<p>​		所以这个实践案例我们干脆用Scala来写。</p>
<p>​		先随机生成一些数据写入到kafka。</p>
<p>​		新建一个maven的项目。在maven项目中写Scala代码。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-27%2017.38.48.jpg" alt="截屏2022-06-27 17.38.48"></p>
<p>​		把maven相关的依赖pom.xml都配置好，刷新一下。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.13<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.13<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql-kafka-0-10_2.13<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--            &lt;scope&gt;test&lt;/scope&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka_2.13<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​		mock_data_from_scope.scala</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka_to_spark_test.mock</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Random</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RandomNumUtil</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> random = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">randomInt</span></span>(from:<span class="type">Int</span>,to:<span class="type">Int</span>):<span class="type">Int</span>=&#123;</span><br><span class="line">    <span class="keyword">if</span>(from &gt; to) <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s&quot;from is <span class="subst">$from</span> should &lt; to = <span class="subst">$to</span>&quot;</span>)</span><br><span class="line">    random.nextInt(to-from+<span class="number">1</span>) + from</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">randomMultiInt</span></span>(from:<span class="type">Int</span>,to:<span class="type">Int</span>,count:<span class="type">Int</span>,canReat:<span class="type">Boolean</span> = <span class="literal">true</span>):<span class="type">List</span>[<span class="type">Int</span>]=&#123;</span><br><span class="line">    <span class="keyword">if</span> (canReat)&#123;</span><br><span class="line">      (<span class="number">1</span> to count).map( _ =&gt; randomInt(from, to)).toList  <span class="comment">// notice toList func != List func</span></span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">      <span class="keyword">if</span> ((to - from)+<span class="number">1</span> &lt; count ) <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">&quot;count must &lt; num of regulations when canReat state &quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> set: mutable.<span class="type">Set</span>[<span class="type">Int</span>] = mutable.<span class="type">Set</span>[<span class="type">Int</span>]()</span><br><span class="line">      <span class="keyword">while</span>(set.size &lt; count)&#123;</span><br><span class="line">        set += randomInt(from, to)</span><br><span class="line">      &#125;</span><br><span class="line">      set.toList</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    println(randomMultiInt(<span class="number">10</span>,<span class="number">14</span>,<span class="number">5</span>,<span class="literal">true</span>  ))</span><br><span class="line">    println(randomMultiInt(<span class="number">10</span>,<span class="number">14</span>,<span class="number">5</span>,<span class="literal">false</span>  ))</span><br><span class="line"><span class="comment">//    println(randomMultiInt(10,14,6,false  ))</span></span><br><span class="line"><span class="comment">//    println(randomMultiInt(20,14,5,true  ))</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​		mock_data_proportion.scala</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka_to_spark_test.mock</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">mock_data_proportion</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>[<span class="type">T</span>](opts: (<span class="type">T</span>,<span class="type">Int</span>)*):mock_data_proportion[<span class="type">T</span>]=&#123;</span><br><span class="line">    <span class="keyword">val</span> randomoptions = <span class="keyword">new</span> mock_data_proportion[<span class="type">T</span>]()</span><br><span class="line">    randomoptions.totalWeight = (<span class="number">0</span> /: opts)(_+_._2)</span><br><span class="line">    opts.foreach &#123;                         <span class="comment">// 偏函数 没有 match 只有case</span></span><br><span class="line">      <span class="keyword">case</span> (value,weight)=&gt;&#123;               <span class="comment">// 制作一张齐全的大表option，从中随机取值。</span></span><br><span class="line">        randomoptions.options ++= (<span class="number">1</span> to weight).map(_ =&gt; value )&#125;&#125;</span><br><span class="line">    randomoptions</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> opts: mock_data_proportion[<span class="type">String</span>] = mock_data_proportion((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;lisi&quot;</span>, <span class="number">10</span>))</span><br><span class="line">    println(opts.getRandomOption())</span><br><span class="line">    println(opts.getRandomOption())</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">mock_data_proportion</span>[<span class="type">T</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> totalWeight:<span class="type">Int</span> = _ <span class="comment">// var tatalWeight:Int = null</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> options: <span class="type">ListBuffer</span>[<span class="type">T</span>] = <span class="type">ListBuffer</span>[<span class="type">T</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getRandomOption</span></span>(): <span class="type">T</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> random = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line">    options(random.nextInt(options.length))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​		cityInfo</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka_to_spark_test.mock</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">cityInfo</span>(<span class="params">city_id: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                    city_name: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                    area: <span class="type">String</span></span>)</span></span><br></pre></td></tr></table></figure>

<p>​		mockRealTime</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka_to_spark_test.mock</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.&#123;<span class="type">KafkaProducer</span>, <span class="type">ProducerRecord</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">mockRealTime</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">mockRealTimeData</span></span>(): <span class="type">ArrayBuffer</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="keyword">var</span> array = <span class="type">ArrayBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">    <span class="keyword">var</span> randomOpts: mock_data_proportion[cityInfo] = mock_data_proportion(</span><br><span class="line">      (cityInfo(<span class="number">1</span>, <span class="string">&quot;北京&quot;</span>, <span class="string">&quot;华北&quot;</span>), <span class="number">30</span>),</span><br><span class="line">      (cityInfo(<span class="number">2</span>, <span class="string">&quot;上海&quot;</span>, <span class="string">&quot;华东&quot;</span>), <span class="number">30</span>),</span><br><span class="line">      (cityInfo(<span class="number">3</span>, <span class="string">&quot;广州&quot;</span>, <span class="string">&quot;华南&quot;</span>), <span class="number">10</span>),</span><br><span class="line">      (cityInfo(<span class="number">4</span>, <span class="string">&quot;深圳&quot;</span>, <span class="string">&quot;华南&quot;</span>), <span class="number">20</span>),</span><br><span class="line">      (cityInfo(<span class="number">5</span>, <span class="string">&quot;杭州&quot;</span>, <span class="string">&quot;华中&quot;</span>), <span class="number">10</span>),</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// 一次生成5个</span></span><br><span class="line">    (<span class="number">1</span> to <span class="number">5</span>).foreach &#123;</span><br><span class="line">      i =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> timestamp = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">        <span class="keyword">val</span> cityInfo = randomOpts.getRandomOption()</span><br><span class="line">        <span class="keyword">val</span> area = cityInfo.area</span><br><span class="line">        <span class="keyword">val</span> adid = cityInfo.city_id</span><br><span class="line">        <span class="keyword">val</span> city = cityInfo.city_name</span><br><span class="line">        <span class="keyword">val</span> userid = <span class="type">RandomNumUtil</span>.randomInt(<span class="number">100</span>, <span class="number">106</span>)</span><br><span class="line">        array += <span class="string">s&quot;<span class="subst">$timestamp</span>,<span class="subst">$userid</span>,<span class="subst">$area</span>,<span class="subst">$city</span>,<span class="subst">$adid</span>&quot;</span></span><br><span class="line">        <span class="type">Thread</span>.sleep(<span class="number">10</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    array</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createKafkaProducer</span></span>: <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span></span><br><span class="line">    props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node01:9092&quot;</span>)</span><br><span class="line">    props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>)</span><br><span class="line">    props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>](props)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">&quot;ads_log&quot;</span></span><br><span class="line">    <span class="keyword">val</span> producer: <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>] = createKafkaProducer</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">      mockRealTimeData().foreach &#123;</span><br><span class="line">        msg =&gt; &#123;</span><br><span class="line">          producer.send(<span class="keyword">new</span> <span class="type">ProducerRecord</span>(topic, msg))</span><br><span class="line">          <span class="type">Thread</span>.sleep(<span class="number">1000</span>)   <span class="comment">// 每隔一秒钟 发送一个到kafka中。</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">5000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​		AdsInfo（从 kafka 读取数据, 为了方便后续处理, 封装数据到 AdsInfo 样例类中）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka_to_spark_test.bean</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Timestamp</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">AdsInfo</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">                  timestamp: <span class="type">Timestamp</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                  dayString: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                  hmString: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                  area: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                  city: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                  userId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                  adsId: <span class="type">String</span></span></span></span><br><span class="line"><span class="params"><span class="class">                  </span>)</span></span><br></pre></td></tr></table></figure>

<p>​		RealtimeApp</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka_to_spark_test.app</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Timestamp</span></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka_to_spark_test.bean.<span class="type">AdsInfo</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealtimeApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;realtimeApp&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    spark.sparkContext.setLogLevel(<span class="string">&quot;WARN&quot;</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> dayStringFormatter = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> hmStringFormatter = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;HH:mm&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    从 kafka 读取数据, 为了方便后续处理, 封装数据到 AdsInfo 样例类中</span></span><br><span class="line">    <span class="keyword">val</span> adsInfoDS: <span class="type">Dataset</span>[<span class="type">AdsInfo</span>] = spark.readStream</span><br><span class="line">      .format(<span class="string">&quot;kafka&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;node01:9092&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;subscribe&quot;</span>, <span class="string">&quot;ads_log&quot;</span>)</span><br><span class="line">      .load</span><br><span class="line">      .select(<span class="string">&quot;value&quot;</span>)</span><br><span class="line">      .as[<span class="type">String</span>]</span><br><span class="line">      .map(v =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> split: <span class="type">Array</span>[<span class="type">String</span>] = v.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> date = <span class="keyword">new</span> <span class="type">Date</span>(split(<span class="number">0</span>).toLong)</span><br><span class="line">        <span class="type">AdsInfo</span>(</span><br><span class="line">          <span class="keyword">new</span> <span class="type">Timestamp</span>(split(<span class="number">0</span>).toLong),</span><br><span class="line">          dayStringFormatter.format(date),</span><br><span class="line">          hmStringFormatter.format(date),</span><br><span class="line">          split(<span class="number">2</span>),</span><br><span class="line">          split(<span class="number">3</span>),</span><br><span class="line">          split(<span class="number">1</span>),</span><br><span class="line">          split(<span class="number">4</span>)</span><br><span class="line">        )</span><br><span class="line">      &#125;)</span><br><span class="line">    adsInfoDS.writeStream</span><br><span class="line">      .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">      .outputMode(<span class="string">&quot;update&quot;</span> )</span><br><span class="line">      .option(<span class="string">&quot;truncate&quot;</span>,<span class="string">&quot;false&quot;</span>)</span><br><span class="line">      .start</span><br><span class="line">      .awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​		先在kafka 内打开一个 topic <code>ads_log</code>。</p>
<p>​		我们先运行RealtimeApp 用spark structure接收kafka 的数据。 再运行  mockRealTime 生产数据进入kafka，可见如下：</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-06-27%2017.55.34.jpg" alt="截屏2022-06-27 17.55.34"></p>
<h4 id="黑名单实时统计"><a href="#黑名单实时统计" class="headerlink" title="黑名单实时统计"></a>黑名单实时统计</h4><p>​		<strong>（需求）</strong></p>
<p>​		实现实时的动态黑名单检测机制：将每天对某个广告点击超过阈值(比如:100次)的用户拉入黑名单。 1. 黑名单应该是每天更新一次. 如果昨天进入黑名单, 今天应该重新再统计 2. 把黑名单写入到 redis 中, 以供其他应用查看 3. 已经进入黑名单的用户不再进行检测(提高效率)</p>
<p>​		<strong>（思路）</strong></p>
<p>​		黑名单存放在 redis 中, 使用 set, set 中的每个元素表示一个用户。通过 sql 查询过滤出来每天每广告点击数超过阈值的用户, 然后使用 foreach 写入到 redis 即可。</p>
<p>​		<strong>（技法）</strong></p>
<p>​		在spark structure streaming 的编写上，要注意：</p>
<p>​		1、编写步骤上、应先用console的形式输出到控制台，再一点一点去完善代码。</p>
<p>​		2、spark处理数据的时间间隔最好要 小于 kafka等数据流生成的时间间隔。</p>
<p>​		3、awaitTermiation() 放的位置是 流式处理的最后一个writestreaming后尾作为收尾标志的，若之前有多次流式处理，写到start() 即可，运行起来以找到 awaitTermiation() 作为一整个流式处理的结束。</p>
<p>​		4、如果运行一直没结果，又不明不白的停滞了，jps 检查一下看看hadoop 有没有 &#x2F;tmp&#x2F;hsperf 错误，zookeeper、kafka、hadoop都重来一遍，checkpointLocation删除重新生成一个新的。再来一遍。</p>
<hr>
<p>​		*** 先安装一下 redis***</p>
<p>​		<code>wget https://github.com/redis/redis/archive/7.0.2.tar.gz</code></p>
<p>​		<code>tar -zvxf 7.0.2.tar.gz</code></p>
<p>​		<code>cd redis-7.0.2/</code></p>
<p>​		<code>make</code>   &#x2F;&#x2F; 编译</p>
<p>​		<code>make PREFIX=/opt/redis-7.0.2 install</code>  &#x2F;&#x2F; 安装</p>
<p>​		<code>vim redis.conf</code>     <code>aemonize yes</code> &#x2F;&#x2F; 打开远程登录功能</p>
<p>​		<code>./bin/redis-server&amp; ./redis.conf</code>  &#x2F;&#x2F; 启动redis服务</p>
<p>​		<code>netstat -an | grep 6379</code>  &#x2F;&#x2F; 检查6379端口是否被监听</p>
<p>​		<code>redis-cli</code> 	&#x2F;&#x2F; 启用redis的客户端</p>
<p>​		<code>flushall</code>    &#x2F;&#x2F; 清空redis的内容</p>
<p>​		<code>keys *</code>       &#x2F;&#x2F;查看</p>
<p>​		<code>smembers</code></p>
<p>​		<code>sadd key value*</code>   &#x2F;&#x2F; 往set里面写key和value<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-01%2023.16.47.jpg" alt="截屏2022-07-01 23.16.47"></p>
<p>​		客户端内 可关闭redis服务   <code>shutdown</code>，退出客户端 <code>exit</code> </p>
<p>​		在maven 内配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.2.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 这样就可以在Scala、python脚本中import redis相关工具，访问类似于mysql的键值对数据库了&gt;</span></span><br></pre></td></tr></table></figure>

<p>​		</p>
<p>​		<em><strong>注意几个redis相关bug 的处理：</strong></em></p>
<p>​		1、<code>redis-sentinel &quot;DENIED Redis is running in protected mode</code> </p>
<p>​				先把redis配置文件conf 中的 protected-mode yes 改成 protected-mod no, 再手动开启redis服务时不开启保护模式 <code>./bin/redis-server --protected-mode no &amp; ./redis.conf</code> ；</p>
<p>​		2、 <code>There is insufficient memory for the Java Runtime Environment to continue</code></p>
<p>​				Java虚拟机内存空间不够了，我们 <code>cd /etc/security/limits.d</code> 将文件 xxxx.config 的 * soft nproc xxxx 注释掉；</p>
<p>​		3、<code>[java.lang.NoSuchMethodError: redis.clients.jedis.JedisPoolConfig.setMinEvictableIdleTime](https://www.cnblogs.com/live41/p/15790868.html)</code></p>
<p>​				这是访问redis 时报的错，原因可能有两个：一：Maven中有其它组件依赖了旧版的jedis，需要排除；二：JedisPoolConfig和JedisSentinelPool使用了apache.commons的commons-pool2包（怀疑是用反射使用的原因）；在maven 中改如下两个配置。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- &lt;dependency&gt;中排除旧版jedis的依赖 --&gt;</span></span><br><span class="line">      	<span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">      </span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.2.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  	<span class="comment">&lt;!-- JedisPoolConfig和JedisSentinelPool使用了apache.commons的commons-pool2包 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-pool2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.11.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<hr>
<p>​		</p>
<p>Redis_Util.scala</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka_to_spark_test.util</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.&#123;<span class="type">Jedis</span>, <span class="type">JedisPool</span>, <span class="type">JedisPoolConfig</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RedisUtil</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> jedisPoolConfig: <span class="type">JedisPoolConfig</span> = <span class="keyword">new</span> <span class="type">JedisPoolConfig</span>()</span><br><span class="line">  jedisPoolConfig.setMaxTotal(<span class="number">100</span>) <span class="comment">//最大连接数</span></span><br><span class="line">  jedisPoolConfig.setMaxIdle(<span class="number">20</span>) <span class="comment">//最大空闲</span></span><br><span class="line">  jedisPoolConfig.setMinIdle(<span class="number">20</span>) <span class="comment">//最小空闲</span></span><br><span class="line">  jedisPoolConfig.setBlockWhenExhausted(<span class="literal">true</span>) <span class="comment">//忙碌时是否等待</span></span><br><span class="line">  jedisPoolConfig.setMaxWaitMillis(<span class="number">500</span>) <span class="comment">//忙碌时等待时长 毫秒</span></span><br><span class="line">  jedisPoolConfig.setTestOnBorrow(<span class="literal">true</span>) <span class="comment">//每次获得连接的进行测试</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> jedisPool: <span class="type">JedisPool</span> = <span class="keyword">new</span> <span class="type">JedisPool</span>(jedisPoolConfig, <span class="string">&quot;node01&quot;</span>, <span class="number">6379</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 直接得到一个 Redis 的连接</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getJedisClient</span></span>: <span class="type">Jedis</span> = &#123;</span><br><span class="line">    jedisPool.getResource</span><br><span class="line">  &#125;</span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>

<p>BlackListApp.scala</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka_to_spark_test.app</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka_to_spark_test.bean.<span class="type">AdsInfo</span></span><br><span class="line"><span class="keyword">import</span> kafka_to_spark_test.util.<span class="type">RedisUtil</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.<span class="type">Trigger</span></span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.<span class="type">Jedis</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">IterableOnce</span>.iterableOnceExtensionMethods</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BlackListApp</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 过滤</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">startBlackList</span></span>(spark:<span class="type">SparkSession</span>,adsInfoDS:<span class="type">Dataset</span>[<span class="type">AdsInfo</span>]):<span class="type">Dataset</span>[<span class="type">AdsInfo</span>]=&#123;  </span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// 过滤黑名单的数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">filter_func</span></span>(adsInfoDS1:<span class="type">Dataset</span>[<span class="type">AdsInfo</span>]):<span class="type">Dataset</span>[<span class="type">AdsInfo</span>]=&#123;</span><br><span class="line">      adsInfoDS1.mapPartitions(adsInfoItera =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> adsInfoList: <span class="type">List</span>[<span class="type">AdsInfo</span>] = adsInfoItera.toList</span><br><span class="line">        <span class="keyword">if</span> (adsInfoList.isEmpty) &#123;</span><br><span class="line">          adsInfoList.toIterator</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">val</span> client: <span class="type">Jedis</span> = <span class="type">RedisUtil</span>.getJedisClient</span><br><span class="line">          <span class="comment">// read blacklist</span></span><br><span class="line">          <span class="keyword">val</span> blackList: util.<span class="type">Set</span>[<span class="type">String</span>] = client.smembers(<span class="string">s&quot;day:blacklist:<span class="subst">$&#123;adsInfoList(0).dayString&#125;</span>&quot;</span>)</span><br><span class="line">          <span class="comment">// filter</span></span><br><span class="line">          adsInfoList.filter(adsInfo =&gt; &#123;</span><br><span class="line">            !blackList.contains(adsInfo.userId)</span><br><span class="line">          &#125;).toIterator</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      adsInfoDS1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> filtered_adsInfoDS: <span class="type">Dataset</span>[<span class="type">AdsInfo</span>] = filter_func(adsInfoDS)</span><br><span class="line"></span><br><span class="line">    filtered_adsInfoDS.createOrReplaceTempView(<span class="string">&quot;tb_adsinfo&quot;</span>)</span><br><span class="line"><span class="comment">//    adsInfoDS.createOrReplaceTempView(&quot;tb_adsinfo&quot;)</span></span><br><span class="line">    <span class="comment">// 每天每用户每id分组, 然后计数, 计数超过阈值(5)的查询出</span></span><br><span class="line">    <span class="keyword">val</span> result1: <span class="type">DataFrame</span> = spark.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |select</span></span><br><span class="line"><span class="string">        | userId,</span></span><br><span class="line"><span class="string">        | dayString</span></span><br><span class="line"><span class="string">        |from tb_adsinfo</span></span><br><span class="line"><span class="string">        |group by userId,dayString,adsId</span></span><br><span class="line"><span class="string">        |having count(1)&gt;5</span></span><br><span class="line"><span class="string">      &quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line"><span class="comment">//    这里是先用控制台验证数据，一点一点完善代码用的。</span></span><br><span class="line"><span class="comment">//    result1.writeStream</span></span><br><span class="line"><span class="comment">//      .format(&quot;console&quot;)</span></span><br><span class="line"><span class="comment">//      .outputMode(&quot;complete&quot;)</span></span><br><span class="line"><span class="comment">//      .trigger(Trigger.ProcessingTime(2000))</span></span><br><span class="line"><span class="comment">//      .option(&quot;checkpointLocation&quot;,&quot;/opt/data/maven_scala_test/src/scalaCode/kafka_to_spark_test/blacklist_log&quot;)</span></span><br><span class="line"><span class="comment">//      .start()</span></span><br><span class="line"><span class="comment">//      .awaitTermination()</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">    result1.writeStream</span><br><span class="line">      .outputMode(<span class="string">&quot;update&quot;</span> )</span><br><span class="line">      .trigger(<span class="type">Trigger</span>.<span class="type">ProcessingTime</span>(<span class="string">&quot;2 seconds&quot;</span>))</span><br><span class="line">      .foreach(<span class="keyword">new</span> <span class="type">ForeachWriter</span>[<span class="type">Row</span>] &#123;</span><br><span class="line">        <span class="keyword">var</span> client : <span class="type">Jedis</span> = _</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(partitionId: <span class="type">Long</span>, epochId: <span class="type">Long</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">          client = <span class="type">RedisUtil</span>.getJedisClient</span><br><span class="line">          client != <span class="literal">null</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(value: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> dayString: <span class="type">String</span> = value.getString(<span class="number">1</span>)</span><br><span class="line">          <span class="keyword">val</span> userId: <span class="type">String</span> = value.getString(<span class="number">0</span>)</span><br><span class="line">          client.sadd(<span class="string">s&quot;day:blacklist:<span class="subst">$dayString</span>&quot;</span>,userId)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(errorOrNull: <span class="type">Throwable</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">if</span>(client!=<span class="literal">null</span>) client.close()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      .option(<span class="string">&quot;checkpointLocation&quot;</span>,<span class="string">&quot;/opt/data/maven_scala_test/src/scalaCode/kafka_to_spark_test/blacklist_log&quot;</span>)</span><br><span class="line">      .start()</span><br><span class="line">      .awaitTermination()  <span class="comment">// 实际 加在需求二的流式写入后 !!!  这里为了测试效果，写在此处。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 再次 滤黑名单的数据</span></span><br><span class="line">    filtered_adsInfoDS = filter_func(filtered_adsInfoDS)</span><br><span class="line">    filtered_adsInfoDS</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>RealtimeApp1.scala</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka_to_spark_test.app</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka_to_spark_test.app.<span class="type">BlackListApp</span></span><br><span class="line"><span class="keyword">import</span> kafka_to_spark_test.bean.<span class="type">AdsInfo</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Timestamp</span></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealtimeApp1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;realtimeApp&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    spark.sparkContext.setLogLevel(<span class="string">&quot;WARN&quot;</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> dayStringFormatter = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> hmStringFormatter = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;HH:mm&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    从 kafka 读取数据, 为了方便后续处理, 封装数据到 AdsInfo 样例类中</span></span><br><span class="line">    <span class="keyword">val</span> adsInfoDS: <span class="type">Dataset</span>[<span class="type">AdsInfo</span>] = spark.readStream</span><br><span class="line">      .format(<span class="string">&quot;kafka&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;node01:9092&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;subscribe&quot;</span>, <span class="string">&quot;ads_log&quot;</span>)</span><br><span class="line">      .load</span><br><span class="line">      .select(<span class="string">&quot;value&quot;</span>)</span><br><span class="line">      .as[<span class="type">String</span>]</span><br><span class="line">      .map(v =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> split: <span class="type">Array</span>[<span class="type">String</span>] = v.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> date = <span class="keyword">new</span> <span class="type">Date</span>(split(<span class="number">0</span>).toLong)</span><br><span class="line">        <span class="type">AdsInfo</span>(</span><br><span class="line">          <span class="keyword">new</span> <span class="type">Timestamp</span>(split(<span class="number">0</span>).toLong),</span><br><span class="line">          dayStringFormatter.format(date),</span><br><span class="line">          hmStringFormatter.format(date),</span><br><span class="line">          split(<span class="number">2</span>),</span><br><span class="line">          split(<span class="number">3</span>),</span><br><span class="line">          split(<span class="number">1</span>),</span><br><span class="line">          split(<span class="number">4</span>)</span><br><span class="line">        )</span><br><span class="line">      &#125;)</span><br><span class="line">      .withWatermark(<span class="string">&quot;timestamp&quot;</span>,<span class="string">&quot;24 hours&quot;</span> )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 需求一：写 黑名单入redis，并且返回过滤后的值</span></span><br><span class="line">    <span class="keyword">val</span> filtered_adsInfoDS: <span class="type">Dataset</span>[<span class="type">AdsInfo</span>] = <span class="type">BlackListApp</span>.startBlackList(spark, adsInfoDS)</span><br><span class="line"><span class="comment">//    BlackListApp.startBlackList(spark, adsInfoDS)</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​		此时在 redis-cli 中查看 redis内容，可见 黑名单 里的数据越来越多了。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-04%2010.08.55.jpg" alt="截屏2022-07-04 10.08.55"></p>
<h4 id="统计广告点击量"><a href="#统计广告点击量" class="headerlink" title="统计广告点击量"></a>统计广告点击量</h4><p>​		<strong>(需求)</strong></p>
<p>​		每天每地区每城市每广告的点击流量实时统计</p>
<p>​		<strong>(技法)</strong></p>
<p>​		在spark structure streaming 的编写上，要注意：</p>
<p>​		1、编写步骤上、应先用console的形式输出到控制台，再一点一点去完善代码。</p>
<p>​		2、spark处理数据的时间间隔最好要 小于 kafka等数据流生成的时间间隔。</p>
<p>​		3、awaitTermiation() 放的位置是 流式处理的最后一个writestreaming后尾作为收尾标志的，若之前有多次流式处理，写到start() 即可，运行起来以找到 awaitTermiation() 作为一整个流式处理的结束。</p>
<p>​		4、如果运行一直没结果，又不明不白的停滞了，jps 检查一下看看hadoop 有没有 &#x2F;tmp&#x2F;hsperf 错误，zookeeper、kafka、hadoop都重来一遍，checkpointLocation删除重新生成一个新的。再来一遍。</p>
<p>​		先在控制台console 中检查数据。如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka_to_spark_test.app</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka_to_spark_test.bean.<span class="type">AdsInfo</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.<span class="type">Trigger</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AdsClickCountApp</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 每天每地区每城市广告点击量实时统计</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">statClickCount</span></span>(spark:<span class="type">SparkSession</span>,filtered_adsInfoDS:<span class="type">Dataset</span>[<span class="type">AdsInfo</span>]):<span class="type">Unit</span>=&#123;</span><br><span class="line">    <span class="keyword">val</span> resultDF: <span class="type">DataFrame</span> = filtered_adsInfoDS</span><br><span class="line">      .groupBy(<span class="string">&quot;dayString&quot;</span>, <span class="string">&quot;area&quot;</span>, <span class="string">&quot;city&quot;</span>, <span class="string">&quot;adsId&quot;</span>)</span><br><span class="line">      .count()</span><br><span class="line"></span><br><span class="line">    resultDF.writeStream</span><br><span class="line">      .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">      .outputMode(<span class="string">&quot;complete&quot;</span> )</span><br><span class="line">      .trigger(<span class="type">Trigger</span>.<span class="type">ProcessingTime</span>(<span class="string">&quot;2 seconds&quot;</span>))</span><br><span class="line">      .start()</span><br><span class="line">      .awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka_to_spark_test.app</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka_to_spark_test.app.&#123;<span class="type">BlackListApp</span>,<span class="type">AdsClickCountApp</span>&#125;</span><br><span class="line"><span class="keyword">import</span> kafka_to_spark_test.bean.<span class="type">AdsInfo</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Timestamp</span></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealtimeApp1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;realtimeApp&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    spark.sparkContext.setLogLevel(<span class="string">&quot;WARN&quot;</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> dayStringFormatter = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> hmStringFormatter = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;HH:mm&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    从 kafka 读取数据, 为了方便后续处理, 封装数据到 AdsInfo 样例类中</span></span><br><span class="line">    <span class="keyword">val</span> adsInfoDS: <span class="type">Dataset</span>[<span class="type">AdsInfo</span>] = spark.readStream</span><br><span class="line">      .format(<span class="string">&quot;kafka&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;node01:9092&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;subscribe&quot;</span>, <span class="string">&quot;ads_log&quot;</span>)</span><br><span class="line">      .load</span><br><span class="line">      .select(<span class="string">&quot;value&quot;</span>)</span><br><span class="line">      .as[<span class="type">String</span>]</span><br><span class="line">      .map(v =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> split: <span class="type">Array</span>[<span class="type">String</span>] = v.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> date = <span class="keyword">new</span> <span class="type">Date</span>(split(<span class="number">0</span>).toLong)</span><br><span class="line">        <span class="type">AdsInfo</span>(</span><br><span class="line">          <span class="keyword">new</span> <span class="type">Timestamp</span>(split(<span class="number">0</span>).toLong),</span><br><span class="line">          dayStringFormatter.format(date),</span><br><span class="line">          hmStringFormatter.format(date),</span><br><span class="line">          split(<span class="number">2</span>),</span><br><span class="line">          split(<span class="number">3</span>),</span><br><span class="line">          split(<span class="number">1</span>),</span><br><span class="line">          split(<span class="number">4</span>)</span><br><span class="line">        )</span><br><span class="line">      &#125;)</span><br><span class="line">      .withWatermark(<span class="string">&quot;timestamp&quot;</span>,<span class="string">&quot;24 hours&quot;</span> )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 需求一：写 黑名单入redis，并且返回过滤后的值</span></span><br><span class="line">    <span class="keyword">val</span> filtered_adsInfoDS: <span class="type">Dataset</span>[<span class="type">AdsInfo</span>] = <span class="type">BlackListApp</span>.startBlackList(spark, adsInfoDS)</span><br><span class="line"><span class="comment">//    BlackListApp.startBlackList(spark, adsInfoDS)</span></span><br><span class="line">    <span class="comment">// 需求2: 每天每地区每城市广告点击量实时统计</span></span><br><span class="line">    <span class="type">AdsClickCountApp</span>.statClickCount(spark, filtered_adsInfoDS)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​		运行 模拟生成器 和 RealtimeApp1，在console可见</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-12%2015.28.13.jpg" alt="截屏2022-07-12 15.28.13"></p>
<p>​		</p>
<p>​		用foreachebatch来写入redis。</p>
<p>​		<strong>注意事项：</strong></p>
<p>​		1、spark structure streaming最好先 <code>jps</code> 检查一下 有无 【进程号】 – process information unavailable 这个错误，如果有，则删除 &#x2F;tmp&#x2F;hsperfdata_{username} 那几个文件夹，再重启hadoop、zookeeper、Kafka；</p>
<p>​		2、jvm 空间不足，除了我们 <code>cd /etc/security/limits.d</code> 将文件 xxxx.config 的 * soft nproc xxxx 注释掉以外；reboot也可以解决；</p>
<p>​		3、总是提示 foreachBatch \ foreachPartition 的参数 类型缺失<code>missing parameter type</code>，我们要手动补全，像这样 <code>foreachBatch&#123; (df:DataFrame,batchId:Long)=&gt;&#123;</code>   <code>df.foreachPartition((rowIt: Iterator[Row]) =&gt; &#123;</code> ；可以看官网，也可以在idea中ctrl+p 查看参数提示，以官网为准；</p>
<p>​		4、hmset 之类的hash格式在python中写入dict即可，scala中需要把scala的map转化成java的map，dataframe中可以保存map格式的数据，df 需要使用getAs写一下Map[…,…]类型，否则Map中的类型无法自动识别，像这样 <code>row.getAs[Long](4)</code>；</p>
<p>​		5、redis 存map需要的是Java 类型的map，scala的map需要先转成Java的map才能存，现在转的途径只有javaconverter配合asjava，而 javaconversion已经不用了；</p>
<p>​		6、<code>报错java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.String</code>  这是因为 java <code>toString</code> 的强制类型转换可能报这个错，用 <code>String.valueOf()</code> 这样强转 不会报错。</p>
<p>​		7、一个非常让我费解的错误 <code>[Overloaded method foreachBatch with alternatives]</code> ，foreachbatch重载后分不清找哪一个， 解决方法我是在这里看到的<code>https://stackoverflow.com/questions/63137538/overloaded-method-foreachbatch-with-alternatives</code>在 foreachbatch 的最后，也就是解除持久化 df.unpersist() 下面加上一个 “ () ” 符号，就能解决问题，我现在也不明白为什么，但是可以解决问题。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// AdsClickCountApp</span></span><br><span class="line"><span class="keyword">package</span> kafka_to_spark_test.app</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka_to_spark_test.bean.<span class="type">AdsInfo</span></span><br><span class="line"><span class="keyword">import</span> kafka_to_spark_test.util.<span class="type">RedisUtil</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.<span class="type">Trigger</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.<span class="type">Jedis</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AdsClickCountApp</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 每天每地区每城市广告点击量实时统计</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">statClickCount</span></span>(spark:<span class="type">SparkSession</span>,filtered_adsInfoDS:<span class="type">Dataset</span>[<span class="type">AdsInfo</span>]):<span class="type">Unit</span>=&#123;</span><br><span class="line">    <span class="keyword">val</span> resultDF: <span class="type">DataFrame</span> = filtered_adsInfoDS</span><br><span class="line">      .groupBy(<span class="string">&quot;dayString&quot;</span>, <span class="string">&quot;area&quot;</span>, <span class="string">&quot;city&quot;</span>, <span class="string">&quot;adsId&quot;</span>)</span><br><span class="line">      .count()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    resultDF.writeStream</span><br><span class="line">      .outputMode(<span class="string">&quot;complete&quot;</span> )</span><br><span class="line">      .foreachBatch&#123; (df:<span class="type">DataFrame</span>,batchId:<span class="type">Long</span>)=&gt;&#123;</span><br><span class="line">        df.persist()</span><br><span class="line">        println(<span class="string">&quot;come here foreachbatch&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span>(df.count()&gt;<span class="number">0</span>)&#123;</span><br><span class="line">          println(<span class="string">&quot;df count &gt; 0&quot;</span>)</span><br><span class="line">          df.foreachPartition((rowIt: <span class="type">Iterator</span>[<span class="type">Row</span>]) =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> client: <span class="type">Jedis</span> = <span class="type">RedisUtil</span>.getJedisClient</span><br><span class="line">            <span class="keyword">var</span> dayString:<span class="type">String</span> = <span class="string">&quot;&quot;</span>        <span class="comment">// 可变对象在 别的方法内加工一下</span></span><br><span class="line">            <span class="keyword">val</span> dict_map_asValue: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = rowIt.map(row =&gt; &#123;</span><br><span class="line">              dayString = row.getString(<span class="number">0</span>)</span><br><span class="line">              <span class="keyword">val</span> area: <span class="type">String</span> = row.getString(<span class="number">1</span>)</span><br><span class="line">              <span class="keyword">val</span> city: <span class="type">String</span> = row.getString(<span class="number">2</span>)</span><br><span class="line">              <span class="keyword">val</span> adsId: <span class="type">String</span> = row.getString(<span class="number">3</span>)</span><br><span class="line">              <span class="keyword">val</span> count: <span class="type">String</span> = <span class="type">String</span>.valueOf(row.getAs[<span class="type">Long</span>](<span class="number">4</span>)) <span class="comment">// toString 报错java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.String.</span></span><br><span class="line">              (<span class="string">s&quot;<span class="subst">$area</span>:<span class="subst">$city</span>:<span class="subst">$adsId</span>&quot;</span>, count)    <span class="comment">//getAs写一下Map[...,...]类型，否则Map中的类型无法自动识别。</span></span><br><span class="line">            &#125;).toMap</span><br><span class="line">            <span class="keyword">if</span>(dict_map_asValue.size&gt;<span class="number">0</span>)&#123;</span><br><span class="line">              <span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line">              <span class="comment">// 因为前面的map都是scala的map，而redis需要java的map，</span></span><br><span class="line">              <span class="comment">// 所以用javaconverter配合asjava 来转成java 版的map。</span></span><br><span class="line">              <span class="keyword">val</span> dict_map_asValue1: java.util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = dict_map_asValue.asJava</span><br><span class="line">              client.hmset(<span class="string">s&quot;date:area:city:ads:<span class="subst">$dayString</span>&quot;</span>,dict_map_asValue1)</span><br><span class="line">            &#125;</span><br><span class="line">            client.close()</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125;</span><br><span class="line">        df.unpersist()</span><br><span class="line">        ()   <span class="comment">// 通过在末尾添加（）,从而解决函数中的模糊性,overloaded method foreachBatch with alternatives:</span></span><br><span class="line">      &#125;&#125;</span><br><span class="line">      .trigger(<span class="type">Trigger</span>.<span class="type">ProcessingTime</span>(<span class="string">&quot;2 seconds&quot;</span>))</span><br><span class="line">      .start()</span><br><span class="line">      .awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// RealtimeApp1</span></span><br><span class="line"><span class="keyword">package</span> kafka_to_spark_test.app</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka_to_spark_test.app.&#123;<span class="type">BlackListApp</span>,<span class="type">AdsClickCountApp</span>&#125;</span><br><span class="line"><span class="keyword">import</span> kafka_to_spark_test.bean.<span class="type">AdsInfo</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Timestamp</span></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealtimeApp1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;realtimeApp&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    spark.sparkContext.setLogLevel(<span class="string">&quot;WARN&quot;</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> dayStringFormatter = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> hmStringFormatter = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;HH:mm&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    从 kafka 读取数据, 为了方便后续处理, 封装数据到 AdsInfo 样例类中</span></span><br><span class="line">    <span class="keyword">val</span> adsInfoDS: <span class="type">Dataset</span>[<span class="type">AdsInfo</span>] = spark.readStream</span><br><span class="line">      .format(<span class="string">&quot;kafka&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;node01:9092&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;subscribe&quot;</span>, <span class="string">&quot;ads_log&quot;</span>)</span><br><span class="line">      .load</span><br><span class="line">      .select(<span class="string">&quot;value&quot;</span>)</span><br><span class="line">      .as[<span class="type">String</span>]</span><br><span class="line">      .map(v =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> split: <span class="type">Array</span>[<span class="type">String</span>] = v.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> date = <span class="keyword">new</span> <span class="type">Date</span>(split(<span class="number">0</span>).toLong)</span><br><span class="line">        <span class="type">AdsInfo</span>(</span><br><span class="line">          <span class="keyword">new</span> <span class="type">Timestamp</span>(split(<span class="number">0</span>).toLong),</span><br><span class="line">          dayStringFormatter.format(date),</span><br><span class="line">          hmStringFormatter.format(date),</span><br><span class="line">          split(<span class="number">2</span>),</span><br><span class="line">          split(<span class="number">3</span>),</span><br><span class="line">          split(<span class="number">1</span>),</span><br><span class="line">          split(<span class="number">4</span>)</span><br><span class="line">        )</span><br><span class="line">      &#125;)</span><br><span class="line">      .withWatermark(<span class="string">&quot;timestamp&quot;</span>,<span class="string">&quot;24 hours&quot;</span> )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 需求一：写 黑名单入redis，并且返回过滤后的值</span></span><br><span class="line">    <span class="keyword">val</span> filtered_adsInfoDS: <span class="type">Dataset</span>[<span class="type">AdsInfo</span>] = <span class="type">BlackListApp</span>.startBlackList(spark, adsInfoDS)</span><br><span class="line"><span class="comment">//    BlackListApp.startBlackList(spark, adsInfoDS)</span></span><br><span class="line">    <span class="comment">// 需求2: 每天每地区每城市广告点击量实时统计</span></span><br><span class="line">    <span class="type">AdsClickCountApp</span>.statClickCount(spark, filtered_adsInfoDS)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​		先运行前面的模拟数据模块 mockRealTime，再运行最后一版的RealtimeApp1 ，启动了流式计算的 “两个流”，前一个流写 黑名单入redis，并且返回过滤后的值，后一个流 实时统计每天每地区每城市广告点击量，运行一段时间后，用显示原数据的模式打开redis客户端 <code>redis-cli --raw</code>（为了显示中文） 可见：</p>
<p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-15%2021.12.08.jpg" alt="截屏2022-07-15 21.12.08"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-15%2021.12.28.jpg" alt="截屏2022-07-15 21.12.28"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-15%2021.14.12.jpg" alt="截屏2022-07-15 21.14.12"></p>
<p>​		等待一会儿，redis 中就有了两个key，一个是黑名单（set类型），一个是最后的统计点击量数据（map类型），我们看见黑名单内数据慢慢增多，最后统计点击量数据 也慢慢增多，在所有数据都入了黑名单之后，最后统计点击量的数据也就不再发生变化。		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-07-15%2021.20.11.jpg" alt="截屏2022-07-15 21.20.11"></p>
<h2 id="词频统计"><a href="#词频统计" class="headerlink" title="词频统计"></a>词频统计</h2><p>​		还是以kafka为输入源。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server node01:9092 --create --topic kafka-structure-stream --partitions 3 --replication-factor 2</span><br><span class="line"></span><br><span class="line">kafka-console-producer.sh --broker-list node01:9092 --topic kafka-structure-stream</span><br></pre></td></tr></table></figure>

<p>​		编写Structured Streaming程序的基本步骤包括：</p>
<ul>
<li>导入pyspark模块</li>
<li>创建SparkSession对象</li>
<li>创建输入数据源</li>
<li>定义流计算过程</li>
<li>启动流计算并输出结果</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/append-output-not-supported-no-watermark">https://docs.microsoft.com/en-us/azure/databricks/kb/streaming/append-output-not-supported-no-watermark</a></p>
<p>注意事项：spark structure streaming 中 append mode情况下，聚合函数像groupby，参数都要加一个时间戳列，或者窗口函数，来确定是在哪个时间范围内做聚合操作  The aggregation must have an event-time column, or a window on the event-time column.。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2011.00.16.jpg" alt="截屏2022-05-28 10.59.05"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/sparkstreaming-%E5%86%85%E6%B6%B5structurestreaming/%E6%88%AA%E5%B1%8F2022-05-28%2021.32.02.jpg" alt="截屏2022-05-28 21.32.02"></p>
<p>​		</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/spark-streaming/" rel="tag"># spark_streaming</a>
              <a href="/tags/structure-streaming/" rel="tag"># structure_streaming</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/SparkSQL/" rel="prev" title="SparkSQL">
                  <i class="fa fa-chevron-left"></i> SparkSQL
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-%E5%86%B3%E7%AD%96%E6%A0%91-%E7%9B%B8%E4%BA%B2%E5%86%B3%E7%AD%96%E6%A1%88%E4%BE%8B/" rel="next" title="spark_决策树-相亲决策案例">
                  spark_决策树-相亲决策案例 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">陈宇韶chenyushao</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  





</body>
</html>
