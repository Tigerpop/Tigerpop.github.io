<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.13.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="这是文章开头，显示在主页面，详情请点击此处。">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch的多GPU训练">
<meta property="og:url" content="http://example.com/2024/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch%E7%9A%84%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/index.html">
<meta property="og:site_name" content="Tiger_pop&#39;s Blog">
<meta property="og:description" content="这是文章开头，显示在主页面，详情请点击此处。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2024/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch%E7%9A%84%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/ddp.jpg">
<meta property="article:published_time" content="2024-11-07T08:58:30.000Z">
<meta property="article:modified_time" content="2024-11-11T00:46:05.047Z">
<meta property="article:author" content="陈宇韶chenyushao">
<meta property="article:tag" content="torch">
<meta property="article:tag" content="GPU">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch%E7%9A%84%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/ddp.jpg">


<link rel="canonical" href="http://example.com/2024/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch%E7%9A%84%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2024/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch%E7%9A%84%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/","path":"2024/11/07/机器学习/pytorch的多GPU训练/","title":"pytorch的多GPU训练"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>pytorch的多GPU训练 | Tiger_pop's Blog</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Tiger_pop's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Tiger_pop's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">tiger_pop 的博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1"><span class="nav-number">1.</span> <span class="nav-text">单机多卡</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#DP%EF%BC%88%E8%A2%AB%E6%B7%98%E6%B1%B0%EF%BC%89"><span class="nav-number">1.1.</span> <span class="nav-text">DP（被淘汰）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DDP-%E6%8E%A8%E8%8D%90"><span class="nav-number">1.2.</span> <span class="nav-text">DDP (推荐)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DDP%E4%BE%8B%E5%AD%90"><span class="nav-number">2.</span> <span class="nav-text">DDP例子</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E6%88%91%E7%BB%8F%E5%B8%B8%E7%94%A8%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">3.</span> <span class="nav-text">一个我经常用的例子</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="陈宇韶chenyushao"
      src="/images/my.jpg">
  <p class="site-author-name" itemprop="name">陈宇韶chenyushao</p>
  <div class="site-description" itemprop="description">爱学习、爱工作、爱生活;         微信号: Tiger_and_master;         手机号码:18515678348 </div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">427</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">201</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch%E7%9A%84%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my.jpg">
      <meta itemprop="name" content="陈宇韶chenyushao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tiger_pop's Blog">
      <meta itemprop="description" content="爱学习、爱工作、爱生活;         微信号: Tiger_and_master;         手机号码:18515678348 ">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="pytorch的多GPU训练 | Tiger_pop's Blog">
      <meta itemprop="description" content="这是文章开头，显示在主页面，详情请点击此处。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          pytorch的多GPU训练
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-11-07 16:58:30" itemprop="dateCreated datePublished" datetime="2024-11-07T16:58:30+08:00">2024-11-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-11-11 08:46:05" itemprop="dateModified" datetime="2024-11-11T08:46:05+08:00">2024-11-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">这是文章开头，显示在主页面，详情请点击此处。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="单机多卡"><a href="#单机多卡" class="headerlink" title="单机多卡"></a>单机多卡</h1><h2 id="DP（被淘汰）"><a href="#DP（被淘汰）" class="headerlink" title="DP（被淘汰）"></a>DP（被淘汰）</h2><p><strong>torch.nn.DataParallel</strong></p>
<ul>
<li>简单一行代码 ，封装model即可。<code>model = DataParallel(model.cuda(),device_ids=[0,1,23] ) </code>;</li>
<li>模型保存与加载；由于被封装以后，model已经是一个DataParallel，所以 torch.save 调用 <code>model.module.state_dict()</code> ,<code>torch.load </code>注意 <code>map_location </code>;</li>
<li>缺点-（单进程，效率低）；</li>
<li>缺点- （不支持多机器）；</li>
<li>缺点-（不支持模型并行）；</li>
<li><code>batch_size </code>改成每个GPU <code>batch_size</code> 总和；</li>
</ul>
<h2 id="DDP-推荐"><a href="#DDP-推荐" class="headerlink" title="DDP (推荐)"></a>DDP (推荐)</h2><p> **torch.nn.parallel.DistributedDataParallel **</p>
<ul>
<li><p>多进程</p>
</li>
<li><p>初始化进程组<code> torch.distributed.init_process_group(&quot;nccl&quot;,world_size=n_gpus,rank=args.local_rank)</code></p>
</li>
<li><p>指定当前进程使用哪一张 GPU 卡</p>
<p> <code>torch.cuda.set_device(args.local_rank)</code> 相当于 CUDA_VISIBLE_DEVICES 环境变量</p>
</li>
<li><p>模型封装<br>(模型先转移到某一张 GPU卡上，然后再被ddp封装，注意是<code>device_ids</code>指定一张卡就好了，一个进程一张卡。) <code>model = DistributedDataParallel(model.cuda(args.local_rank),device_ids=[args.local_rank])</code></p>
</li>
<li><p>将数据分布式分配到GPU上，给出一个训练顺序</p>
<p><code>train_sampler = DistributedSampler(train_dataset)</code><br>最好读一读 源码 <code>torch/util/data/distributed.py</code></p>
</li>
<li><p>每个周期开始前，调用<code>train_sampler.set_epoch(epoch)</code> 打乱数据</p>
</li>
<li><p>有了 <code>sampler</code> 就不需要在 <code>DataLoader </code>设置<code>shuffle=True</code></p>
</li>
<li><p>将<code>train_sampler</code>传入 <code>train_dataloader</code> 中</p>
<p><code>train_dataloader = DataLoader(...,sampler=train_sampler)</code></p>
</li>
<li><p>数据拷贝到GPU卡</p>
<p><code>data = data.cuda(args.local_rank)</code></p>
</li>
<li><p>执行命令</p>
<p>(定一个节点上用几张卡)</p>
<p><code>python -m torch.distributed.launch --nproc_per_node=n_gpus train.py</code></p>
<p><code>torch.distributed.launch</code> 会定多少个进程，根据前面传入的 节点内卡的数量,launch这个进程会向 每一个 train.py 传入  local_rank。</p>
</li>
<li><p>模型保存和加载</p>
<p><code>torch.save</code>在<code>local_rank=0</code> 的位置进行保存，同样注意调用<code> model.module.state_dict()</code></p>
<p><code>torch.load</code>注意<code>map_location</code></p>
</li>
<li><p>每一个进程所需要的 <code>batch_size</code> 应该是每一个 GPU所需要的 <code>batch_size</code>大小</p>
</li>
</ul>
<h1 id="DDP例子"><a href="#DDP例子" class="headerlink" title="DDP例子"></a>DDP例子</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.distributed <span class="keyword">import</span> DistributedSampler</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="comment"># torch.distributed: 用于分布式训练的主要模块</span></span><br><span class="line"><span class="comment"># DistributedDataParallel: 用于模型的分布式封装</span></span><br><span class="line"><span class="comment"># DistributedSampler: 用于数据的分布式采样</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里创建了一个模拟数据集：</span></span><br><span class="line"><span class="comment"># 生成随机的3通道32x32图像</span></span><br><span class="line"><span class="comment"># 生成0-9之间的随机标签</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size=<span class="number">1000</span></span>):</span><br><span class="line">        self.size = size</span><br><span class="line">        self.data = torch.randn(size, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)  <span class="comment"># 模拟图像数据</span></span><br><span class="line">        self.labels = torch.randint(<span class="number">0</span>, <span class="number">10</span>, (size,))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.size</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx], self.labels[idx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个简单的卷积神经网络：</span></span><br><span class="line"><span class="comment"># 特征提取部分：两个卷积层，每个后面跟ReLU和池化</span></span><br><span class="line"><span class="comment"># 分类器部分：两个全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvNet, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">128</span> * <span class="number">8</span> * <span class="number">8</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分布式设置函数:</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setup_distributed</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="comment"># 可以通过环境变量设置 如果注释 就需要在 终端-m torch.distributed.launch 后指定端口号。</span></span><br><span class="line">    <span class="comment"># os.environ[&#x27;MASTER_PORT&#x27;] = &#x27;29500&#x27;  </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化进程组</span></span><br><span class="line">    torch.distributed.init_process_group(</span><br><span class="line">        backend=<span class="string">&quot;nccl&quot;</span>,         <span class="comment"># 使用NCCL后端，专门优化用于GPU</span></span><br><span class="line">        world_size=args.n_gpus, <span class="comment"># 总GPU数量</span></span><br><span class="line">        rank=args.local_rank    <span class="comment"># 当前进程的rank</span></span><br><span class="line">    ) </span><br><span class="line">    torch.cuda.set_device(args.local_rank) <span class="comment"># 设置当前进程使用的GPU</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cleanup</span>():</span><br><span class="line">    dist.destroy_process_group()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型保存函数:</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_model</span>(<span class="params">model, epoch, optimizer, args</span>):</span><br><span class="line">    <span class="keyword">if</span> args.local_rank == <span class="number">0</span>:  <span class="comment"># 只在主进程保存</span></span><br><span class="line">        checkpoint = &#123;</span><br><span class="line">            <span class="string">&#x27;epoch&#x27;</span>: epoch,</span><br><span class="line">            <span class="string">&#x27;model_state_dict&#x27;</span>: model.module.state_dict(),<span class="comment"># 注意使用model.module获取原始模型参数</span></span><br><span class="line">            <span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),<span class="comment"># 保存优化器状态</span></span><br><span class="line">        &#125;</span><br><span class="line">        save_path = <span class="string">f&#x27;checkpoint_epoch_<span class="subst">&#123;epoch&#125;</span>.pt&#x27;</span></span><br><span class="line">        torch.save(checkpoint, save_path)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Saved checkpoint to <span class="subst">&#123;save_path&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型加载函数:</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_model</span>(<span class="params">model, optimizer, checkpoint_path, args</span>):</span><br><span class="line">    map_location = &#123;<span class="string">&#x27;cuda:%d&#x27;</span> % <span class="number">0</span>: <span class="string">&#x27;cuda:%d&#x27;</span> % args.local_rank&#125; <span class="comment"># 保模型加载到正确的GPU</span></span><br><span class="line">    checkpoint = torch.load(checkpoint_path, map_location=map_location)</span><br><span class="line">    model.module.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>]) <span class="comment"># 加载模型状态</span></span><br><span class="line">    optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>]) <span class="comment"># 加载优化器状态</span></span><br><span class="line">    <span class="keyword">return</span> checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="comment"># 1. 设置分布式环境</span></span><br><span class="line">    setup_distributed(args)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. 创建数据集和数据加载器</span></span><br><span class="line">    train_dataset = CustomDataset()</span><br><span class="line">    train_sampler = DistributedSampler(train_dataset)</span><br><span class="line">    </span><br><span class="line">    train_loader = DataLoader(</span><br><span class="line">        train_dataset,</span><br><span class="line">        batch_size=args.batch_size,  <span class="comment"># 这里的batch_size是每个GPU的batch_size</span></span><br><span class="line">        sampler=train_sampler,</span><br><span class="line">        num_workers=<span class="number">4</span>,</span><br><span class="line">        pin_memory=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 创建模型和优化器</span></span><br><span class="line">    model = ConvNet()</span><br><span class="line">    model = model.cuda(args.local_rank)  <span class="comment"># 先移到对应GPU</span></span><br><span class="line">    model = DDP(</span><br><span class="line">        model,</span><br><span class="line">        device_ids=[args.local_rank],</span><br><span class="line">        output_device=args.local_rank</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. 如果有检查点，加载模型</span></span><br><span class="line">    start_epoch = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> args.resume <span class="keyword">and</span> os.path.exists(args.checkpoint):</span><br><span class="line">        start_epoch = load_model(model, optimizer, args.checkpoint, args)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 5. 训练循环</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(start_epoch, args.epochs):</span><br><span class="line">        train_sampler.set_epoch(epoch)  <span class="comment"># 确保每个epoch数据被打乱</span></span><br><span class="line">        </span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">            <span class="comment"># 数据移到对应GPU</span></span><br><span class="line">            data = data.cuda(args.local_rank, non_blocking=<span class="literal">True</span>)</span><br><span class="line">            target = target.cuda(args.local_rank, non_blocking=<span class="literal">True</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 前向传播</span></span><br><span class="line">            output = model(data)</span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 只在主进程打印信息</span></span><br><span class="line">            <span class="keyword">if</span> batch_idx % args.print_freq == <span class="number">0</span> <span class="keyword">and</span> args.local_rank == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch&#125;</span>/<span class="subst">&#123;args.epochs&#125;</span>, &#x27;</span></span><br><span class="line">                      <span class="string">f&#x27;Batch: <span class="subst">&#123;batch_idx&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(train_loader)&#125;</span>, &#x27;</span></span><br><span class="line">                      <span class="string">f&#x27;Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每个epoch结束后保存模型</span></span><br><span class="line">        save_model(model, epoch, optimizer, args)</span><br><span class="line">    </span><br><span class="line">    cleanup()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--local_rank&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=-<span class="number">1</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--n_gpus&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">4</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">32</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--print_freq&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--resume&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--checkpoint&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    </span><br><span class="line">    train(args)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>



<p>在终端输入</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python -m torch.distributed.launch \</span><br><span class="line">    --nproc_per_node=4 \</span><br><span class="line">    --master_port=29500 \</span><br><span class="line">    train.py \</span><br><span class="line">    --n_gpus=4 \</span><br><span class="line">    --batch_size=32 \</span><br><span class="line">    --epochs=10</span><br></pre></td></tr></table></figure>

<blockquote>
<p>找到 PyTorch 安装目录中的 distributed&#x2F;launch.py<br>执行这个启动器脚本<br>启动器会创建多个进程来运行 train.py<br><code>-m</code> 表示将一个模块作为脚本运行，python a.py arg1 b.py arg2 c.py arg3 经常这样用。</p>
<p>命令行参数解释：</p>
<ul>
<li><code>--nproc_per_node=4</code>: 使用4个GPU</li>
<li><code>--master_port=29500</code>: 指定主进程端口</li>
<li><code>--n_gpus=4</code>: 总GPU数量</li>
<li><code>--batch_size=32</code>: 每个GPU的batch size</li>
<li><code>--epochs=10</code>: 训练轮数</li>
</ul>
</blockquote>
<h1 id="一个我经常用的例子"><a href="#一个我经常用的例子" class="headerlink" title="一个我经常用的例子"></a>一个我经常用的例子</h1><p>这个是做 环境部署和GPU性能检测的。<img src="/2024/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch%E7%9A%84%E5%A4%9AGPU%E8%AE%AD%E7%BB%83/ddp.jpg" alt="ddp"></p>
<p><code>ddp5_env_check.py</code>  </p>
<blockquote>
<p>他会检测 驱动 torch conda等情况，并用此脚本文件夹中图片做多GPU测试。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> platform</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models, transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> ResNet50_Weights</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="comment"># from tqdm import tqdm</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader, DistributedSampler</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查并安装 tqdm</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">install_tqdm</span>():</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">import</span> tqdm</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;tqdm 已安装，不需要重复安装。&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> ModuleNotFoundError:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;tqdm 未安装，正在使用 pip 自动安装...&quot;</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            subprocess.check_call([<span class="string">f&quot;<span class="subst">&#123;os.getenv(<span class="string">&#x27;CONDA_PREFIX&#x27;</span>)&#125;</span>/bin/pip&quot;</span>, <span class="string">&quot;install&quot;</span>, <span class="string">&quot;tqdm&quot;</span>, <span class="string">&quot;-i&quot;</span>, <span class="string">&quot;https://pypi.tuna.tsinghua.edu.cn/simple&quot;</span>])</span><br><span class="line">            <span class="keyword">import</span> tqdm</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;tqdm 安装成功！&quot;</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;tqdm 安装失败: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">            exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查系统信息</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">system_info</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;系统信息:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;操作系统: <span class="subst">&#123;platform.system()&#125;</span> <span class="subst">&#123;platform.release()&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> platform.system() == <span class="string">&#x27;Linux&#x27;</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;/etc/os-release&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">&quot;PRETTY_NAME&quot;</span> <span class="keyword">in</span> line:</span><br><span class="line">                        ubuntu_version = line.split(<span class="string">&quot;=&quot;</span>)[<span class="number">1</span>].strip().strip(<span class="string">&#x27;&quot;&#x27;</span>)</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">f&quot;Ubuntu 版本: <span class="subst">&#123;ubuntu_version&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;无法获取 Ubuntu 版本信息: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Ubuntu 版本: N/A&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Python 版本: <span class="subst">&#123;platform.python_version()&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Conda 虚拟环境: <span class="subst">&#123;os.getenv(<span class="string">&#x27;CONDA_DEFAULT_ENV&#x27;</span>, <span class="string">&#x27;Not in a conda environment&#x27;</span>)&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 PyTorch 和 CUDA 信息</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pytorch_cuda_info</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;PyTorch 和 CUDA 信息:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;PyTorch 版本: <span class="subst">&#123;torch.__version__&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;CUDA 可用性: <span class="subst">&#123;torch.cuda.is_available()&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;CUDA 版本: <span class="subst">&#123;torch.version.cuda&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;cuDNN 版本: <span class="subst">&#123;torch.backends.cudnn.version()&#125;</span>&quot;</span>)</span><br><span class="line">        gpu_count = torch.cuda.device_count()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;检测到 <span class="subst">&#123;gpu_count&#125;</span> 个 GPU&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(gpu_count):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;GPU <span class="subst">&#123;i&#125;</span> 名称: <span class="subst">&#123;torch.cuda.get_device_name(i)&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;CUDA 未启用，请检查 CUDA 安装&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 PyTorch 是否调用了虚拟环境中的版本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_virtual_env</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;正在检查是否使用虚拟环境中的 PyTorch 版本...&quot;</span>)</span><br><span class="line">    conda_env = os.getenv(<span class="string">&#x27;CONDA_DEFAULT_ENV&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> conda_env:</span><br><span class="line">        pytorch_path = torch.__file__</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;PyTorch 安装路径: <span class="subst">&#123;pytorch_path&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> conda_env <span class="keyword">in</span> pytorch_path:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;PyTorch 来自虚拟环境 &#x27;<span class="subst">&#123;conda_env&#125;</span>&#x27;&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;警告: PyTorch 没有来自当前虚拟环境&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;未在 Conda 虚拟环境中运行&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImageDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_paths, transform</span>):</span><br><span class="line">        self.img_paths = img_paths</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_paths)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_path = self.img_paths[idx]</span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line">        <span class="keyword">return</span> img, img_path</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image_computation_test</span>(<span class="params">local_rank, args</span>):</span><br><span class="line">    <span class="comment"># 初始化进程组</span></span><br><span class="line">    torch.cuda.set_device(local_rank)</span><br><span class="line">    dist.init_process_group(backend=<span class="string">&quot;nccl&quot;</span>, </span><br><span class="line">                          init_method=<span class="string">f&quot;env://&quot;</span>,</span><br><span class="line">                          world_size=args.n_gpus,</span><br><span class="line">                          rank=local_rank)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Rank <span class="subst">&#123;local_rank&#125;</span>/<span class="subst">&#123;args.n_gpus&#125;</span> 开始工作...&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化模型</span></span><br><span class="line">    model = models.resnet50(weights=ResNet50_Weights.DEFAULT).cuda(local_rank)</span><br><span class="line">    model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 图片预处理</span></span><br><span class="line">    preprocess = transforms.Compose([</span><br><span class="line">        transforms.Resize(<span class="number">256</span>),</span><br><span class="line">        transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建数据集和数据加载器</span></span><br><span class="line">    dataset = ImageDataset(args.img_paths, preprocess)</span><br><span class="line">    sampler = DistributedSampler(dataset)</span><br><span class="line">    dataloader = DataLoader(dataset, </span><br><span class="line">                          batch_size=args.batch_size,</span><br><span class="line">                          sampler=sampler,</span><br><span class="line">                          num_workers=args.num_workers,</span><br><span class="line">                          pin_memory=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">        <span class="keyword">if</span> local_rank == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;\nEpoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;args.epochs&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        sampler.set_epoch(epoch)  <span class="comment"># 确保每个epoch的数据顺序不同</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> img_tensor, img_path <span class="keyword">in</span> dataloader:</span><br><span class="line">            img_tensor = img_tensor.cuda(local_rank)</span><br><span class="line">            iterations = args.iterations</span><br><span class="line">            success = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(iterations), </span><br><span class="line">                            desc=<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, 处理 <span class="subst">&#123;img_path[<span class="number">0</span>]&#125;</span>&quot;</span>, </span><br><span class="line">                            disable=local_rank!=<span class="number">0</span>):</span><br><span class="line">                    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                        output = model(img_tensor)</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;运算失败: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">                success = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> success:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Rank <span class="subst">&#123;local_rank&#125;</span> 运算结束，处理了 <span class="subst">&#123;iterations&#125;</span> 次图片 &#x27;<span class="subst">&#123;img_path[<span class="number">0</span>]&#125;</span>&#x27;&quot;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Rank <span class="subst">&#123;local_rank&#125;</span> 图片 &#x27;<span class="subst">&#123;img_path[<span class="number">0</span>]&#125;</span>&#x27; 运算失败&quot;</span>)</span><br><span class="line"></span><br><span class="line">    elapsed = time.time() - start_time</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Rank <span class="subst">&#123;local_rank&#125;</span> 总耗时: <span class="subst">&#123;elapsed:<span class="number">.2</span>f&#125;</span> 秒&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 同步所有进程的时间</span></span><br><span class="line">    elapsed_tensor = torch.tensor([elapsed], device=<span class="string">f&quot;cuda:<span class="subst">&#123;local_rank&#125;</span>&quot;</span>)</span><br><span class="line">    gathered_times = [torch.zeros_like(elapsed_tensor) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(args.n_gpus)]</span><br><span class="line">    dist.all_gather(gathered_times, elapsed_tensor)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> local_rank == <span class="number">0</span>:</span><br><span class="line">        total_time = <span class="built_in">sum</span>(t.item() <span class="keyword">for</span> t <span class="keyword">in</span> gathered_times)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\n并行计算耗时: <span class="subst">&#123;elapsed:<span class="number">.2</span>f&#125;</span> 秒&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;所有 GPU 总共耗时: <span class="subst">&#123;total_time:<span class="number">.2</span>f&#125;</span> 秒&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> rank, time_val <span class="keyword">in</span> <span class="built_in">enumerate</span>(gathered_times):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Rank <span class="subst">&#123;rank&#125;</span> 耗时: <span class="subst">&#123;time_val.item():<span class="number">.2</span>f&#125;</span> 秒&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\n总共实际耗时: <span class="subst">&#123;elapsed:<span class="number">.2</span>f&#125;</span> 秒&quot;</span>)</span><br><span class="line"></span><br><span class="line">    dist.destroy_process_group()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">&#x27;多GPU分布式图像处理测试&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 分布式训练相关参数</span></span><br><span class="line">    <span class="comment"># parser.add_argument(&#x27;--local_rank&#x27;, type=int, default=-1,</span></span><br><span class="line">    <span class="comment">#                     help=&#x27;DDP参数，由torch.distributed.launch自动传入&#x27;)</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--n_gpus&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;使用的GPU数量&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 数据加载相关参数</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;每个GPU的batch size&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--num_workers&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">4</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;数据加载的worker数量&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练相关参数</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;运行的epoch数量&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--iterations&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">8000</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;每张图片的处理迭代次数&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 其他参数</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">42</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;随机种子&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    args.local_rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;LOCAL_RANK&quot;</span>])   <span class="comment"># 将环境变量中的 local_rank 添加到 args</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置随机种子</span></span><br><span class="line">    torch.manual_seed(args.seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed_all(args.seed)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.local_rank == <span class="number">0</span>:</span><br><span class="line">        install_tqdm()</span><br><span class="line">        system_info()</span><br><span class="line">        pytorch_cuda_info()</span><br><span class="line">        check_virtual_env()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.cuda.is_available():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;没有可用的 GPU，退出。&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.local_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;使用 GPU 数量: <span class="subst">&#123;args.n_gpus&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取所有图片路径</span></span><br><span class="line">    img_paths = glob.glob(<span class="string">&quot;*.jpg&quot;</span>) + glob.glob(<span class="string">&quot;*.png&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> img_paths:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;没有找到任何图片，请检查路径。&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">    args.img_paths = img_paths</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 运行计算测试</span></span><br><span class="line">    image_computation_test(args.local_rank, args)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">【使用方法】</span></span><br><span class="line"><span class="string">(淘汰)</span></span><br><span class="line"><span class="string">python -m torch.distributed.launch \</span></span><br><span class="line"><span class="string">    --nproc_per_node=4 \</span></span><br><span class="line"><span class="string">    --master_port=29500 \</span></span><br><span class="line"><span class="string">    script_name.py \</span></span><br><span class="line"><span class="string">    --n_gpus=4 \</span></span><br><span class="line"><span class="string">    --batch_size=32 \</span></span><br><span class="line"><span class="string">    --epochs=10 \</span></span><br><span class="line"><span class="string">    --num_workers=4 \</span></span><br><span class="line"><span class="string">    --iterations=8000</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torchrun \</span></span><br><span class="line"><span class="string">    --nproc_per_node=4 \</span></span><br><span class="line"><span class="string">    --master_port=29500 \</span></span><br><span class="line"><span class="string">    script_name.py \</span></span><br><span class="line"><span class="string">    --n_gpus=4 \</span></span><br><span class="line"><span class="string">    --batch_size=32 \</span></span><br><span class="line"><span class="string">    --epochs=10 \</span></span><br><span class="line"><span class="string">    --num_workers=4 \</span></span><br><span class="line"><span class="string">    --iterations=8000</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">实际上发生的是：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">整个脚本会被启动4次（假设使用4个GPU）</span></span><br><span class="line"><span class="string">每次启动都是完整的脚本</span></span><br><span class="line"><span class="string">每个进程都有不同的 local_rank 值（0,1,2,3）</span></span><br><span class="line"><span class="string">每个进程被分配到不同的 GPU</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<p><code>start_ddp.sh</code> 平时运行它、修改它就好了。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------------------------</span></span><br><span class="line"><span class="comment"># 分布式训练启动脚本</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># 当使用 torch.distributed.launch 启动时：</span></span><br><span class="line"><span class="comment"># 1. 这个命令会自动启动n个进程(n=nproc_per_node指定的数量)</span></span><br><span class="line"><span class="comment"># 2. 每个进程都会运行同一个Python脚本(multi_gpu_process.py)</span></span><br><span class="line"><span class="comment"># 3. 每个进程会被自动分配一个local_rank(0,1,2,3...)</span></span><br><span class="line"><span class="comment"># 4. 每个进程会被自动分配到对应的GPU上</span></span><br><span class="line"><span class="comment"># ---------------------------------------------</span></span><br><span class="line"></span><br><span class="line">torchrun \</span><br><span class="line">   --nproc_per_node=2 \</span><br><span class="line">   --master_port=29501 \</span><br><span class="line">   ddp5_env_check.py \</span><br><span class="line">   --n_gpus=2 \</span><br><span class="line">   --batch_size=32 \</span><br><span class="line">   --epochs=10 \</span><br><span class="line">   --num_workers=4 \</span><br><span class="line">   --iterations=800</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------------------------</span></span><br><span class="line"><span class="comment"># 参数说明：</span></span><br><span class="line"><span class="comment"># --nproc_per_node: 要启动的进程数，通常等于GPU数量</span></span><br><span class="line"><span class="comment"># --master_port: 主进程的通信端口</span></span><br><span class="line"><span class="comment"># --n_gpus: 传递给Python脚本的GPU数量参数</span></span><br><span class="line"><span class="comment"># --batch_size: 每个GPU的批处理大小</span></span><br><span class="line"><span class="comment"># --epochs: 训练轮数</span></span><br><span class="line"><span class="comment"># --num_workers: 数据加载的工作进程数</span></span><br><span class="line"><span class="comment"># --iterations: 每张图片的处理迭代次数</span></span><br><span class="line"><span class="comment"># ---------------------------------------------</span></span><br></pre></td></tr></table></figure>

<p> 以上 例子 需要说明的是 ddp 方式 来做 多进程的 GPU 操作，是不存在主进程的，实际是 用 rank0 所在的进程兼职了主进程，它一边运行着自己的GPU训练一边兼职了主进程的活。所以 我们上面写的 统计 总共用时的写法 是不准确的，事实上也只会 体现出 rank0的 运行时间，但是 我没有想看那么精确 也就没有改了。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/torch/" rel="tag"># torch</a>
              <a href="/tags/GPU/" rel="tag"># GPU</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/10/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/GPU%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BD%AF%E4%BB%B6%E7%AC%94%E8%AE%B0/" rel="prev" title="GPU服务器软件笔记">
                  <i class="fa fa-chevron-left"></i> GPU服务器软件笔记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/12/16/windows_ubuntu%E5%8F%8C%E7%B3%BB%E7%BB%9F/Ubuntu%E8%A3%85clash/" rel="next" title="Ubuntu装clash">
                  Ubuntu装clash <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">陈宇韶chenyushao</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  





</body>
</html>
