<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.13.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="这是文章开头，显示在主页面，详情请点击此处。">
<meta property="og:type" content="article">
<meta property="og:title" content="spark_core">
<meta property="og:url" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/index.html">
<meta property="og:site_name" content="Tiger_pop&#39;s Blog">
<meta property="og:description" content="这是文章开头，显示在主页面，详情请点击此处。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.20.53-7050493.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.22.37.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.25.54.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.28.24.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.35.02.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.36.58.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.46.10.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.48.45.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2020.41.47.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2020.45.40.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2020.53.54.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2021.02.41.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.26.55.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.28.23.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.29.03.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.36.49.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.49.25.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2015.34.32.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.55.52.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.59.30.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2016.01.31.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2016.03.41.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2011.28.49.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2011.40.26.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2017.20.39.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2017.35.00.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2017.38.41.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2017.49.30.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2017.58.32.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.14.04.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.15.25.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.19.23.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.23.47.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.30.15.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.38.00.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.41.12.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2021.23.42.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2021.27.06.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2021.29.29.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2021.35.08.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2021.36.58.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2021.42.59.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2022.30.00.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2022.36.13.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.30.51.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.43.24.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.45.36.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.50.43.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.55.46.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.56.58.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.57.20.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.58.46.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.03.41.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.13.34.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.14.48.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.21.04.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.31.54.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.26.11.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.32.50.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2018.27.27.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2018.39.51.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.05.16.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.28.02.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.41.32.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.43.49.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.44.55.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.48.47.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.50.05.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.51.54.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.54.44.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2021.52.11.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2022.56.34.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2022.58.46.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2023.46.32.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2023.48.24.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2023.51.39.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2023.53.32.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2023.57.15.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-09%2000.11.58.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-09%2000.14.10.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-09%2016.10.48.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-09%2016.15.27.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-09%2016.16.51.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-09%2020.07.39.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2014.26.53.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2019.10.10.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2019.47.16.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2019.57.16.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2022.51.45.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2022.45.47.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2022.54.01.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2022.55.38.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2023.19.55.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-11%2016.29.54.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-11%2016.34.28.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-11%2017.17.35.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-11%2019.25.18.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-11%2023.03.07.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2012.12.56.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2012.19.50.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2012.26.17.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2012.28.25.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2012.29.03.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2012.30.46.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2015.43.06.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2015.55.35.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.01.16.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.04.08.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.09.57.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.36.06.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.37.22.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.38.15.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.42.51.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2017.56.45.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2015.43.06.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2018.10.54.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2018.17.43.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2021.54.34.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2021.57.06.jpg">
<meta property="og:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2021.58.32.jpg">
<meta property="article:published_time" content="2022-10-29T14:40:55.000Z">
<meta property="article:modified_time" content="2022-10-29T14:53:47.425Z">
<meta property="article:author" content="陈宇韶chenyushao">
<meta property="article:tag" content="生命在于运动">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.20.53-7050493.jpg">


<link rel="canonical" href="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/","path":"2022/10/29/spark学习/spark-core/","title":"spark_core"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>spark_core | Tiger_pop's Blog</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Tiger_pop's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Tiger_pop's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">tiger_pop 的博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%80%E4%BB%8BRDD"><span class="nav-number">1.</span> <span class="nav-text">简介RDD</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RDD%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8"><span class="nav-number">2.</span> <span class="nav-text">RDD编程入门</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E7%9A%84%E5%88%9B%E5%BB%BA"><span class="nav-number">2.1.</span> <span class="nav-text">RDD的创建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E7%AE%97%E5%AD%90"><span class="nav-number">2.2.</span> <span class="nav-text">RDD算子</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B"><span class="nav-number">2.2.1.</span> <span class="nav-text">案例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8Action%E7%AE%97%E5%AD%90"><span class="nav-number">2.3.</span> <span class="nav-text">常用Action算子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E6%93%8D%E4%BD%9C%E7%AE%97%E5%AD%90"><span class="nav-number">2.4.</span> <span class="nav-text">分区操作算子</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RDD%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">3.</span> <span class="nav-text">RDD的持久化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%A5%E5%85%85%EF%BC%9A%E6%9F%A5%E7%9C%8B%E6%BA%90%E7%A0%81%E5%B0%8F%E6%8A%80%E5%B7%A7%EF%BC%9A"><span class="nav-number">3.0.1.</span> <span class="nav-text">补充：查看源码小技巧：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E7%BC%93%E5%AD%98"><span class="nav-number">3.1.</span> <span class="nav-text">RDD缓存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-checkpoint"><span class="nav-number">3.2.</span> <span class="nav-text">RDD_checkpoint</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">3.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B%E7%BB%83%E4%B9%A0"><span class="nav-number">4.</span> <span class="nav-text">案例练习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9C%80%E6%B1%821%EF%BC%9A%E7%BB%9F%E8%AE%A1%E6%90%9C%E7%B4%A2%E8%AF%8D%E6%8E%92%E5%90%8D%E5%89%8D%E4%BA%94%E7%9A%84%E5%85%B3%E9%94%AE%E8%AF%8D%E3%80%82"><span class="nav-number">4.1.</span> <span class="nav-text">需求1：统计搜索词排名前五的关键词。</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9C%80%E6%B1%822-%E7%94%A8%E6%88%B7%E5%92%8C%E5%85%B3%E9%94%AE%E8%AF%8D%E7%BB%84%E5%90%88%E5%88%86%E6%9E%90"><span class="nav-number">4.2.</span> <span class="nav-text">需求2: 用户和关键词组合分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9C%80%E6%B1%823-%E7%83%AD%E9%97%A8%E6%90%9C%E7%B4%A2%E6%97%B6%E9%97%B4%E6%AE%B5%E5%88%86%E6%9E%90"><span class="nav-number">4.3.</span> <span class="nav-text">需求3: 热门搜索时间段分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E4%B8%AD%E8%BF%90%E8%A1%8C%E4%B8%8A%E8%BF%B0%E4%B8%89%E4%B8%AA%E9%9C%80%E6%B1%82"><span class="nav-number">4.4.</span> <span class="nav-text">集群中运行上述三个需求</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%8B%E6%A6%A8%E9%9B%86%E7%BE%A4%E7%9A%84%E6%80%A7%E8%83%BD-%E6%8F%90%E4%BA%A4"><span class="nav-number">4.4.1.</span> <span class="nav-text">压榨集群的性能 提交</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F"><span class="nav-number">5.</span> <span class="nav-text">共享变量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="nav-number">5.1.</span> <span class="nav-text">广播变量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">5.2.</span> <span class="nav-text">累加器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%80%81%E7%89%88%E6%9C%ACspark%E7%B4%AF%E5%8A%A0%E5%99%A8-%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%88%E6%96%B0%E7%89%88%E6%9C%AC%E5%A5%BD%E5%83%8F%E5%B7%B2%E7%BB%8F%E8%A7%A3%E5%86%B3%E4%BA%86%EF%BC%89%EF%BC%9A"><span class="nav-number">5.2.1.</span> <span class="nav-text">老版本spark累加器 需要注意的问题（新版本好像已经解决了）：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B"><span class="nav-number">5.3.</span> <span class="nav-text">综合案例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89"><span class="nav-number">6.</span> <span class="nav-text">内核调度（重要）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#DAG-%E6%9C%89%E5%90%91%E6%97%A0%E7%8E%AF%E5%9B%BE"><span class="nav-number">6.1.</span> <span class="nav-text">DAG 有向无环图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%BD%E7%AA%84%E4%BE%9D%E8%B5%96%E5%92%8C%E9%98%B6%E6%AE%B5%E5%88%92%E5%88%86"><span class="nav-number">6.2.</span> <span class="nav-text">宽窄依赖和阶段划分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97"><span class="nav-number">6.3.</span> <span class="nav-text">内存迭代计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark%E5%B9%B6%E8%A1%8C%E5%BA%A6"><span class="nav-number">6.4.</span> <span class="nav-text">spark并行度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6"><span class="nav-number">6.5.</span> <span class="nav-text">spark任务调度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark%E5%B8%B8%E7%94%A8%E8%AF%8D%E6%B1%87"><span class="nav-number">6.6.</span> <span class="nav-text">spark常用词汇</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="陈宇韶chenyushao"
      src="/images/my.jpg">
  <p class="site-author-name" itemprop="name">陈宇韶chenyushao</p>
  <div class="site-description" itemprop="description">爱学习、爱工作、爱生活;         微信号: Tiger_and_master;         手机号码:18515678348 </div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">427</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">201</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my.jpg">
      <meta itemprop="name" content="陈宇韶chenyushao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tiger_pop's Blog">
      <meta itemprop="description" content="爱学习、爱工作、爱生活;         微信号: Tiger_and_master;         手机号码:18515678348 ">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="spark_core | Tiger_pop's Blog">
      <meta itemprop="description" content="这是文章开头，显示在主页面，详情请点击此处。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          spark_core
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-10-29 22:40:55 / 修改时间：22:53:47" itemprop="dateCreated datePublished" datetime="2022-10-29T22:40:55+08:00">2022-10-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/spark/" itemprop="url" rel="index"><span itemprop="name">spark</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">这是文章开头，显示在主页面，详情请点击此处。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>简介 <span id="more"></span></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.20.53-7050493.jpg" alt="截屏2022-04-04 15.20.53"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.22.37.jpg" alt="截屏2022-04-04 15.22.37"></p>
<h1 id="简介RDD"><a href="#简介RDD" class="headerlink" title="简介RDD"></a>简介RDD</h1><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.25.54.jpg" alt="截屏2022-04-04 15.25.54"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.28.24.jpg" alt="截屏2022-04-04 15.28.24"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.35.02.jpg" alt="截屏2022-04-04 15.35.02"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.36.58.jpg" alt="截屏2022-04-04 15.36.58"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.46.10.jpg" alt="截屏2022-04-04 15.46.10"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2015.48.45.jpg" alt="截屏2022-04-04 15.48.45"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2020.41.47.jpg" alt="截屏2022-04-04 20.41.47"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2020.45.40.jpg" alt="截屏2022-04-04 20.45.40"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf ,SparkContext</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;wordcounthelloworld&quot;</span>)</span><br><span class="line">    <span class="comment"># local model</span></span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    file_rdd = sc.textFile(<span class="string">&quot;hdfs://node01:9000/tmp/words&quot;</span>) <span class="comment"># local is also ok</span></span><br><span class="line">    words_rdd = file_rdd.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    words_with_one_rdd = words_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">    result_rdd = words_with_one_rdd.reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(result_rdd.collect())</span><br></pre></td></tr></table></figure>

<p>​		以上代码分析如下。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2020.53.54.jpg" alt="截屏2022-04-04 20.53.54"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-04%2021.02.41.jpg" alt="截屏2022-04-04 21.02.41"></p>
<h1 id="RDD编程入门"><a href="#RDD编程入门" class="headerlink" title="RDD编程入门"></a>RDD编程入门</h1><h2 id="RDD的创建"><a href="#RDD的创建" class="headerlink" title="RDD的创建"></a>RDD的创建</h2><p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.26.55.jpg" alt="截屏2022-04-05 11.26.55"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.28.23.jpg" alt="截屏2022-04-05 11.28.23"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.29.03.jpg" alt="截屏2022-04-05 11.29.03"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.36.49.jpg" alt="截屏2022-04-05 11.36.49"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.49.25.jpg" alt="截屏2022-04-05 11.49.25"></p>
<p>​		我这里的地址默认就是从hdfs 的位置去找了，即便是不写hdfs:&#x2F;&#x2F;node01:9000 也是从这里去找而不是本地，代码写了想最少分3组，但是实际上还是科学的分了4组，可见参数2不是强制的。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2015.34.32.jpg" alt="截屏2022-04-05 15.34.32"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.55.52.jpg" alt="截屏2022-04-05 11.55.52"></p>
<p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2011.59.30.jpg" alt="截屏2022-04-05 11.59.30"></p>
<p>​		顺带提一下，python中的<code>map(lambda x: x[1]，迭代器)</code> x 是迭代器中的每一个元素，此元素作为lambda函数的参数进入lambda函数，一个一个输出，在python中用<code>list（）</code>函数收集成list ，rdd 中，每个分组都会计算一次<code>map（）</code>函数，用<code>collect()</code>函数在每个组中收集并最终汇总到一个list中。</p>
<h2 id="RDD算子"><a href="#RDD算子" class="headerlink" title="RDD算子"></a>RDD算子</h2><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2016.01.31.jpg" alt="截屏2022-04-05 16.01.31"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-05%2016.03.41.jpg" alt="截屏2022-04-05 16.03.41"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2011.28.49.jpg" alt="截屏2022-04-06 11.28.49"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2011.40.26.jpg" alt="截屏2022-04-06 11.40.26"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2017.20.39.jpg" alt="截屏2022-04-06 17.20.39"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2017.35.00.jpg" alt="截屏2022-04-06 17.35.00"></p>
<p>​		flatMap 就是内部fun函数处理完毕之后再解除嵌套，如果仅仅是想解除嵌套不处理可以 把内部的fun 写成 <code>lambda x:x </code> 不做处理只解除嵌套。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2017.38.41.jpg" alt="截屏2022-04-06 17.38.41"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2017.49.30.jpg" alt="截屏2022-04-06 17.49.30"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2017.58.32.jpg" alt="截屏2022-04-06 17.58.32"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.14.04.jpg" alt="截屏2022-04-06 18.14.04"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.15.25.jpg" alt="截屏2022-04-06 18.15.25"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.19.23.jpg" alt="截屏2022-04-06 18.19.23"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.23.47.jpg" alt="截屏2022-04-06 18.23.47"></p>
<p>union算子不会去重，而且元素是不同类型的rdd一样可以被union到一起。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.30.15.jpg" alt="截屏2022-04-06 18.30.15"></p>
<p>​		join的条件是 按照二元元组的  key 值来连接。</p>
<pre><code>     ![截屏2022-04-06 18.34.26](/Users/chenyushao/Library/Application Support/typora-user-images/截屏2022-04-06 18.34.26.jpg)
</code></pre>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.38.00.jpg" alt="截屏2022-04-06 18.38.00"></p>
<p>​		注意intersection相交和join内连接的不同，join 是根据key，intersection完全一样才保留。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2018.41.12.jpg" alt="截屏2022-04-06 18.41.12"></p>
<p>​		方便查看数据分区情况，可以用flatmap（lambda x：x）来解除嵌套。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2021.23.42.jpg" alt="截屏2022-04-06 21.23.42"></p>
<p>groupbykey 和groupby 的区别就是 前者仅仅保留 value，后者保留了元素的全部。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2021.27.06.jpg" alt="截屏2022-04-06 21.27.06"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2021.29.29.jpg" alt="截屏2022-04-06 21.29.29"></p>
<p>注意：排序是在executor内完成的，sortby仅仅能保证在一个executor内有序。如果想要结果全局有序，需要定义numpartitions&#x3D;1，上面这种numpartitions不等于1也全局有序了，是因为用的是local模式在一个executor上运行的。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2021.35.08.jpg" alt="截屏2022-04-06 21.35.08"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2021.36.58.jpg" alt="截屏2022-04-06 21.36.58"></p>
<p>注意：其中对小写的lower（）转换，只是改变了排序过程中的值，最终结果还是按照原数据排序。</p>
<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2021.42.59.jpg" alt="截屏2022-04-06 21.42.59"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">conding:utf8</span></span><br><span class="line">from pyspark import SparkConf,SparkContext</span><br><span class="line">import json  # 字符串里面是字典，就是json格式。</span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    conf = SparkConf().setAppName(&quot;creat_test&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.textFile(&#x27;/spark_study/order.text&#x27;) # 这里是hdfs 中的路径，之前已经 Hadoop fs -put 上了文件。</span><br><span class="line">    # print(rdd.collect())</span><br><span class="line">    json_rdd = rdd.flatMap(lambda jsons: jsons.split(&#x27;|&#x27;))</span><br><span class="line">    print(&#x27;json_rdd_is: %s&#x27;%json_rdd.collect())</span><br><span class="line">    dict_rdd = json_rdd.map(lambda x: json.loads(x))</span><br><span class="line">    print(&#x27;dict_rdd_is: %s&#x27;%dict_rdd.collect())</span><br><span class="line">    bejing_rdd = dict_rdd.filter(lambda x: x[&#x27;areaName&#x27;]==&#x27;北京&#x27;)</span><br><span class="line">    print(&#x27;bejing_rdd is %s&#x27;%bejing_rdd.collect())</span><br><span class="line">    bejing_distinct_category_rdd = bejing_rdd.map(lambda x: x[&#x27;areaName&#x27;]+&#x27;-&#x27;+x[&#x27;category&#x27;]).distinct()</span><br><span class="line">    print(&#x27;bejing_distinct_category is \n%s&#x27;%bejing_distinct_category_rdd.collect())</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">结果</span></span><br><span class="line">bejing_distinct_category is </span><br><span class="line">[&#x27;北京-平板电脑&#x27;, &#x27;北京-家电&#x27;, &#x27;北京-电脑&#x27;, &#x27;北京-书籍&#x27;, &#x27;北京-服饰&#x27;, &#x27;北京-手机&#x27;, &#x27;北京-家具&#x27;, &#x27;北京-食品&#x27;]</span><br></pre></td></tr></table></figure>

<p>spark集群运算时，别的机器找不到本地的依赖方法，</p>
<p>​		1、要记得用<code>conf.set()</code>来让hdfs 能够找到依赖。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2022.30.00.jpg" alt="截屏2022-04-06 22.30.00"></p>
<p>​		2、若用spark-submit 工具直接提交，要用<code>--py-file 依赖文件路径</code> 让hdfs找到依赖哦。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-06%2022.36.13.jpg" alt="截屏2022-04-06 22.36.13"></p>
<p>用spark集群跑刚才的代码（先把其中的一个方法独立到这个脚本外面，做成一个依赖）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">vim defs_areaName_add_category.py</span><br><span class="line">def areaName_add_category(x):</span><br><span class="line">    return x[&#x27;areaName&#x27;]+&#x27;-&#x27;+x[&#x27;category&#x27;]</span><br><span class="line"></span><br><span class="line">vim rdd_operator_demo_runYarn.py</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">conding:utf8</span></span><br><span class="line">from pyspark import SparkConf,SparkContext</span><br><span class="line">import json</span><br><span class="line">from defs_areaName_add_category import areaName_add_category</span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    conf = SparkConf().setAppName(&quot;creat_test&quot;) # 命令窗口指定master，这里就不写setMster了。</span><br><span class="line">    # 指出依赖。</span><br><span class="line">    conf.set(&#x27;spark.submit.pyFiles&#x27;,&#x27;defs_areaName_add_category.py&#x27;)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.textFile(&#x27;/spark_study/order.text&#x27;)</span><br><span class="line">    json_rdd = rdd.flatMap(lambda jsons: jsons.split(&#x27;|&#x27;))</span><br><span class="line">    dict_rdd = json_rdd.map(lambda x: json.loads(x))</span><br><span class="line">    bejing_rdd = dict_rdd.filter(lambda x: x[&#x27;areaName&#x27;]==&#x27;北京&#x27;)</span><br><span class="line">    # 用依赖的方法 代替 原来的方法。考察集群内其他机器能不能找到依赖。</span><br><span class="line">    bejing_distinct_category_rdd = bejing_rdd.map(areaName_add_category).distinct()</span><br><span class="line">    print(&#x27;bejing_distinct_category is \n%s&#x27;%bejing_distinct_category_rdd.collect())</span><br></pre></td></tr></table></figure>

<p>​		在编辑py文件的位置打开命令窗口运行</p>
<p><code>/opt/spark/spark/bin/spark-submit --master yarn --py-files ./defs_areaName_add_category.py  ./rdd_operator_demo_runYarn.py</code></p>
<p>​		结果如下(有交互式反馈，是客户端模式运行)：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bejing_distinct_category is </span><br><span class="line">[&#x27;北京-平板电脑&#x27;, &#x27;北京-家电&#x27;, &#x27;北京-电脑&#x27;, &#x27;北京-书籍&#x27;, &#x27;北京-服饰&#x27;, &#x27;北京-食品&#x27;, &#x27;北京-手机&#x27;, &#x27;北京-家具&#x27;]</span><br></pre></td></tr></table></figure>



<h2 id="常用Action算子"><a href="#常用Action算子" class="headerlink" title="常用Action算子"></a>常用Action算子</h2><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.30.51.jpg" alt="截屏2022-04-08 14.30.51"></p>
<p>统计key的出现次数，和value没关系，这里value设置成2也是一样的。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.43.24.jpg" alt="截屏2022-04-08 14.43.24"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.45.36.jpg" alt="截屏2022-04-08 14.45.36"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.50.43.jpg" alt="截屏2022-04-08 14.50.43"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.55.46.jpg" alt="截屏2022-04-08 14.55.46"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.56.58.jpg" alt="截屏2022-04-08 14.56.58"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.57.20.jpg" alt="截屏2022-04-08 14.57.20"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2014.58.46.jpg" alt="截屏2022-04-08 14.58.46"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.03.41.jpg" alt="截屏2022-04-08 15.03.41"></p>
<p>种子参数如果不变，随机出来的数据会是一样的，所以我们尽量不要写种子参数。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.13.34.jpg" alt="截屏2022-04-08 15.13.34"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.14.48.jpg" alt="截屏2022-04-08 15.14.48"></p>
<p>​		注意：排序完了以后，输出还是原来这个位置的元素，那个lambda 匿名函数只是改变排序规则而已。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.21.04.jpg" alt="截屏2022-04-08 15.21.04"></p>
<p>​		注意 <code>foreach</code>是没有返回值的，而且foreach 是executor里直接输出的，不像collect那些action算子 一样汇总到driver中输出。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.31.54.jpg" alt="截屏2022-04-08 15.31.54"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.26.11.jpg" alt="截屏2022-04-08 15.26.11"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2015.32.50.jpg" alt="截屏2022-04-08 15.32.50"></p>
<h2 id="分区操作算子"><a href="#分区操作算子" class="headerlink" title="分区操作算子"></a>分区操作算子</h2><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2018.27.27.jpg" alt="截屏2022-04-08 18.27.27"></p>
<p>​		和map一次处理一个元素不同的是，mapPartion算子是整个迭代器传递。走网络的时候，一次性传输一个迭代器对象 效率一定高于一次传一个元素。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">conding:utf8</span></span><br><span class="line">from pyspark import SparkConf,SparkContext</span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    conf = SparkConf().setAppName(&quot;creat_test&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([1,3,2,5,4,3],3)</span><br><span class="line"></span><br><span class="line">    def process(iter):</span><br><span class="line">        result = []</span><br><span class="line">        for it in iter:</span><br><span class="line">            result.append(it*10)</span><br><span class="line">        return result</span><br><span class="line"></span><br><span class="line">    result = rdd.mapPartitions(process).collect()</span><br><span class="line">    print(result)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">output</span></span><br><span class="line">[10, 30, 20, 50, 40, 30]</span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2018.39.51.jpg" alt="截屏2022-04-08 18.39.51"></p>
<p>foreachPartion 除了是一个没有返回值的mapPartitions 以外，还是一个action算子，所以不用收集。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.05.16.jpg" alt="截屏2022-04-08 19.05.16"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">conding:utf8</span></span><br><span class="line">from pyspark import SparkConf,SparkContext</span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    conf = SparkConf().setAppName(&quot;creat_test&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([(&#x27;hadoop&#x27;,1),(&#x27;hadoop&#x27;,2),(&#x27;spark&#x27;,1),(&#x27;hello&#x27;,1),(&#x27;flink&#x27;,1)],4)</span><br><span class="line">    print(rdd.glom().collect())    # 用默认分区来对比一下。</span><br><span class="line"></span><br><span class="line">    def process(k):</span><br><span class="line">        if k == &#x27;hadoop&#x27; or k == &#x27;spark&#x27;: return 0</span><br><span class="line">        if k == &#x27;flink&#x27;: return 1</span><br><span class="line">        return 2</span><br><span class="line"></span><br><span class="line">    result = rdd.partitionBy(3,process).glom().collect()</span><br><span class="line">    print(result)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">output</span></span><br><span class="line">[[(&#x27;hadoop&#x27;, 1)], [(&#x27;hadoop&#x27;, 2)], [(&#x27;spark&#x27;, 1)], [(&#x27;hello&#x27;, 1), (&#x27;flink&#x27;, 1)]]</span><br><span class="line">[[(&#x27;hadoop&#x27;, 1), (&#x27;hadoop&#x27;, 2), (&#x27;spark&#x27;, 1)], [(&#x27;flink&#x27;, 1)], [(&#x27;hello&#x27;, 1)]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.28.02.jpg" alt="截屏2022-04-08 19.28.02"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">conding:utf8</span></span><br><span class="line">from pyspark import SparkConf,SparkContext</span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    conf = SparkConf().setAppName(&quot;creat_test&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([1,2,3,4,5],3)</span><br><span class="line">    print(rdd.repartition(1).getNumPartitions())</span><br><span class="line">    print(rdd.repartition(4).getNumPartitions())</span><br><span class="line"></span><br><span class="line">    print(rdd.coalesce(1).getNumPartitions())</span><br><span class="line">    print(rdd.coalesce(10,shuffle=True).getNumPartitions()) # coalesce 和repartition 功能一样，但是coalesce有一个shuffle 开关，在分区增加时 要手动打开。 </span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.41.32.jpg" alt="截屏2022-04-08 19.41.32"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.43.49.jpg" alt="截屏2022-04-08 19.43.49"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.44.55.jpg" alt="截屏2022-04-08 19.44.55"></p>
<p>网络io 来看，groupbykey 开销太大了，而reducebykey rdd内先聚合过了，所以开销小很多。</p>
<p>答题：从 功能方面看；从 性能方面看。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.48.47.jpg" alt="截屏2022-04-08 19.48.47"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.50.05.jpg" alt="截屏2022-04-08 19.50.05"></p>
<h1 id="RDD的持久化"><a href="#RDD的持久化" class="headerlink" title="RDD的持久化"></a>RDD的持久化</h1><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.51.54.jpg" alt="截屏2022-04-08 19.51.54"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2019.54.44.jpg" alt="截屏2022-04-08 19.54.44"></p>
<p>一次到collect（），只会有一个rdd，找不到想要的rdd就又要重新运行一遍，花销很大。</p>
<p>”持久化“ 就是解决这个问题。</p>
<h3 id="补充：查看源码小技巧："><a href="#补充：查看源码小技巧：" class="headerlink" title="补充：查看源码小技巧："></a>补充：查看源码小技巧：</h3><p>​		在idea中快速查看源码，比如一个要引入的方法我们不知道它需要哪个包，怎么用等等，可以先   <code>ctrl+单击鼠标左键点方法名</code>   快速查看方法的源码，Mac上需要   <code>ctrl+command+单击方法名</code>    快速查看方法源码。</p>
<p>​		打开源码后，看看方法需要的参数，在源码文件夹搜索参数名，往上找 import 可以看到这个方法想用此参数需要导入什么包。回到我们的代码中，import 需要导入的包名就行了，此时方法内可以正常使用该参数了。</p>
<h2 id="RDD缓存"><a href="#RDD缓存" class="headerlink" title="RDD缓存"></a>RDD缓存</h2><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2021.52.11.jpg" alt="截屏2022-04-08 21.52.11"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">conding:utf8</span></span><br><span class="line">from pyspark import SparkConf,SparkContext</span><br><span class="line">import time</span><br><span class="line">from pyspark.storagelevel import StorageLevel  # 通过上面查看源码小技巧从 persis里找要导入什么包。</span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    conf = SparkConf().setAppName(&quot;creat_test&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd1 = sc.textFile(&#x27;/spark_study/words.txt&#x27;)</span><br><span class="line">    rdd2 = rdd1.flatMap(lambda x: x.split(&#x27; &#x27;))</span><br><span class="line">    rdd3 = rdd2.map(lambda x: (x,1))</span><br><span class="line"></span><br><span class="line">    rdd3.cache()                           # 用cache方法缓存                  </span><br><span class="line">    rdd3.persist(StorageLevel.MEMORY_ONLY) # 用persist持久化缓存。</span><br><span class="line"></span><br><span class="line">    rdd4 = rdd3.reduceByKey(lambda a,b: a+b)</span><br><span class="line">    print(rdd4.collect())</span><br><span class="line"></span><br><span class="line">    rdd5 = rdd3.groupByKey()</span><br><span class="line">    rdd6 = rdd5.mapValues(lambda x:sum(x))</span><br><span class="line">    print(rdd6.collect())</span><br><span class="line"></span><br><span class="line">    rdd3.unpersist()                       # 记得释放缓存 </span><br><span class="line">    time.sleep(10000)                      # 故意让进程睡眠，方便4040端口实时查看</span><br></pre></td></tr></table></figure>

<p>​		程序运行中，可以从 node01:4040 端口查看spark的运行情况。可见有两个任务在运行，一个是  rdd1-&gt;rdd2-&gt;rdd3-&gt;rdd4;  另一个是 rdd1-&gt;rdd2-&gt;rdd3-&gt;rdd5-&gt;rdd6。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2022.56.34.jpg" alt="截屏2022-04-08 22.56.34"></p>
<p>​		点击进任意一个任务，再点击  DAG Visualization  可视化，可见一个绿色的小点，说明任务在运行时，并非每次都从头运行一遍，而是从这个绿点缓存位置再向下运行的。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2022.58.46.jpg" alt="截屏2022-04-08 22.58.46"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2023.46.32.jpg" alt="截屏2022-04-08 23.46.32"></p>
<p>缓存仅仅在逻辑上是一个整体，具体物理存储实际上是分散存储的。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2023.48.24.jpg" alt="截屏2022-04-08 23.48.24"></p>
<h2 id="RDD-checkpoint"><a href="#RDD-checkpoint" class="headerlink" title="RDD_checkpoint"></a>RDD_checkpoint</h2><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2023.51.39.jpg" alt="截屏2022-04-08 23.51.39"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2023.53.32.jpg" alt="截屏2022-04-08 23.53.32"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-08%2023.57.15.jpg" alt="截屏2022-04-08 23.57.15"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">conding:utf8</span></span><br><span class="line">from pyspark import SparkConf,SparkContext</span><br><span class="line">import time</span><br><span class="line">from pyspark.storagelevel import StorageLevel</span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    conf = SparkConf().setAppName(&quot;creat_test&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    sc.setCheckpointDir(&#x27;/spark_study/checkpoint_log&#x27;) #指定checkpoint存储位置</span><br><span class="line"></span><br><span class="line">    rdd1 = sc.textFile(&#x27;/spark_study/words.txt&#x27;)</span><br><span class="line">    rdd2 = rdd1.flatMap(lambda x: x.split(&#x27; &#x27;))</span><br><span class="line">    rdd3 = rdd2.map(lambda x: (x,1))</span><br><span class="line"></span><br><span class="line">    # rdd3.cache()</span><br><span class="line">    # rdd3.persist(StorageLevel.MEMORY_ONLY)</span><br><span class="line">    rdd3.checkpoint()  # 从这里 checkpoint。</span><br><span class="line"></span><br><span class="line">    rdd4 = rdd3.reduceByKey(lambda a,b: a+b)</span><br><span class="line">    print(rdd4.collect())</span><br><span class="line"></span><br><span class="line">    rdd5 = rdd3.groupByKey()</span><br><span class="line">    rdd6 = rdd5.mapValues(lambda x:sum(x))</span><br><span class="line">    print(rdd6.collect())</span><br><span class="line"></span><br><span class="line">    rdd3.unpersist()</span><br><span class="line">    time.sleep(10000)</span><br></pre></td></tr></table></figure>

<p>​		我们可以从4040端口的可视化试图看见，没有绿色的cache点了，而且程序开头直接从checkpoint 开始运行。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-09%2000.11.58.jpg" alt="截屏2022-04-09 00.11.58"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-09%2000.14.10.jpg" alt="截屏2022-04-09 00.14.10"></p>
<h1 id="案例练习"><a href="#案例练习" class="headerlink" title="案例练习"></a>案例练习</h1><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-09%2016.10.48.jpg" alt="截屏2022-04-09 16.10.48"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-09%2016.15.27.jpg" alt="截屏2022-04-09 16.15.27"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-09%2016.16.51.jpg" alt="截屏2022-04-09 16.16.51"></p>
<p>​		先在 conda 的 pyspark虚拟环境 <code>pip install jieba</code>。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-09%2020.07.39.jpg" alt="截屏2022-04-09 20.07.39"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    content = <span class="string">&#x27;小明硕士毕业于中国科学院计算所，后在深造。&#x27;</span></span><br><span class="line"></span><br><span class="line">    result = jieba.cut(content,<span class="literal">True</span>) <span class="comment"># true 就是允许词汇的二次组合。</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">list</span>(result))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(result))  <span class="comment"># 可见jieba.cut返回的都是 生成器，用一次就把自己清空。</span></span><br><span class="line"></span><br><span class="line">    result2 = jieba.cut(content,<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">list</span>(result2))</span><br><span class="line"></span><br><span class="line">    result3 = jieba.cut_for_search(content) <span class="comment"># 等价于 jieba.cut(content,True)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;,&#x27;</span>.join(result3))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">list</span>(result3),<span class="built_in">type</span>(result3))		<span class="comment"># 前面result3用过一次，生成器清空了。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">[<span class="string">&#x27;小&#x27;</span>, <span class="string">&#x27;明&#x27;</span>, <span class="string">&#x27;硕士&#x27;</span>, <span class="string">&#x27;毕业&#x27;</span>, <span class="string">&#x27;于&#x27;</span>, <span class="string">&#x27;中国&#x27;</span>, <span class="string">&#x27;中国科学院&#x27;</span>, <span class="string">&#x27;科学&#x27;</span>, <span class="string">&#x27;科学院&#x27;</span>, <span class="string">&#x27;学院&#x27;</span>, <span class="string">&#x27;计算&#x27;</span>, <span class="string">&#x27;计算所&#x27;</span>, <span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;后&#x27;</span>, <span class="string">&#x27;在&#x27;</span>, <span class="string">&#x27;深造&#x27;</span>, <span class="string">&#x27;。&#x27;</span>]</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;generator&#x27;</span>&gt;</span><br><span class="line">[<span class="string">&#x27;小明&#x27;</span>, <span class="string">&#x27;硕士&#x27;</span>, <span class="string">&#x27;毕业&#x27;</span>, <span class="string">&#x27;于&#x27;</span>, <span class="string">&#x27;中国科学院&#x27;</span>, <span class="string">&#x27;计算所&#x27;</span>, <span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;后&#x27;</span>, <span class="string">&#x27;在&#x27;</span>, <span class="string">&#x27;深造&#x27;</span>, <span class="string">&#x27;。&#x27;</span>]</span><br><span class="line">小明,硕士,毕业,于,中国,科学,学院,科学院,中国科学院,计算,计算所,，,后,在,深造,。</span><br><span class="line">[] &lt;<span class="keyword">class</span> <span class="string">&#x27;generator&#x27;</span>&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>文件内容如下：</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2014.26.53.jpg" alt="截屏2022-04-10 14.26.53"></p>
<h2 id="需求1：统计搜索词排名前五的关键词。"><a href="#需求1：统计搜索词排名前五的关键词。" class="headerlink" title="需求1：统计搜索词排名前五的关键词。"></a>需求1：统计搜索词排名前五的关键词。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">vim log_analize.py</span><br><span class="line"><span class="comment"># conding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf,SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel</span><br><span class="line"><span class="keyword">from</span> defs <span class="keyword">import</span> context_jieba,filter_words,append_words</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;creat_test&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    file_rdd = sc.textFile(<span class="string">&#x27;/spark_study/SogouQ.txt&#x27;</span>)</span><br><span class="line">    split_rdd = file_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;\t&#x27;</span>))</span><br><span class="line">    <span class="comment"># print(split_rdd.collect())</span></span><br><span class="line">    split_rdd.persist(StorageLevel.DISK_ONLY)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需求一：</span></span><br><span class="line">    context_rdd = split_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">2</span>])</span><br><span class="line">    <span class="comment"># print(context_rdd.collect())</span></span><br><span class="line">    words_rdd = context_rdd.flatMap( context_jieba)</span><br><span class="line">    <span class="comment"># print(words_rdd.collect())</span></span><br><span class="line">    filtered_rdd = words_rdd.<span class="built_in">filter</span>( filter_words)</span><br><span class="line">    <span class="comment"># print(filtered_rdd.collect())</span></span><br><span class="line">    final_words_rdd = filtered_rdd.<span class="built_in">map</span>( append_words)</span><br><span class="line">    <span class="comment"># print(final_words_rdd.collect())</span></span><br><span class="line">    result = final_words_rdd.reduceByKey(<span class="keyword">lambda</span> a,b : a+b).\</span><br><span class="line">        sortBy(<span class="keyword">lambda</span> x: x[<span class="number">1</span>],ascending=<span class="literal">False</span>,numPartitions=<span class="number">1</span> ).take(<span class="number">5</span>)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">vim defs.py</span><br><span class="line"><span class="comment"># conding:utf8</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">context_jieba</span>(<span class="params">data</span>):</span><br><span class="line">    seg = jieba.cut_for_search(data)</span><br><span class="line">    l = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> seg:</span><br><span class="line">        l.append(word)</span><br><span class="line">    <span class="keyword">return</span> l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">filter_words</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="keyword">return</span> data <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;谷&#x27;</span>,<span class="string">&#x27;帮&#x27;</span>,<span class="string">&#x27;客&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">append_words</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="keyword">if</span> data == <span class="string">&#x27;传智播&#x27;</span>: data = <span class="string">&#x27;传智播客&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> data == <span class="string">&#x27;院校&#x27;</span> : data = <span class="string">&#x27;院校帮&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> data == <span class="string">&#x27;博学&#x27;</span>: data = <span class="string">&#x27;博学谷&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> (data,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">[(<span class="string">&#x27;scala&#x27;</span>, <span class="number">2310</span>), (<span class="string">&#x27;hadoop&#x27;</span>, <span class="number">2268</span>), (<span class="string">&#x27;博学谷&#x27;</span>, <span class="number">2002</span>), (<span class="string">&#x27;传智汇&#x27;</span>, <span class="number">1918</span>), (<span class="string">&#x27;itheima&#x27;</span>, <span class="number">1680</span>)]</span><br></pre></td></tr></table></figure>

<h2 id="需求2-用户和关键词组合分析"><a href="#需求2-用户和关键词组合分析" class="headerlink" title="需求2: 用户和关键词组合分析"></a>需求2: 用户和关键词组合分析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">vim log_analize.py</span><br><span class="line"><span class="comment"># conding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf,SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel</span><br><span class="line"><span class="keyword">from</span> defs <span class="keyword">import</span> context_jieba,filter_words,append_words</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;creat_test&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    file_rdd = sc.textFile(<span class="string">&#x27;/spark_study/SogouQ.txt&#x27;</span>)</span><br><span class="line">    split_rdd = file_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;\t&#x27;</span>))</span><br><span class="line">    <span class="comment"># print(split_rdd.collect())</span></span><br><span class="line">    split_rdd.persist(StorageLevel.DISK_ONLY)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需求二：</span></span><br><span class="line">		user_content_rdd = split_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x[<span class="number">1</span>],x[<span class="number">2</span>]))</span><br><span class="line">    <span class="comment"># print(user_content_rdd.collect())</span></span><br><span class="line">    user_word_with_one_rdd = user_content_rdd.flatMap( extract_user_and_word)</span><br><span class="line">    <span class="comment"># print(user_word_with_one_rdd.collect())</span></span><br><span class="line">    result2 = user_word_with_one_rdd.reduceByKey(<span class="keyword">lambda</span> a,b:a+b).\</span><br><span class="line">        sortBy(<span class="keyword">lambda</span> x:x[<span class="number">1</span>],ascending=<span class="literal">False</span>,numPartitions=<span class="number">1</span>).take(<span class="number">5</span>)</span><br><span class="line">    <span class="built_in">print</span>(result2)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">vim defs.py</span><br><span class="line"><span class="comment"># conding:utf8</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">context_jieba</span>(<span class="params">data</span>):</span><br><span class="line">    seg = jieba.cut_for_search(data)</span><br><span class="line">    l = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> seg:</span><br><span class="line">        l.append(word)</span><br><span class="line">    <span class="keyword">return</span> l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">filter_words</span>(<span class="params">data</span>):             <span class="comment"># return false or true</span></span><br><span class="line">    <span class="keyword">return</span> data <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;谷&#x27;</span>,<span class="string">&#x27;帮&#x27;</span>,<span class="string">&#x27;客&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">append_words</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="keyword">if</span> data == <span class="string">&#x27;传智播&#x27;</span>: data = <span class="string">&#x27;传智播客&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> data == <span class="string">&#x27;院校&#x27;</span> : data = <span class="string">&#x27;院校帮&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> data == <span class="string">&#x27;博学&#x27;</span>: data = <span class="string">&#x27;博学谷&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> (data,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_user_and_word</span>(<span class="params">data</span>):</span><br><span class="line">    userid = <span class="built_in">str</span>(data[<span class="number">0</span>])</span><br><span class="line">    word_all = <span class="built_in">str</span>(data[<span class="number">1</span>])</span><br><span class="line">    words = context_jieba(word_all)</span><br><span class="line">    list_ = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        whether_word_is_null = filter_words(word)</span><br><span class="line">        <span class="keyword">if</span> whether_word_is_null:</span><br><span class="line">            word = append_words(word)[<span class="number">0</span>]</span><br><span class="line">            list_.append((userid+<span class="string">&#x27; &#x27;</span>+word,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> (list_)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">[(<span class="string">&#x27;6185822016522959  scala&#x27;</span>, <span class="number">2016</span>), (<span class="string">&#x27;41641664258866384  博学谷&#x27;</span>, <span class="number">1372</span>), (<span class="string">&#x27;44801909258572364  hadoop&#x27;</span>, <span class="number">1260</span>), (<span class="string">&#x27;7044693659960919  数据&#x27;</span>, <span class="number">1120</span>), (<span class="string">&#x27;7044693659960919  数据仓库&#x27;</span>, <span class="number">1120</span>)]</span><br></pre></td></tr></table></figure>

<p>这样是方便针对性营销。</p>
<h2 id="需求3-热门搜索时间段分析"><a href="#需求3-热门搜索时间段分析" class="headerlink" title="需求3: 热门搜索时间段分析"></a>需求3: 热门搜索时间段分析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">vim log_analize.py</span><br><span class="line"><span class="comment"># conding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf,SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel</span><br><span class="line"><span class="keyword">from</span> defs <span class="keyword">import</span> context_jieba,filter_words,append_words</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;creat_test&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    file_rdd = sc.textFile(<span class="string">&#x27;/spark_study/SogouQ.txt&#x27;</span>)</span><br><span class="line">    split_rdd = file_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;\t&#x27;</span>))</span><br><span class="line">    <span class="comment"># print(split_rdd.collect())</span></span><br><span class="line">    split_rdd.persist(StorageLevel.DISK_ONLY)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需求三：</span></span><br><span class="line">    time_rdd = split_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    hourse_with_one_rdd = time_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x.split(<span class="string">&#x27;:&#x27;</span>)[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">    result3 = hourse_with_one_rdd.reduceByKey(<span class="keyword">lambda</span> a,b:a+b).\</span><br><span class="line">        sortBy(<span class="keyword">lambda</span> x:x[<span class="number">1</span>],ascending=<span class="literal">False</span>,numPartitions=<span class="number">1</span>).collect()</span><br><span class="line">    <span class="built_in">print</span>(result3)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">[(<span class="string">&#x27;20&#x27;</span>, <span class="number">3479</span>), (<span class="string">&#x27;23&#x27;</span>, <span class="number">3087</span>), (<span class="string">&#x27;21&#x27;</span>, <span class="number">2989</span>), (<span class="string">&#x27;22&#x27;</span>, <span class="number">2499</span>), (<span class="string">&#x27;01&#x27;</span>, <span class="number">1365</span>), (<span class="string">&#x27;10&#x27;</span>, <span class="number">973</span>), (<span class="string">&#x27;11&#x27;</span>, <span class="number">875</span>), (<span class="string">&#x27;05&#x27;</span>, <span class="number">798</span>), (<span class="string">&#x27;02&#x27;</span>, <span class="number">756</span>), (<span class="string">&#x27;19&#x27;</span>, <span class="number">735</span>), (<span class="string">&#x27;12&#x27;</span>, <span class="number">644</span>), (<span class="string">&#x27;14&#x27;</span>, <span class="number">637</span>), (<span class="string">&#x27;00&#x27;</span>, <span class="number">504</span>), (<span class="string">&#x27;16&#x27;</span>, <span class="number">497</span>), (<span class="string">&#x27;08&#x27;</span>, <span class="number">476</span>), (<span class="string">&#x27;04&#x27;</span>, <span class="number">476</span>), (<span class="string">&#x27;03&#x27;</span>, <span class="number">385</span>), (<span class="string">&#x27;09&#x27;</span>, <span class="number">371</span>), (<span class="string">&#x27;15&#x27;</span>, <span class="number">350</span>), (<span class="string">&#x27;06&#x27;</span>, <span class="number">294</span>), (<span class="string">&#x27;13&#x27;</span>, <span class="number">217</span>), (<span class="string">&#x27;18&#x27;</span>, <span class="number">112</span>), (<span class="string">&#x27;17&#x27;</span>, <span class="number">77</span>), (<span class="string">&#x27;07&#x27;</span>, <span class="number">70</span>)]</span><br></pre></td></tr></table></figure>

<p>可见晚上的20点是搜索高峰期。</p>
<h2 id="集群中运行上述三个需求"><a href="#集群中运行上述三个需求" class="headerlink" title="集群中运行上述三个需求"></a>集群中运行上述三个需求</h2><p>先在每台机器上 安装好jieba，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">vim log_analize_on_yarn.py</span><br><span class="line"><span class="comment"># conding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf,SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel</span><br><span class="line"><span class="keyword">from</span> defs_on_yarn <span class="keyword">import</span> context_jieba,filter_words,append_words,extract_user_and_word</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;analize_log&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    file_rdd = sc.textFile(<span class="string">&#x27;/spark_study/SogouQ.txt&#x27;</span>)</span><br><span class="line">    split_rdd = file_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;\t&#x27;</span>))</span><br><span class="line">    <span class="comment"># print(split_rdd.collect())</span></span><br><span class="line">    split_rdd.persist(StorageLevel.DISK_ONLY)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需求一：</span></span><br><span class="line">    context_rdd = split_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">2</span>])</span><br><span class="line">    <span class="comment"># print(context_rdd.collect())</span></span><br><span class="line">    words_rdd = context_rdd.flatMap( context_jieba)</span><br><span class="line">    <span class="comment"># print(words_rdd.collect())</span></span><br><span class="line">    filtered_rdd = words_rdd.<span class="built_in">filter</span>( filter_words)</span><br><span class="line">    <span class="comment"># print(filtered_rdd.collect())</span></span><br><span class="line">    final_words_rdd = filtered_rdd.<span class="built_in">map</span>( append_words)</span><br><span class="line">    <span class="comment"># print(final_words_rdd.collect())</span></span><br><span class="line">    result = final_words_rdd.reduceByKey(<span class="keyword">lambda</span> a,b : a+b).\</span><br><span class="line">        sortBy(<span class="keyword">lambda</span> x: x[<span class="number">1</span>],ascending=<span class="literal">False</span>,numPartitions=<span class="number">1</span> ).take(<span class="number">5</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;需求1： \n&#x27;</span>,result)</span><br><span class="line"><span class="comment"># 需求二：</span></span><br><span class="line">		user_content_rdd = split_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x[<span class="number">1</span>],x[<span class="number">2</span>]))</span><br><span class="line">    <span class="comment"># print(user_content_rdd.collect())</span></span><br><span class="line">    user_word_with_one_rdd = user_content_rdd.flatMap( extract_user_and_word)</span><br><span class="line">    <span class="comment"># print(user_word_with_one_rdd.collect())</span></span><br><span class="line">    result2 = user_word_with_one_rdd.reduceByKey(<span class="keyword">lambda</span> a,b:a+b).\</span><br><span class="line">        sortBy(<span class="keyword">lambda</span> x:x[<span class="number">1</span>],ascending=<span class="literal">False</span>,numPartitions=<span class="number">1</span>).take(<span class="number">5</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;需求2： \n&#x27;</span>,result2)</span><br><span class="line"><span class="comment"># 需求三：</span></span><br><span class="line">    time_rdd = split_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    hourse_with_one_rdd = time_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x.split(<span class="string">&#x27;:&#x27;</span>)[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">    result3 = hourse_with_one_rdd.reduceByKey(<span class="keyword">lambda</span> a,b:a+b).\</span><br><span class="line">        sortBy(<span class="keyword">lambda</span> x:x[<span class="number">1</span>],ascending=<span class="literal">False</span>,numPartitions=<span class="number">1</span>).collect()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;需求3： \n&#x27;</span>,result3)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">vim defs_on_yarn.py</span><br><span class="line"><span class="comment"># conding:utf8</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">context_jieba</span>(<span class="params">data</span>):</span><br><span class="line">    seg = jieba.cut_for_search(data)</span><br><span class="line">    l = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> seg:</span><br><span class="line">        l.append(word)</span><br><span class="line">    <span class="keyword">return</span> l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">filter_words</span>(<span class="params">data</span>):             <span class="comment"># return false or true</span></span><br><span class="line">    <span class="keyword">return</span> data <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;谷&#x27;</span>,<span class="string">&#x27;帮&#x27;</span>,<span class="string">&#x27;客&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">append_words</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="keyword">if</span> data == <span class="string">&#x27;传智播&#x27;</span>: data = <span class="string">&#x27;传智播客&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> data == <span class="string">&#x27;院校&#x27;</span> : data = <span class="string">&#x27;院校帮&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> data == <span class="string">&#x27;博学&#x27;</span>: data = <span class="string">&#x27;博学谷&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> (data,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_user_and_word</span>(<span class="params">data</span>):</span><br><span class="line">    userid = <span class="built_in">str</span>(data[<span class="number">0</span>])</span><br><span class="line">    word_all = <span class="built_in">str</span>(data[<span class="number">1</span>])</span><br><span class="line">    words = context_jieba(word_all)</span><br><span class="line">    list_ = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        whether_word_is_null = filter_words(word)</span><br><span class="line">        <span class="keyword">if</span> whether_word_is_null:</span><br><span class="line">            word = append_words(word)[<span class="number">0</span>]</span><br><span class="line">            list_.append((userid+<span class="string">&#x27; &#x27;</span>+word,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> (list_)</span><br></pre></td></tr></table></figure>

<p>yarn集群中运行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/spark/spark/bin/spark-submit --master yarn --py-files /opt/data/idea_pyspark/RDD_1/example/defs_on_yarn.py /opt/data/idea_pyspark/RDD_1/example/log_analize_on_yarn.py</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">需求<span class="number">1</span>： </span><br><span class="line"> [(<span class="string">&#x27;scala&#x27;</span>, <span class="number">2310</span>), (<span class="string">&#x27;hadoop&#x27;</span>, <span class="number">2268</span>), (<span class="string">&#x27;博学谷&#x27;</span>, <span class="number">2002</span>), (<span class="string">&#x27;传智汇&#x27;</span>, <span class="number">1918</span>), (<span class="string">&#x27;itheima&#x27;</span>, <span class="number">1680</span>)]</span><br><span class="line">需求<span class="number">2</span>： </span><br><span class="line"> [(<span class="string">&#x27;6185822016522959 scala&#x27;</span>, <span class="number">2016</span>), (<span class="string">&#x27;41641664258866384 博学谷&#x27;</span>, <span class="number">1372</span>), (<span class="string">&#x27;44801909258572364 hadoop&#x27;</span>, <span class="number">1260</span>), (<span class="string">&#x27;7044693659960919 仓库&#x27;</span>, <span class="number">1120</span>), (<span class="string">&#x27;7044693659960919 数据&#x27;</span>, <span class="number">1120</span>)]</span><br><span class="line">需求<span class="number">3</span>： </span><br><span class="line"> [(<span class="string">&#x27;20&#x27;</span>, <span class="number">3479</span>), (<span class="string">&#x27;23&#x27;</span>, <span class="number">3087</span>), (<span class="string">&#x27;21&#x27;</span>, <span class="number">2989</span>), (<span class="string">&#x27;22&#x27;</span>, <span class="number">2499</span>), (<span class="string">&#x27;01&#x27;</span>, <span class="number">1365</span>), (<span class="string">&#x27;10&#x27;</span>, <span class="number">973</span>), (<span class="string">&#x27;11&#x27;</span>, <span class="number">875</span>), (<span class="string">&#x27;05&#x27;</span>, <span class="number">798</span>), (<span class="string">&#x27;02&#x27;</span>, <span class="number">756</span>), (<span class="string">&#x27;19&#x27;</span>, <span class="number">735</span>), (<span class="string">&#x27;12&#x27;</span>, <span class="number">644</span>), (<span class="string">&#x27;14&#x27;</span>, <span class="number">637</span>), (<span class="string">&#x27;00&#x27;</span>, <span class="number">504</span>), (<span class="string">&#x27;16&#x27;</span>, <span class="number">497</span>), (<span class="string">&#x27;08&#x27;</span>, <span class="number">476</span>), (<span class="string">&#x27;04&#x27;</span>, <span class="number">476</span>), (<span class="string">&#x27;03&#x27;</span>, <span class="number">385</span>), (<span class="string">&#x27;09&#x27;</span>, <span class="number">371</span>), (<span class="string">&#x27;15&#x27;</span>, <span class="number">350</span>), (<span class="string">&#x27;06&#x27;</span>, <span class="number">294</span>), (<span class="string">&#x27;13&#x27;</span>, <span class="number">217</span>), (<span class="string">&#x27;18&#x27;</span>, <span class="number">112</span>), (<span class="string">&#x27;17&#x27;</span>, <span class="number">77</span>), (<span class="string">&#x27;07&#x27;</span>, <span class="number">70</span>)]</span><br></pre></td></tr></table></figure>



<h3 id="压榨集群的性能-提交"><a href="#压榨集群的性能-提交" class="headerlink" title="压榨集群的性能 提交"></a>压榨集群的性能 提交</h3><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2019.10.10.jpg" alt="截屏2022-04-10 19.10.10"></p>
<p>一般是每个虚拟机有多少个cpu， 就开多少个executors <code>--num-executor 6</code>。</p>
<p>每个executor 的core可以完全压榨，但是 每个 executor 内存需要留出一些空间给别的进程用，不能完全压榨！</p>
<p>由于我的几台虚拟机配置惨不忍睹，如下：</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2019.47.16.jpg" alt="截屏2022-04-10 19.47.16"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/opt/spark/spark/bin/spark-submit --master yarn --py-files /opt/data/idea_pyspark/RDD_1/example/defs_on_yarn.py </span><br><span class="line">--executor-memory 1g --executor-cores 1 --num-executors 3</span><br><span class="line">/opt/data/idea_pyspark/RDD_1/example/log_analize_on_yarn.py</span><br></pre></td></tr></table></figure>

<p>​		老实说上面压榨方法我都怕斯基，捂脸。总之压榨性能就是这么干。工作中用得到。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2019.57.16.jpg" alt="截屏2022-04-10 19.57.16"></p>
<h1 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h1><p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2022.51.45.jpg" alt="截屏2022-04-10 22.51.45"></p>
<h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2022.45.47.jpg" alt="截屏2022-04-10 22.45.47"></p>
<p>​		 rdd 才运行在executor 内，之前的部分都在driver 内，以上代码中 <code>stu_info_list</code> 就是driver中的文件。</p>
<p>​		如果 executor 需要 driver 发送一些文件过来用，是一个分区发送一份吗？如果是这样会造成io资源和内存资源的浪费，因为executor 是进程，里面的分区可以视为线程，能共享进程的资源，理论上一个executor 发送一份文件就行了，executor 内部的分区共享driver 发送来的文件。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2022.54.01.jpg" alt="截屏2022-04-10 22.54.01"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2022.55.38.jpg" alt="截屏2022-04-10 22.55.38"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># conding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf,SparkContext</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;creat_test&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    stu_info_list = [(<span class="number">1</span>,<span class="string">&#x27;张大仙&#x27;</span>,<span class="number">11</span>),</span><br><span class="line">                     (<span class="number">2</span>,<span class="string">&#x27;王小小&#x27;</span>,<span class="number">13</span>),</span><br><span class="line">                     (<span class="number">3</span>,<span class="string">&#x27;张甜甜&#x27;</span>,<span class="number">11</span>),</span><br><span class="line">                     (<span class="number">4</span>,<span class="string">&#x27;王大力&#x27;</span>,<span class="number">11</span>)]</span><br><span class="line">    broadcast = sc.broadcast(stu_info_list)          <span class="comment"># 广播变量</span></span><br><span class="line">    score_info_rdd = sc.parallelize([(<span class="number">1</span>,<span class="string">&#x27;语文&#x27;</span>,<span class="number">99</span>),</span><br><span class="line">                                     (<span class="number">2</span>,<span class="string">&#x27;数学&#x27;</span>,<span class="number">99</span>),</span><br><span class="line">                                     (<span class="number">3</span>,<span class="string">&#x27;英语&#x27;</span>,<span class="number">99</span>),</span><br><span class="line">                                     (<span class="number">4</span>,<span class="string">&#x27;编程&#x27;</span>,<span class="number">99</span>),</span><br><span class="line">                                     (<span class="number">1</span>,<span class="string">&#x27;语文&#x27;</span>,<span class="number">99</span>),</span><br><span class="line">                                     (<span class="number">2</span>,<span class="string">&#x27;数学&#x27;</span>,<span class="number">99</span>),</span><br><span class="line">                                     (<span class="number">3</span>,<span class="string">&#x27;英语&#x27;</span>,<span class="number">99</span>),</span><br><span class="line">                                     (<span class="number">4</span>,<span class="string">&#x27;编程&#x27;</span>,<span class="number">99</span>)])</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">map_func</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="built_in">id</span> = data[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> stu_info <span class="keyword">in</span> broadcast.value:						<span class="comment"># 广播变量复原</span></span><br><span class="line">            stu_id = stu_info[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">id</span> == stu_id:</span><br><span class="line">                name = stu_info[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> (name,data[<span class="number">1</span>],data[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(score_info_rdd.<span class="built_in">map</span>(map_func).collect())</span><br><span class="line"><span class="comment">#output </span></span><br><span class="line">[(<span class="string">&#x27;张大仙&#x27;</span>, <span class="string">&#x27;语文&#x27;</span>, <span class="number">99</span>), (<span class="string">&#x27;王小小&#x27;</span>, <span class="string">&#x27;数学&#x27;</span>, <span class="number">99</span>), (<span class="string">&#x27;张甜甜&#x27;</span>, <span class="string">&#x27;英语&#x27;</span>, <span class="number">99</span>), (<span class="string">&#x27;王大力&#x27;</span>, <span class="string">&#x27;编程&#x27;</span>, <span class="number">99</span>), (<span class="string">&#x27;张大仙&#x27;</span>, <span class="string">&#x27;语文&#x27;</span>, <span class="number">99</span>), (<span class="string">&#x27;王小小&#x27;</span>, <span class="string">&#x27;数学&#x27;</span>, <span class="number">99</span>), (<span class="string">&#x27;张甜甜&#x27;</span>, <span class="string">&#x27;英语&#x27;</span>, <span class="number">99</span>), (<span class="string">&#x27;王大力&#x27;</span>, <span class="string">&#x27;编程&#x27;</span>, <span class="number">99</span>)]</span><br></pre></td></tr></table></figure>

<p>​		有人有疑问，既然放在本地容易造成io资源浪费，那直接把大量的文件以rdd形式直接存放在executor 中，以此避免资源浪费行吗？</p>
<p>​		不行！</p>
<p>​		因为大量的文件rdd 存储在 分区的话，其实是分布式存储，等于各个分区均匀存储一部分数据，如果要运算这样的数据，就需要各个 分区之间 传输数据，也就是shuffle，非常浪费资源！如下还仅仅是小的数据就已经有了这么多传输线。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-10%2023.19.55.jpg" alt="截屏2022-04-10 23.19.55"></p>
<p>​		但是，当数据集有 10g 或者比较大的情况下，不要再放在本地 用driver存储了，最好还是分布式存储。相当于小的数据时，把数据存放在本地driver，然后广播变量性能更好一些，大的数据时情况就变了。</p>
<h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># conding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf,SparkContext</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;creat_test&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],<span class="number">2</span>)</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">map_fun</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="keyword">global</span> count</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">    <span class="comment"># map_fun(1)</span></span><br><span class="line">    <span class="comment"># print(count)    # 全局变量 count 调用过一次以后就变成1了。</span></span><br><span class="line">    rdd.<span class="built_in">map</span>(map_fun).collect()</span><br><span class="line">    <span class="built_in">print</span>(count)   <span class="comment"># result is 0  why? 看下图的解释。</span></span><br></pre></td></tr></table></figure>

<p>​		首先我们要明确，map这样的rdd算子是在executor 中计算的，实际发生在分区内，而不在driver内，第一个count 在driver中，而map_fun() 方法是在两个分区内被调用的，最后的print(count) 输出的还是driver内的初始count。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-11%2016.29.54.jpg" alt="截屏2022-04-11 16.29.54"></p>
<p>分布式计算想要解决这个问题，就用到了累加器。</p>
<p>累加器，就是让分区内此对象的操作（相加） 能够同步到 driver中，让driver内的相应对象也操作（相加）一遍。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-11%2016.34.28.jpg" alt="截屏2022-04-11 16.34.28"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># conding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf,SparkContext</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;creat_test&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 新建一个累加器变量 count</span></span><br><span class="line">    count = sc.accumulator(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">map_fun</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="keyword">global</span> count</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">				<span class="built_in">print</span>(count)</span><br><span class="line">    rdd.<span class="built_in">map</span>(map_fun).collect()</span><br><span class="line">    <span class="built_in">print</span>(count) </span><br><span class="line"><span class="comment"># output </span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">9</span></span><br></pre></td></tr></table></figure>

<h3 id="老版本spark累加器-需要注意的问题（新版本好像已经解决了）："><a href="#老版本spark累加器-需要注意的问题（新版本好像已经解决了）：" class="headerlink" title="老版本spark累加器 需要注意的问题（新版本好像已经解决了）："></a>老版本spark累加器 需要注意的问题（新版本好像已经解决了）：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># conding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf,SparkContext</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;creat_test&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 新建一个累加器变量 count</span></span><br><span class="line">    count = sc.accumulator(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">map_fun</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="keyword">global</span> count</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">    rdd2 = rdd.<span class="built_in">map</span>(map_fun)</span><br><span class="line">    rdd2.collect()</span><br><span class="line">    rdd3 = rdd2.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x)</span><br><span class="line">    <span class="built_in">print</span>(count) </span><br><span class="line"><span class="comment">#output </span></span><br><span class="line"><span class="number">18</span>      </span><br><span class="line"><span class="comment"># 为什么输出是18？ 因为rdd 在collect后已经没有了，但是累加器count已经在driver中变成9了，rdd2 重新调用 rdd 时，rdd会再重新生成一遍又加 9 次，同步到driver，累加器就变成18了。</span></span><br><span class="line"><span class="comment"># 想要避免，就需要在 rdd的collect 之前 做好持久化或者cache缓存。</span></span><br><span class="line"><span class="comment"># rdd.cache()</span></span><br><span class="line"><span class="comment"># rdd.map(map_fun).collect()</span></span><br><span class="line"><span class="comment"># 这样，下一次调用rdd时，在缓存位置继续执行，不会让累加器再加一遍了。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># （注意）</span></span><br><span class="line"><span class="comment"># 新的版本的spark ，已经把这个问题解决了，就算不cache或者持久化，再生成一次rdd，累加器不会再加一遍了！</span></span><br></pre></td></tr></table></figure>

<h2 id="综合案例"><a href="#综合案例" class="headerlink" title="综合案例"></a>综合案例</h2><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-11%2017.17.35.jpg" alt="截屏2022-04-11 17.17.35"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># conding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf,SparkContext</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;creat_test&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    file_rdd = sc.textFile(<span class="string">&#x27;file:///opt/data/idea_pyspark/RDD_1/data/accumulator_broadcast_data.txt&#x27;</span>)</span><br><span class="line">    abnormal_char = [<span class="string">&#x27;,&#x27;</span>,<span class="string">&#x27;.&#x27;</span>,<span class="string">&#x27;!&#x27;</span>,<span class="string">&#x27;#&#x27;</span>,<span class="string">&#x27;$&#x27;</span>,<span class="string">&#x27;%&#x27;</span>]</span><br><span class="line">		</span><br><span class="line">    broadcast = sc.broadcast(abnormal_char)  <span class="comment"># 广播变量，用于减少资源浪费。</span></span><br><span class="line"></span><br><span class="line">    acmlt = sc.accumulator(<span class="number">0</span>)                <span class="comment"># 累加器，方便driver计数。</span></span><br><span class="line"></span><br><span class="line">    lines_rdd = file_rdd.<span class="built_in">filter</span>(<span class="keyword">lambda</span> line: line.strip()) <span class="comment"># row空strip返回false，实现过滤空行。</span></span><br><span class="line">    <span class="comment"># print(lines_rdd.collect())</span></span><br><span class="line">    data_rdd = lines_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> line: line.strip()) <span class="comment"># 保留行去头尾空格。</span></span><br><span class="line">    <span class="comment"># print(data_rdd.collect())</span></span><br><span class="line">    word_rdd = data_rdd.flatMap(<span class="keyword">lambda</span> line: re.split(<span class="string">&#x27;\s+&#x27;</span>,line )) <span class="comment"># 有的两个空格有的一个空格作分隔符，所以用re 库的正则化 来切分。</span></span><br><span class="line">    <span class="comment"># print(word_rdd.collect())</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 过滤掉特殊字符，把每一个rdd的元素data 和 广播来的特殊符号对比，如果是特殊符号就false 过滤掉，并且 累加器+1，正常字符就保留。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">filter_func</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="keyword">global</span> acmlt</span><br><span class="line">        abnormal_char_list = broadcast.value</span><br><span class="line">        <span class="keyword">if</span> data <span class="keyword">in</span> abnormal_char_list:</span><br><span class="line">            acmlt += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    normal_words_rdd = word_rdd.<span class="built_in">filter</span>(filter_func) <span class="comment"># 过滤掉特殊字符</span></span><br><span class="line">    result_rdd = normal_words_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x,<span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> a,b: a+b).\</span><br><span class="line">        sortBy(<span class="keyword">lambda</span> x: x[<span class="number">1</span>],ascending=<span class="literal">False</span>,numPartitions=<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(result_rdd.collect())</span><br><span class="line">    <span class="built_in">print</span>(acmlt)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">[(<span class="string">&#x27;spark&#x27;</span>, <span class="number">11</span>), (<span class="string">&#x27;hive&#x27;</span>, <span class="number">6</span>), (<span class="string">&#x27;mapreduce&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;hadoop&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;hdfs&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;sql&#x27;</span>, <span class="number">2</span>)]</span><br><span class="line"><span class="number">8</span></span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-11%2019.25.18.jpg" alt="截屏2022-04-11 19.25.18"></p>
<h1 id="内核调度（重要）"><a href="#内核调度（重要）" class="headerlink" title="内核调度（重要）"></a>内核调度（重要）</h1><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-11%2023.03.07.jpg" alt="截屏2022-04-11 23.03.07"></p>
<h2 id="DAG-有向无环图"><a href="#DAG-有向无环图" class="headerlink" title="DAG 有向无环图"></a>DAG 有向无环图</h2><p>​		有向无环图，可以参考我之前的《数据结构_浙江大学》-《41拓扑排序.py》。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># AOV 如果有合理的拓扑序，则必定是 有向、无环、图。</span></span><br><span class="line"><span class="comment">#（Directed、 Acyclic、 Graph, ADG ）</span></span><br><span class="line"><span class="comment"># 因为，如果有一个cyclic环 ，v就必须在v开始前结束，逻辑矛盾。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拓扑排序 改进算法伪代码：</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">TopSort</span>():</span><br><span class="line">    <span class="keyword">for</span> 图中每个顶点 v：</span><br><span class="line">        <span class="keyword">if</span> Indegree[v] == <span class="number">0</span>:  <span class="comment"># 如果入度为0</span></span><br><span class="line">            Enqueue(v,Q)      <span class="comment"># 入队列</span></span><br><span class="line">    <span class="keyword">while</span> !IsEmpty(Q):</span><br><span class="line">        v = Dequeue(Q) </span><br><span class="line">        输出，或者记录v的输出序号</span><br><span class="line">        count ++</span><br><span class="line">        <span class="keyword">for</span> v 的每个邻接点 w：</span><br><span class="line">            <span class="keyword">if</span> --Indegree[w] == <span class="number">0</span>:</span><br><span class="line">                Enqueue(w,Q)</span><br><span class="line">    <span class="keyword">if</span> count != v_num:   <span class="comment"># 图中依然有点未输出。</span></span><br><span class="line">        Error 图中有回路(有环)</span><br></pre></td></tr></table></figure>

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2012.12.56.jpg" alt="截屏2022-04-12 12.12.56"></p>
<p>​		上图是之前 案例练习 三个需求的DAG ，实际上三个 action算子 就是三个 job任务（action算子才能把rdd链条运行起来），在4040端口或者历史端口的DAG可视化 也能看见。</p>
<p>​		代码运行起来以后，把其中一个 job任务链条 多分区拆开来看，如下：</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2012.19.50.jpg" alt="截屏2022-04-12 12.19.50"></p>
<h2 id="宽窄依赖和阶段划分"><a href="#宽窄依赖和阶段划分" class="headerlink" title="宽窄依赖和阶段划分"></a>宽窄依赖和阶段划分</h2><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2012.26.17.jpg" alt="截屏2022-04-12 12.26.17"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2012.28.25.jpg" alt="截屏2022-04-12 12.28.25"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2012.29.03.jpg" alt="截屏2022-04-12 12.29.03"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2012.30.46.jpg" alt="截屏2022-04-12 12.30.46"></p>
<h2 id="内存迭代计算"><a href="#内存迭代计算" class="headerlink" title="内存迭代计算"></a>内存迭代计算</h2><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2015.43.06.jpg" alt="截屏2022-04-12 15.43.06"></p>
<p>​		上图是三个executor 并行计算的DAG，一个task 是一个executor内的线程，这样可以避免 stage 内走网络。</p>
<p>​		task 之间是不需要交互的，所以task 之间可以并行计算，要保证spark的性能，首先要“并行”，其次要减少网络io传输。  000</p>
<p>​		两个stage 之间的宽依赖 连接时，比如reducebykey（） ，不可避免走网络 在executor之间交互内容。</p>
<p>​		像local模式，直接走内存，不走任何网络，因为只有一个executor  实际是用内部的多个线程来模拟 并行计算，实现不了spark集群多个executor 的并行计算性能优势。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2015.55.35.jpg" alt="截屏2022-04-12 15.55.35"></p>
<p>​		像上图一样，在rdd3 的位置，如果分成5个分区，破坏了之前 3 个分区的 结构，就会出现“分叉”也就是 “宽依赖”，缩短了 pipeline 内存计算管道（红色框内容）长度，增加了stage 的个数，同时大幅增加了 shuffle 的的复杂度。</p>
<p>​		spark 就是将尽可能多的计算步骤在  pipeline 内存计算管道（红色框内容）内完成，以实现spark 内存并行计算的 “高性能”。</p>
<p>​		因此 ，spark 加了 “全局并行度限制”。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.01.16.jpg" alt="截屏2022-04-12 16.01.16"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.04.08.jpg" alt="截屏2022-04-12 16.04.08"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.09.57.jpg" alt="截屏2022-04-12 16.09.57"></p>
<h2 id="spark并行度"><a href="#spark并行度" class="headerlink" title="spark并行度"></a>spark并行度</h2><p>​		逻辑上是先确定了并行度，再根据并行度确定分区数。</p>
<p>​		一个分区在一个stage内 只会被一个 task 处理，一个task 处理多个rdd 的某一个分区 的内容。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.36.06.jpg" alt="截屏2022-04-12 16.36.06"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.37.22.jpg" alt="截屏2022-04-12 16.37.22"></p>
<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.38.15.jpg" alt="截屏2022-04-12 16.38.15" style="zoom:50%;">

<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2016.42.51.jpg" alt="截屏2022-04-12 16.42.51"></p>
<p>​		理论上，并行的task越多，每个的task大小就都会变小，相当于摊薄了task，最后的一批task任务结束时间就会相对接近，从这个角度看，也可以降低 cpu 空闲。</p>
<h2 id="spark任务调度"><a href="#spark任务调度" class="headerlink" title="spark任务调度"></a>spark任务调度</h2><p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2017.56.45.jpg" alt="截屏2022-04-12 17.56.45"></p>
<p>​		executor 的数量 由 <code>--num-executors</code> 决定，和并行度 没有关系。</p>
<p>​		DAGscheduler 先 分解task 任务，再根据executor 数量分配下去。(关键)</p>
<p>​		Taskscheduler 仅仅是把 DAGscheduler 安排好的策略 发到executor，自己扮演一个“监工”。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2015.43.06.jpg" alt="截屏2022-04-12 15.43.06"></p>
<p>​		如果是 3 个executor ，DAGscheduler 就会把上面6个task 按如下方法分配给executor。</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2018.10.54.jpg" alt="截屏2022-04-12 18.10.54"></p>
<p>​		有多少个 机器，设置多少个 executor ，保证都用上就行。</p>
<p>​		如果只有2个机器，却强制开启了4个 executor 进程，反而会降低性能，因为进程之间的通信要走网络，无法像进程内部的不同task线程通信一样走内存，如下图：</p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2018.17.43.jpg" alt="截屏2022-04-12 18.17.43"></p>
<p>​		理论上，一个服务器内，cpu的核心数量（严格来说是线程数量） &#x3D;&#x3D; task分配进此服务器的数量，也就是把cpu性能吃满，效率会最高。</p>
<h2 id="spark常用词汇"><a href="#spark常用词汇" class="headerlink" title="spark常用词汇"></a>spark常用词汇</h2><p>​		<img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2021.54.34.jpg" alt="截屏2022-04-12 21.54.34"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2021.57.06.jpg" alt="截屏2022-04-12 21.57.06"></p>
<p><img src="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-core/%E6%88%AA%E5%B1%8F2022-04-12%2021.58.32.jpg" alt="截屏2022-04-12 21.58.32"></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark-dataFrame%E5%88%97%E7%9A%84%E5%90%88%E5%B9%B6%E4%B8%8E%E6%8B%86%E5%88%86/" rel="prev" title="spark_dataFrame列的合并与拆分">
                  <i class="fa fa-chevron-left"></i> spark_dataFrame列的合并与拆分
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/10/29/spark%E5%AD%A6%E4%B9%A0/spark%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/" rel="next" title="spark基础入门及安装部署">
                  spark基础入门及安装部署 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">陈宇韶chenyushao</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  





</body>
</html>
